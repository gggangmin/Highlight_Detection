{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import string\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "import math\n",
    "import torch.utils.data as data\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy\n",
    "import torch.utils.data.sampler as sampler\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config Path 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open('config.yaml') as f:\n",
    "    conf = yaml.full_load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [option]dataset 확인 (형태확인 및 피쳐수 확인 필요시)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(conf['dataset']['chat_path'],\"rb\") as f1:  \n",
    "    chat=pickle.load(f1)\n",
    "\n",
    "with open(conf['dataset']['audio_path'],\"rb\") as f2:  \n",
    "    audio=json.load(f2)\n",
    "with open(conf['dataset']['video_path'],\"rb\") as f2:  \n",
    "    video=json.load(f2)\n",
    "with open(conf['dataset']['label_path'],\"rb\") as f4:  \n",
    "    real_result=pickle.load(f4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2069"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(audio['102844412722519367'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "video 형태가 안맞았는지, 혼자만 코드 이상하게 되어있음. \n",
    "\n",
    "기본 형태 : 'gameid':[[0초 피쳐],[1초 피쳐]]\n",
    "\n",
    "video는 1개의 피쳐가 그냥 'gameid':[0초피쳐,1초피쳐,,,,]로 되어있음.\n",
    " --> 추후 수정시, \n",
    "\n",
    "**s_window+=list(self.video[game_id][vframe+idx])**\n",
    "\n",
    "로만 바꿔주시오~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "class Mul_data(data.Dataset):\n",
    "    def __init__(self,d_type):\n",
    "        self.d_type=d_type\n",
    "\n",
    "        with open(conf['dataset']['chat_path'],\"rb\") as f1:  \n",
    "            self.chat=pickle.load(f1)\n",
    "       \n",
    "        with open(conf['dataset']['audio_path'],\"rb\") as f2:  \n",
    "            self.audio=json.load(f2)\n",
    "        with open(conf['dataset']['video_path'],\"rb\") as f2:  \n",
    "            self.video=json.load(f2)\n",
    "        with open(conf['dataset']['label_path'],\"rb\") as f4:  \n",
    "            self.real_result=pickle.load(f4)\n",
    "            \n",
    "        if d_type=='train':\n",
    "            self.sample = ['102844412722519367','102844212429550795','102844401151219358','102844401154430631','102844412717014335','102844401153971877','102844224148503678','102844412722847048','102844401152857762','102844412707380528','102844212431516886','102844283027925085','102844412716227901','102844412710001974','102844294670878922','102844294670551241','102844283023599703','102844412704496937','102844235751783874','102844401152071328','102844412709674293','102844401153447587','102844224148896895','102844235746868664','102979081290790284','102844283027531868','102844212431975640','102844401155937960','102844212429092040','102844341906649746','102844412706987311','102844412721339716','102844212430402768','102844341905011343','102844235753356742','102844235750997440','102844412709346612','102844412705217835','102844235752963525','102844412712164667','102844412705545516','102844341912220311','102844341907370644','102844235749424575','102844212429419722','102844294669568199','102844212431779031','102844294666422466','102844224146472059','102844212428895431','102844212429747404','102844235748703677','102844224146930812','102844212430730450','102844294674876621','102844341909598870','102844283020453971','102844294670026952','102844412723174729','102844341904683662','102844283025696858','102844235747261881','102844401154168486','102844235748310460','102844412711836986','102844412723567946','102844235749031358','102844294674286796','102844294666881219','102844412716686654']\n",
    "        if d_type=='val':\n",
    "            self.sample = ['102844294671796427','102844224145685626','102844412717407552','102844235751390657','102844401156069033','102904869420860038','102910307641576395','102844341905404560','102844341906977427','102844212430075086','102844412711116088','102844401153578660','102844294667405508','102844412706659630']\n",
    "        if d_type=='test':\n",
    "            self.sample = ['102844212431058132','102844341902586509','102844401152267937','102844212430927059','102844412708953395','102844212429944013','102844341912679064','102844235753749959','102844341908026005','102844283023206486','102844224147717245','102844412704890154','102844212430599377','102844412711443769','102844235747982779']\n",
    "            \n",
    "        self.WeightedSampling=[]\n",
    "        for i in self.sample:\n",
    "            self.WeightedSampling.extend(copy.copy(self.real_result[str(i)]))\n",
    "        \n",
    "        sampling = np.array(self.WeightedSampling)\n",
    "        neg_idx = np.where(sampling == 0)[0] #general\n",
    "        pos_idx = np.where(sampling == 1)[0] #highlight\n",
    "        sampling = sampling.astype(np.float32)\n",
    "        \n",
    "        sampling.fill(0)\n",
    "        sampling[neg_idx] = len(sampling) / float(len(neg_idx))\n",
    "       # self.WeightedSampling[pos_idx] = len(self.WeightedSampling) / float(len(pos_idx))\n",
    "        sampling[pos_idx] = len(sampling) / float(len(pos_idx))\n",
    "        self.WeightedSampling = sampling\n",
    "\n",
    "        \n",
    "        self.sum=np.insert(np.cumsum([len(self.audio[str(i)]) for i in self.sample]),0,0)\n",
    "        print(\"data load fin\")\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.sum[-1]\n",
    "    def __getitem__(self,index):\n",
    "            vid=np.histogram(index,self.sum)#sum으로 누적으로 히스토그램이 깔려있음/ 그중에 index의 위치\n",
    "            vid = np.where(vid[0]>0)[0][0]#몇번째 game을 쓸지!\n",
    "            vframe=index-self.sum[vid]#그 게임 안에서의 몇번째 프레임인지\n",
    "            game_id=str(self.sample[vid])\n",
    "\n",
    "            window=[]#batch*7(window size)*3(highlight result)\n",
    "            window_size=conf['trn_args']['window_size']\n",
    "            std_size = min(len(self.audio[game_id]),min(len(self.chat[game_id]),len(self.video[game_id]))) #데이터가 가장 짧은거에 맞춤 (오류방지)\n",
    "            chat_size = conf['dataset']['chat_size']#c초당 피쳐 개수\n",
    "            audio_size = conf['dataset']['audio_size']\n",
    "            video_size = conf['dataset']['video_size']\n",
    "            global total_size\n",
    "            total_size = int(chat_size)+int(audio_size)+int(video_size)\n",
    "            \n",
    "            \n",
    "            for idx in range(window_size): #7 : window size\n",
    "                s_window=[]\n",
    "                if vframe+idx<std_size:#아래는 데이터 형태가 조금씩 달라서 다음과 같이 진행.\n",
    "                    s_window+=list((self.chat[game_id][vframe+idx]))#vframe의 chat\n",
    "                    s_window+=list(self.audio[game_id][vframe+idx])#vframe의 image\n",
    "                    s_window+=list(self.video[game_id][vframe+idx])\n",
    "                else:\n",
    "                    #s_window=[0,0,0]#padding value\n",
    "                    s_window=[0]*(total_size)#초가 초과되는 경우 0으로 피쳐 패\n",
    "                window.append(s_window)\n",
    "\n",
    "\n",
    "            label=int(self.real_result[game_id][vframe])\n",
    "            return game_id,np.array(window),label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load fin\n",
      "data load fin\n",
      "\n",
      "\n",
      "datset testing....\n",
      "('102844412722519367', array([[ 0.02620566,  0.0118512 , -0.02706682, ..., -0.02692822,\n",
      "        -0.03071774, -0.02038495],\n",
      "       [-0.06698951, -0.03389126, -0.00954571, ..., -0.02692822,\n",
      "        -0.03071774, -0.02038495],\n",
      "       [-0.06153676, -0.02784403, -0.01758359, ..., -0.02692822,\n",
      "        -0.03071774, -0.02038495],\n",
      "       ...,\n",
      "       [-0.08774932, -0.03491368, -0.01236366, ..., -0.02692822,\n",
      "        -0.03071774, -0.02038495],\n",
      "       [-0.0150439 , -0.00701944, -0.01754227, ..., -0.02692822,\n",
      "        -0.03071774, -0.02038495],\n",
      "       [ 0.01076563,  0.00668381, -0.01694852, ..., -0.02692822,\n",
      "        -0.03071774, -0.02038495]]), 0)\n",
      "23\n",
      "\n",
      "****************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train=Mul_data('train')\n",
    "val=Mul_data('val')\n",
    "\n",
    "#test\n",
    "print(\"\\n\\ndatset testing....\")\n",
    "print(train[0])\n",
    "print(len(train[100][1]))\n",
    "print(\"\\n****************\\n\")\n",
    "\n",
    "\n",
    "#loader \n",
    "sampler1 = torch.utils.data.sampler.WeightedRandomSampler(weights=train.WeightedSampling.tolist(), num_samples=conf['trn_args']['sampling'])\n",
    "train_loader=torch.utils.data.DataLoader(train,batch_size=conf['trn_args']['trn_bs'],sampler=sampler1)\n",
    "# train_loader=torch.utils.data.DataLoader(train,batch_size=32)\n",
    "val_loader=torch.utils.data.DataLoader(val,batch_size=conf['trn_args']['val_bs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "global input_size,hidden_size,num_layers\n",
    "input_size=total_size\n",
    "hidden_size=conf['trn_args']['hidden_size']\n",
    "num_layers=conf['trn_args']['num_layer']\n",
    "\n",
    "print(input_size)\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        self._clf1 = nn.LSTM(input_size, hidden_size,num_layers,batch_first=True)\n",
    "        self._lin = nn.Sequential(nn.Linear(hidden_size, hidden_size),\n",
    "                                 nn.Linear(hidden_size,2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=x.cuda()\n",
    "        hidden = Variable(torch.zeros(num_layers,x.size(0),hidden_size)).cuda() # (num_layers * num_directions, batch, hidden_size)\n",
    "        cell = Variable(torch.zeros(num_layers,x.size(0),hidden_size)).cuda() # (num_layers * num_directions, batch, hidden_size)        out,hidden = self._clf1(x,h0)\n",
    "        out,hidden = self._clf1(x,(hidden,cell))#batch*7*3\n",
    "        feature = out[:,-1,:]\n",
    "        out = self._lin(out[:,-1,:])\n",
    "        return out,feature\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = conf['trn_args']['lr']\n",
    "\n",
    "model=LSTM().cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr,momentum=0.9,weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##best 값을 확인하기위함\n",
    "\n",
    "def fmeasure(output, target):\n",
    "    _, pred = output.topk(1, 1, True, True)\n",
    "    pred = pred.view(-1,1)\n",
    "    target = target.view(-1,1)\n",
    "\n",
    "    #overlap = ((pred== 1) + (target == 1)).gt(1)\n",
    "    #overlap = overlap.view(-1,1)\n",
    "    TP = len(np.where((pred==1)&(target==1)==True)[0]) # True positive\n",
    "    FP = len(np.where((pred==1)&(target==0)==True)[0]) # Condition positive = TP + FN\n",
    "    TN = len(np.where((pred==0)&(target==0)==True)[0])\n",
    "    FN = len(np.where((pred==0)&(target==1)==True)[0])\n",
    "\n",
    "    \n",
    "    #overlap_len = overlap.data.long().sum()\n",
    "    pred_len = pred.data.long().sum()\n",
    "    gt_len   =  target.data.long().sum()\n",
    "\n",
    "    return TP,FP,TN,FN,pred_len, gt_len,pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "epoch 0 train_loss : 0 , val_loss : 0.6905721426010132,p 0, r 0, f 0\n",
      "\n",
      "1\n",
      "epoch 1 train_loss : 0 , val_loss : 0.7004979848861694,p 0, r 0, f 0\n",
      "\n",
      "2\n",
      "epoch 2 train_loss : 0 , val_loss : 0.6888850927352905,p 0, r 0, f 0\n",
      "\n",
      "3\n",
      "epoch 3 train_loss : 0 , val_loss : 0.6876826882362366,p 0, r 0, f 0\n",
      "\n",
      "4\n",
      "epoch 4 train_loss : 0 , val_loss : 0.6938268542289734,p 0.19273755554558103, r 0.9479028697571744, f 0.32034018426647765\n",
      "\n",
      "5\n",
      "epoch 5 train_loss : 0 , val_loss : 0.687271237373352,p 0, r 0, f 0\n",
      "\n",
      "6\n",
      "epoch 6 train_loss : 0 , val_loss : 0.6877807974815369,p 0.7, r 0.04481236203090508, f 0.08423236514522821\n",
      "\n",
      "7\n",
      "epoch 7 train_loss : 0 , val_loss : 0.6905197501182556,p 0.41522359657469077, r 0.4816777041942605, f 0.4459887583035258\n",
      "\n",
      "8\n",
      "epoch 8 train_loss : 0 , val_loss : 0.6992735266685486,p 0, r 0, f 0\n",
      "\n",
      "9\n",
      "epoch 9 train_loss : 0 , val_loss : 0.6965732574462891,p 0.17961634561134843, r 0.9838852097130243, f 0.3037758996728462\n",
      "\n",
      "10\n",
      "epoch 10 train_loss : 0 , val_loss : 0.6987226009368896,p 0.1749078921853791, r 0.9955849889624724, f 0.29754247072406403\n",
      "\n",
      "11\n",
      "epoch 11 train_loss : 0 , val_loss : 0.6918612718582153,p 0.26005895357406045, r 0.779028697571744, f 0.3899447513812155\n",
      "\n",
      "12\n",
      "epoch 12 train_loss : 0 , val_loss : 0.6906922459602356,p 0.2724252491694352, r 0.7602649006622516, f 0.4011180992313068\n",
      "\n",
      "13\n",
      "epoch 13 train_loss : 0 , val_loss : 0.6875752806663513,p 0.2955388652610623, r 0.720971302428256, f 0.4192285475900135\n",
      "\n",
      "14\n",
      "epoch 14 train_loss : 0 , val_loss : 0.6748507022857666,p 0.35943335479716676, r 0.6161147902869757, f 0.45400569337128915\n",
      "\n",
      "15\n",
      "epoch 15 train_loss : 0 , val_loss : 0.6524524688720703,p 0.3684141546526868, r 0.6205298013245033, f 0.4623355263157895\n",
      "\n",
      "16\n",
      "epoch 16 train_loss : 0 , val_loss : 0.5633220076560974,p 0.38499270847143047, r 0.6410596026490066, f 0.4810734697258345\n",
      "\n",
      "17\n",
      "epoch 17 train_loss : 0 , val_loss : 0.5282246470451355,p 0.40336134453781514, r 0.6887417218543046, f 0.5087647778230738\n",
      "\n",
      "18\n",
      "epoch 18 train_loss : 0 , val_loss : 0.4961758255958557,p 0.43392070484581496, r 0.6958057395143488, f 0.5345090724096998\n",
      "\n",
      "19\n",
      "epoch 19 train_loss : 0 , val_loss : 0.48819854855537415,p 0.45077072628801557, r 0.7165562913907285, f 0.5534055067769159\n",
      "\n",
      "20\n",
      "epoch 20 train_loss : 0 , val_loss : 0.4517516493797302,p 0.48546644346370416, r 0.7041942604856513, f 0.5747229979281145\n",
      "\n",
      "21\n",
      "epoch 21 train_loss : 0 , val_loss : 0.4493182301521301,p 0.48768768768768767, r 0.7169977924944813, f 0.5805183199285077\n",
      "\n",
      "22\n",
      "epoch 22 train_loss : 0 , val_loss : 0.42776554822921753,p 0.5067342734907305, r 0.7059602649006622, f 0.5899824739415184\n",
      "\n",
      "23\n",
      "epoch 23 train_loss : 0 , val_loss : 0.42093560099601746,p 0.5081320069477341, r 0.7103752759381898, f 0.5924698517904815\n",
      "\n",
      "24\n",
      "epoch 24 train_loss : 0 , val_loss : 0.46875444054603577,p 0.45848806366047745, r 0.7631346578366446, f 0.5728251864125932\n",
      "\n",
      "25\n",
      "epoch 25 train_loss : 0 , val_loss : 0.4232845604419708,p 0.49693251533742333, r 0.733112582781457, f 0.5923481673058058\n",
      "\n",
      "26\n",
      "epoch 26 train_loss : 0 , val_loss : 0.46495321393013,p 0.45795993674222457, r 0.7671081677704195, f 0.5735269846509324\n",
      "\n",
      "27\n",
      "epoch 27 train_loss : 0 , val_loss : 0.45713159441947937,p 0.4659827213822894, r 0.7620309050772627, f 0.5783213268554197\n",
      "\n",
      "28\n",
      "epoch 28 train_loss : 0 , val_loss : 0.4451707899570465,p 0.47571606475716066, r 0.7589403973509934, f 0.584843072212299\n",
      "\n",
      "29\n",
      "epoch 29 train_loss : 0 , val_loss : 0.46294933557510376,p 0.45826190162204933, r 0.7671081677704195, f 0.5737637249236358\n",
      "\n",
      "30\n",
      "epoch 30 train_loss : 0 , val_loss : 0.4474038779735565,p 0.47106307292379257, r 0.7600441501103753, f 0.5816369625812992\n",
      "\n",
      "31\n",
      "epoch 31 train_loss : 0 , val_loss : 0.44737640023231506,p 0.46889432219505567, r 0.7620309050772627, f 0.5805583585603767\n",
      "\n",
      "32\n",
      "epoch 32 train_loss : 0 , val_loss : 0.4919596314430237,p 0.431945458972486, r 0.7832229580573952, f 0.556811048336472\n",
      "\n",
      "33\n",
      "epoch 33 train_loss : 0 , val_loss : 0.43926671147346497,p 0.47930648769574946, r 0.7567328918322296, f 0.5868858072247902\n",
      "\n",
      "34\n",
      "epoch 34 train_loss : 0 , val_loss : 0.36347874999046326,p 0.5602653888684114, r 0.6710816777041942, f 0.6106870229007634\n",
      "\n",
      "35\n",
      "epoch 35 train_loss : 0 , val_loss : 0.5243331789970398,p 0.41163055872291904, r 0.7969094922737306, f 0.5428571428571428\n",
      "\n",
      "36\n",
      "epoch 36 train_loss : 0 , val_loss : 0.39613473415374756,p 0.5219629927594529, r 0.7161147902869757, f 0.6038157282456955\n",
      "\n",
      "37\n",
      "epoch 37 train_loss : 0 , val_loss : 0.42537420988082886,p 0.4919716476204253, r 0.7507726269315673, f 0.5944245390194879\n",
      "\n",
      "38\n",
      "epoch 38 train_loss : 0 , val_loss : 0.4225406348705292,p 0.4912685813248665, r 0.7514348785871965, f 0.5941181603979405\n",
      "\n",
      "39\n",
      "epoch 39 train_loss : 0 , val_loss : 0.4237265884876251,p 0.4907154167266446, r 0.7525386313465784, f 0.5940576805785484\n",
      "\n",
      "40\n",
      "epoch 40 train_loss : 0 , val_loss : 0.3999428153038025,p 0.51530852567122, r 0.7245033112582782, f 0.6022570878062208\n",
      "\n",
      "41\n",
      "epoch 41 train_loss : 0 , val_loss : 0.3918689489364624,p 0.523109243697479, r 0.714569536423841, f 0.6040306027243889\n",
      "\n",
      "42\n",
      "epoch 42 train_loss : 0 , val_loss : 0.4442797303199768,p 0.4741674660819515, r 0.7637969094922737, f 0.5851018855161917\n",
      "\n",
      "43\n",
      "epoch 43 train_loss : 0 , val_loss : 0.46347635984420776,p 0.4535455486542443, r 0.7737306843267108, f 0.571871430902268\n",
      "\n",
      "44\n",
      "epoch 44 train_loss : 0 , val_loss : 0.3919949233531952,p 0.5246193715581471, r 0.7150110375275939, f 0.6051943198804186\n",
      "\n",
      "45\n",
      "epoch 45 train_loss : 0 , val_loss : 0.44951331615448,p 0.47732927840752004, r 0.7622516556291391, f 0.5870452227133627\n",
      "\n",
      "46\n",
      "epoch 46 train_loss : 0 , val_loss : 0.4745895266532898,p 0.45030737704918034, r 0.776158940397351, f 0.5699465067271843\n",
      "\n",
      "47\n",
      "epoch 47 train_loss : 0 , val_loss : 0.4281432628631592,p 0.49256961477420286, r 0.7536423841059603, f 0.5957595323270221\n",
      "\n",
      "48\n",
      "epoch 48 train_loss : 0 , val_loss : 0.4353647232055664,p 0.4823776223776224, r 0.7613686534216335, f 0.5905821917808218\n",
      "\n",
      "49\n",
      "epoch 49 train_loss : 0 , val_loss : 0.41488972306251526,p 0.5016320474777448, r 0.7463576158940397, f 0.5999999999999999\n",
      "\n",
      "50\n",
      "epoch 50 train_loss : 0 , val_loss : 0.43304407596588135,p 0.4836720392431675, r 0.7618101545253864, f 0.5916845263609087\n",
      "\n",
      "51\n",
      "epoch 51 train_loss : 0 , val_loss : 0.4013266861438751,p 0.5137230578384245, r 0.7313465783664459, f 0.6035158028964386\n",
      "\n",
      "52\n",
      "epoch 52 train_loss : 0 , val_loss : 0.39385560154914856,p 0.5241909644344761, r 0.7222958057395144, f 0.6075009283327145\n",
      "\n",
      "53\n",
      "epoch 53 train_loss : 0 , val_loss : 0.4398970603942871,p 0.4757015742642026, r 0.7671081677704195, f 0.5872412336290663\n",
      "\n",
      "54\n",
      "epoch 54 train_loss : 0 , val_loss : 0.3582531809806824,p 0.5630712979890311, r 0.6799116997792495, f 0.6160000000000001\n",
      "\n",
      "55\n",
      "epoch 55 train_loss : 0 , val_loss : 0.5092315673828125,p 0.42288265006460707, r 0.7947019867549668, f 0.5520202407421605\n",
      "\n",
      "56\n",
      "epoch 56 train_loss : 0 , val_loss : 0.4106072187423706,p 0.507311925222373, r 0.7428256070640177, f 0.6028845292484101\n",
      "\n",
      "57\n",
      "epoch 57 train_loss : 0 , val_loss : 0.41525712609291077,p 0.5019937970757643, r 0.7503311258278146, f 0.6015396867533847\n",
      "\n",
      "58\n",
      "epoch 58 train_loss : 0 , val_loss : 0.38888877630233765,p 0.5267126584309322, r 0.7247240618101545, f 0.6100529592121156\n",
      "\n",
      "59\n",
      "epoch 59 train_loss : 0 , val_loss : 0.41653338074684143,p 0.49912459877443827, r 0.7551876379690949, f 0.6010189739985945\n",
      "\n",
      "60\n",
      "epoch 60 train_loss : 0 , val_loss : 0.4159294068813324,p 0.5028156490812092, r 0.7490066225165563, f 0.6017024295087783\n",
      "\n",
      "61\n",
      "epoch 61 train_loss : 0 , val_loss : 0.5341887474060059,p 0.40367073304758766, r 0.8108167770419427, f 0.5389977254384034\n",
      "\n",
      "62\n",
      "epoch 62 train_loss : 0 , val_loss : 0.42784231901168823,p 0.49157142857142855, r 0.7596026490066226, f 0.596877710320902\n",
      "\n",
      "63\n",
      "epoch 63 train_loss : 0 , val_loss : 0.35978448390960693,p 0.5620658949243099, r 0.6966887417218544, f 0.6221784130113357\n",
      "\n",
      "64\n",
      "epoch 64 train_loss : 0 , val_loss : 0.470872163772583,p 0.45400898011545865, r 0.7812362030905077, f 0.5742799188640973\n",
      "\n",
      "65\n",
      "epoch 65 train_loss : 0 , val_loss : 0.43829917907714844,p 0.48187102131192916, r 0.7686534216335541, f 0.5923783599863899\n",
      "\n",
      "66\n",
      "epoch 66 train_loss : 0 , val_loss : 0.5324348211288452,p 0.4031766753698869, r 0.8181015452538631, f 0.540154496429092\n",
      "\n",
      "67\n",
      "epoch 67 train_loss : 0 , val_loss : 0.4383701980113983,p 0.4798028207585924, r 0.7735099337748345, f 0.5922420349869011\n",
      "\n",
      "68\n",
      "epoch 68 train_loss : 0 , val_loss : 0.3444557189941406,p 0.5819579894973743, r 0.6849889624724061, f 0.629284120867978\n",
      "\n",
      "69\n",
      "epoch 69 train_loss : 0 , val_loss : 0.4314388632774353,p 0.48229783716765395, r 0.7728476821192053, f 0.5939435066587496\n",
      "\n",
      "70\n",
      "epoch 70 train_loss : 0 , val_loss : 0.35550087690353394,p 0.5670048051254671, r 0.7033112582781457, f 0.6278451078924031\n",
      "\n",
      "71\n",
      "epoch 71 train_loss : 0 , val_loss : 0.4290924072265625,p 0.49041443473357765, r 0.7679911699779249, f 0.598589125946318\n",
      "\n",
      "72\n",
      "epoch 72 train_loss : 0 , val_loss : 0.38728392124176025,p 0.5271900955963015, r 0.7426048565121413, f 0.6166254238841535\n",
      "\n",
      "73\n",
      "epoch 73 train_loss : 0 , val_loss : 0.4139668643474579,p 0.5038919077691291, r 0.7573951434878587, f 0.6051680042331775\n",
      "\n",
      "74\n",
      "epoch 74 train_loss : 0 , val_loss : 0.4375153183937073,p 0.4803974952355023, r 0.779028697571744, f 0.5943078477601885\n",
      "\n",
      "75\n",
      "epoch 75 train_loss : 0 , val_loss : 0.33529025316238403,p 0.5917823853909073, r 0.6867549668874172, f 0.6357412894656177\n",
      "\n",
      "76\n",
      "epoch 76 train_loss : 0 , val_loss : 0.6260835528373718,p 0.35779898332292875, r 0.8856512141280353, f 0.5096868449469606\n",
      "\n",
      "77\n",
      "epoch 77 train_loss : 0 , val_loss : 0.49068817496299744,p 0.43559885931558934, r 0.8092715231788079, f 0.5663525413255059\n",
      "\n",
      "78\n",
      "epoch 78 train_loss : 0 , val_loss : 0.4111264944076538,p 0.5021853146853147, r 0.7609271523178808, f 0.6050552922590837\n",
      "\n",
      "79\n",
      "epoch 79 train_loss : 0 , val_loss : 0.4311498999595642,p 0.47712068504147714, r 0.7871964679911699, f 0.5941352882372541\n",
      "\n",
      "80\n",
      "epoch 80 train_loss : 0 , val_loss : 0.5082165598869324,p 0.4186435015035082, r 0.8298013245033112, f 0.5565178769709083\n",
      "\n",
      "81\n",
      "epoch 81 train_loss : 0 , val_loss : 0.5125567317008972,p 0.4220204574186875, r 0.8105960264900662, f 0.5550600861612879\n",
      "\n",
      "82\n",
      "epoch 82 train_loss : 0 , val_loss : 0.42325684428215027,p 0.4880968762900784, r 0.7830022075055187, f 0.6013393235568365\n",
      "\n",
      "83\n",
      "epoch 83 train_loss : 0 , val_loss : 0.4249311089515686,p 0.4873891352549889, r 0.7763796909492273, f 0.5988421590328622\n",
      "\n",
      "84\n",
      "epoch 84 train_loss : 0 , val_loss : 0.4618387222290039,p 0.45170135859404215, r 0.8, f 0.5773918585198757\n",
      "\n",
      "85\n",
      "epoch 85 train_loss : 0 , val_loss : 0.292233407497406,p 0.7784333005571944, r 0.5242825607064018, f 0.6265664160401003\n",
      "\n",
      "86\n",
      "epoch 86 train_loss : 0 , val_loss : 0.3571940064430237,p 0.5567703952901598, r 0.7306843267108167, f 0.631980906921241\n",
      "\n",
      "87\n",
      "epoch 87 train_loss : 0 , val_loss : 0.3213856816291809,p 0.626487439400617, r 0.6275938189845475, f 0.6270401411557124\n",
      "\n",
      "88\n",
      "epoch 88 train_loss : 0 , val_loss : 0.3755079209804535,p 0.5426277122842986, r 0.7011037527593819, f 0.6117692381777906\n",
      "\n",
      "89\n",
      "epoch 89 train_loss : 0 , val_loss : 0.4770289957523346,p 0.4424145167263301, r 0.7911699779249448, f 0.5674926767476843\n",
      "\n",
      "90\n",
      "epoch 90 train_loss : 0 , val_loss : 0.42306503653526306,p 0.4898274168654413, r 0.7706401766004415, f 0.5989534185467958\n",
      "\n",
      "91\n",
      "epoch 91 train_loss : 0 , val_loss : 0.3564305305480957,p 0.5509868421052632, r 0.739514348785872, f 0.6314797360980207\n",
      "\n",
      "92\n",
      "epoch 92 train_loss : 0 , val_loss : 0.3275505602359772,p 0.620625, r 0.6576158940397351, f 0.6385852090032155\n",
      "\n",
      "93\n",
      "epoch 93 train_loss : 0 , val_loss : 0.4422367215156555,p 0.4855796687607082, r 0.7507726269315673, f 0.5897346974163343\n",
      "\n",
      "94\n",
      "epoch 94 train_loss : 0 , val_loss : 0.42173340916633606,p 0.4951754385964912, r 0.747682119205298, f 0.5957783641160949\n",
      "\n",
      "95\n",
      "epoch 95 train_loss : 0 , val_loss : 0.41523534059524536,p 0.4949509521061743, r 0.7573951434878587, f 0.5986738789042052\n",
      "\n",
      "96\n",
      "epoch 96 train_loss : 0 , val_loss : 0.40847641229629517,p 0.4992048575972242, r 0.7622516556291391, f 0.6033021752424216\n",
      "\n",
      "97\n",
      "epoch 97 train_loss : 0 , val_loss : 0.40212374925613403,p 0.49821505069256034, r 0.7701986754966887, f 0.6050463886239487\n",
      "\n",
      "98\n",
      "epoch 98 train_loss : 0 , val_loss : 0.5411638021469116,p 0.40419008686765456, r 0.8730684326710817, f 0.5525672371638142\n",
      "\n",
      "99\n",
      "epoch 99 train_loss : 0 , val_loss : 0.3469853103160858,p 0.5584974350488168, r 0.7450331125827815, f 0.6384186134493521\n",
      "\n"
     ]
    }
   ],
   "source": [
    "global weight_dir\n",
    "weight_dir= conf['dataset']['weight_path']##weight 저장\n",
    "model_epoch = conf['trn_args']['epoch']\n",
    "\n",
    "# dataset=weight_dir+'best'\n",
    "# checkpoint=torch.load(dataset,map_location='cuda:0')\n",
    "# model.load_state_dict(checkpoint)\n",
    "\n",
    "if not os.path.exists(weight_dir):\n",
    "    os.makedirs(weight_dir)\n",
    "with open(weight_dir+'train_result','a') as f:\n",
    "    f.write('=====result=======\\n')\n",
    "f1_best=0\n",
    "for epoch in range(model_epoch):\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    print(epoch)\n",
    "    model.train()\n",
    "    for i, (g,inputs,labels) in enumerate(train_loader):\n",
    "        inputs=inputs.float()\n",
    "        inputs=inputs.cuda()\n",
    "        labels=labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        out,_=model(inputs)\n",
    "        out=out.cuda()\n",
    "        loss=criterion(out,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    model.eval()\n",
    "    val_losses=AverageMeter()\n",
    "    acc=0\n",
    "    gt_sum=0\n",
    "    tp_sum=0\n",
    "    fp_sum=0\n",
    "    fn_sum=0\n",
    "    acc=0\n",
    "    sum=0\n",
    "    pred_sum=0\n",
    "    \n",
    "    \n",
    "    with open(weight_dir+'train_result','a') as f:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for it, (g,inputs,labels) in enumerate(val_loader):\n",
    "                inputs=inputs.float()\n",
    "                inputs=inputs.cuda()\n",
    "                labels=labels.cuda()\n",
    "                out,_=model(inputs)\n",
    "                out=out.cuda()\n",
    "                loss=criterion(out,labels)\n",
    "                val_losses.update(loss,labels.size(0))\n",
    "                TP,FP,TN,FN,pred_len, gt_len,pred=fmeasure(out.cpu(),labels.cpu())\n",
    "                tp_sum += TP\n",
    "                fp_sum += FP\n",
    "                fn_sum += FN\n",
    "                pred_sum += pred_len\n",
    "                gt_sum += gt_len\n",
    "                acc=acc+TP+TN\n",
    "                sum+=len(out)\n",
    "            if tp_sum>0 and fp_sum>0 and fn_sum>0:\n",
    "                precision = tp_sum/(tp_sum+fp_sum)\n",
    "                recall = tp_sum / (tp_sum+fn_sum)\n",
    "                f1 = (2*precision*recall / (precision + recall))\n",
    "                accuracy=acc/sum\n",
    "                print(\"epoch {} train_loss : {} , val_loss : {},p {}, r {}, f {}\\n\".format(epoch,losses.avg,val_losses.avg,precision,recall,f1))\n",
    "                f.write(\"epoch {} train_loss : {} , val_loss : {},p {}, r {}, f {}\\n\".format(epoch,losses.avg,val_losses.avg,precision,recall,f1))\n",
    "                torch.save(model.state_dict(),'{}'.format(weight_dir+str(epoch)+\"train\"))\n",
    "                if f1_best<f1:\n",
    "                    f.write(\"== best epoch {} train_loss : {} , val_loss : {},p {}, r {}, f {}\\n\".format(epoch,losses.avg,val_losses.avg,precision,recall,f1))\n",
    "                    torch.save(model.state_dict(),'{}'.format(weight_dir+\"best\"))\n",
    "                    f1_best=f1\n",
    "\n",
    "            else:\n",
    "                print(\"epoch {} train_loss : {} , val_loss : {},p {}, r {}, f {}\\n\".format(epoch,losses.avg,val_losses.avg,0,0,0))\n",
    "                f.write(\"epoch {} train_loss : {} , val_loss : {},p {}, r {}, f {}\\n\".format(epoch,losses.avg,val_losses.avg,0,0,0))\n",
    "                torch.save(model.state_dict(),'{}'.format(weight_dir+str(epoch)+\"train\"))                \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load fin\n",
      "102844212431058132\n",
      "precision : 0.8393782383419689, recall : 0.4251968503937008, f1 : 0.5644599303135889, accuracy : 0.898538961038961\n",
      "102844341902586509\n",
      "precision : 0.5669099756690997, recall : 0.9209486166007905, f1 : 0.7018072289156627, accuracy : 0.9073900841908326\n",
      "102844401152267937\n",
      "precision : 0.3132780082987552, recall : 0.3756218905472637, f1 : 0.3416289592760181, accuracy : 0.745740498034076\n",
      "102844212430927059\n",
      "precision : 0.7455089820359282, recall : 0.7545454545454545, f1 : 0.7500000000000001, accuracy : 0.9127004996055745\n",
      "102844412708953395\n",
      "precision : 0.8027522935779816, recall : 0.6340579710144928, f1 : 0.708502024291498, accuracy : 0.9299610894941635\n",
      "102844212429944013\n",
      "precision : 0.7, recall : 0.5094339622641509, f1 : 0.5897035881435257, accuracy : 0.8714565004887586\n",
      "102844341912679064\n",
      "precision : 0.39552238805970147, recall : 0.9380530973451328, f1 : 0.5564304461942257, accuracy : 0.8257731958762886\n",
      "102844235753749959\n",
      "precision : 0.3148614609571788, recall : 0.31806615776081426, f1 : 0.3164556962025316, accuracy : 0.7563176895306859\n",
      "102844341908026005\n",
      "precision : 0.6630581867388363, recall : 0.7435508345978755, f1 : 0.7010014306151645, accuracy : 0.8567020911895783\n",
      "102844283023206486\n",
      "precision : 0.7987987987987988, recall : 0.7450980392156863, f1 : 0.7710144927536231, accuracy : 0.9137554585152838\n",
      "102844224147717245\n",
      "precision : 0.9155555555555556, recall : 0.6645161290322581, f1 : 0.7700934579439251, accuracy : 0.9329334787350054\n",
      "102844412704890154\n",
      "precision : 0.8138686131386861, recall : 0.7263843648208469, f1 : 0.7676419965576591, accuracy : 0.9349397590361446\n",
      "102844212430599377\n",
      "precision : 0.6352941176470588, recall : 0.6864406779661016, f1 : 0.6598778004073319, accuracy : 0.9284796573875803\n",
      "102844412711443769\n",
      "precision : 0.643312101910828, recall : 0.8938053097345132, f1 : 0.7481481481481481, accuracy : 0.839622641509434\n",
      "102844235747982779\n",
      "precision : 0.7350069735006973, recall : 0.7268965517241379, f1 : 0.7309292649098474, accuracy : 0.8577712609970675\n",
      "==precision : 0.6588737129487383, recall : 0.6708410605042147, f1 : 0.6451796309781833, accuracy : 0.874138857708629\n"
     ]
    }
   ],
   "source": [
    "def fmeasure2(frames,label): ##measure def for test\n",
    "    average = [0,0,0,0,0]\n",
    "    for key in frames.keys():\n",
    "        TP = len(np.where((np.array(frames[key])==1)&(label[key]==1)==True)[0])\n",
    "        FP = len(np.where((np.array(frames[key])==1)&(label[key]==0)==True)[0])\n",
    "        TN = len(np.where((np.array(frames[key])==0)&(label[key]==0)==True)[0])\n",
    "        FN = len(np.where((np.array(frames[key])==0)&(label[key]==1)==True)[0])\n",
    "        precision = TP/(TP+FP)\n",
    "        recall = TP/(TP+FN)\n",
    "        accuracy = (TP+TN)/(TP+FN+FP+TN)\n",
    "        if precision==0 and recall == 0:\n",
    "            print('!')\n",
    "        else:\n",
    "            f1 = (2*precision*recall / (precision + recall))\n",
    "            print(key)\n",
    "            print('precision : {}, recall : {}, f1 : {}, accuracy : {}'.format(precision,recall,f1,accuracy))\n",
    "            average[0]+= precision\n",
    "            average[1]+= recall\n",
    "            average[2]+= f1\n",
    "            average[3]+= accuracy\n",
    "            average[4]+=1\n",
    "    print('==precision : {}, recall : {}, f1 : {}, accuracy : {}'.format(average[0]/average[4],average[1]/average[4],average[2]/average[4],average[3]/average[4]))\n",
    "\n",
    "\n",
    "test=Mul_data('test')\n",
    "test_loader=torch.utils.data.DataLoader(test,batch_size=conf['trn_args']['val_bs'])\n",
    "dataset=weight_dir+'best'\n",
    "checkpoint=torch.load(dataset,map_location='cuda:0')\n",
    "model.load_state_dict(checkpoint)\n",
    "model.eval()\n",
    "pred_sum = 0#model output\n",
    "gt_sum = 0#label\n",
    "tp_sum=0\n",
    "fp_sum=0\n",
    "fn_sum=0\n",
    "acc=0\n",
    "sum=0\n",
    "result={}\n",
    "\n",
    "\n",
    "#evaluation\n",
    "with torch.no_grad():\n",
    "    for it, (game_id,inputs,labels) in enumerate(test_loader):\n",
    "        inputs=inputs.float()\n",
    "        labels=labels\n",
    "        output,_=model(inputs)\n",
    "        TP,FP,TN,FN,pred_len, gt_len,pred=fmeasure(output.cpu(),labels.cpu())\n",
    "        for idx,g in enumerate(game_id):\n",
    "            if g not in result.keys():\n",
    "                result[g]=pred[idx].tolist()\n",
    "            else:\n",
    "                result[g]+=pred[idx].tolist()\n",
    "with open(conf['dataset']['label_path'],\"rb\") as f4:  \n",
    "    real_result=pickle.load(f4)\n",
    "fmeasure2(result,real_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "47cfe0b77a86444241c9d26f8eb452e44deb4d4dd9b2830dc549158d4e6f39d5"
  },
  "kernelspec": {
   "display_name": "mm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
