{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import string\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "import math\n",
    "import torch.utils.data as data\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy\n",
    "import torch.utils.data.sampler as sampler\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/video_statistic_features_one2.pickle','rb') as f2:\n",
    "    temp_data = pickle.load(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    if epoch<20:\n",
    "        lr = 0.01\n",
    "    else:\n",
    "        lr = 0.001\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model\n",
    "output, feature를 한번에 뽑음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size=1\n",
    "hidden_size=128\n",
    "num_layers=3\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._clf1 = nn.LSTM(input_size, hidden_size,3,batch_first=True)\n",
    "        self._lin = nn.Sequential(nn.Linear(hidden_size, hidden_size),\n",
    "                                 nn.Linear(hidden_size,2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=x.cuda()\n",
    "        hidden = Variable(torch.zeros(num_layers,x.size(0),hidden_size)).cuda() # (num_layers * num_directions, batch, hidden_size)\n",
    "        cell = Variable(torch.zeros(num_layers,x.size(0),hidden_size)).cuda() # (num_layers * num_directions, batch, hidden_size)        out,hidden = self._clf1(x,h0)\n",
    "        out,hidden = self._clf1(x,(hidden,cell))#batch*7*3\n",
    "        ft = out[:,-1,:]\n",
    "        out = self._lin(out[:,-1,:])\n",
    "        return out,ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 준비\n",
    "1. d_type : train, val, test 를 받음\n",
    "2. 7초 window size shift 진행 --> *** 이부분 수정해야함.***\n",
    "3. 하이라이트 구간 25%추출 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "class data_ds(data.Dataset):\n",
    "    def __init__(self,d_type):\n",
    "        self.gt_range =  1-0.25\n",
    "        self.d_type=d_type\n",
    "        \n",
    "        #label load\n",
    "        with open('label/label.pickle','rb') as f1:  \n",
    "            self.gt=pickle.load(f1)\n",
    "        \n",
    "        #원본 데이터로드\n",
    "        with open('./data/video_raw_feature_no_smoothing.pickle','rb') as f2:\n",
    "            temp_data = pickle.load(f2)\n",
    "            if temp_data['102844412722519367'][0] is not list:\n",
    "                for i in temp_data:\n",
    "                    temp_data[i] = np.array(temp_data[i])[:,np.newaxis]\n",
    "            self.data= temp_data\n",
    "        if d_type=='train':\n",
    "            self.sample = ['102844412722519367','102844212429550795','102844401151219358','102844401154430631','102844412717014335','102844401153971877','102844224148503678','102844412722847048','102844401152857762','102844412707380528','102844212431516886','102844283027925085','102844412716227901','102844412710001974','102844294670878922','102844294670551241','102844283023599703','102844412704496937','102844235751783874','102844401152071328','102844412709674293','102844401153447587','102844224148896895','102844235746868664','102979081290790284','102844283027531868','102844212431975640','102844401155937960','102844212429092040','102844341906649746','102844412706987311','102844412721339716','102844212430402768','102844341905011343','102844235753356742','102844235750997440','102844412709346612','102844412705217835','102844235752963525','102844412712164667','102844412705545516','102844341912220311','102844341907370644','102844235749424575','102844212429419722','102844294669568199','102844212431779031','102844294666422466','102844224146472059','102844212428895431','102844212429747404','102844235748703677','102844224146930812','102844212430730450','102844294674876621','102844341909598870','102844283020453971','102844294670026952','102844412723174729','102844341904683662','102844283025696858','102844235747261881','102844401154168486','102844235748310460','102844412711836986','102844412723567946','102844235749031358','102844294674286796','102844294666881219','102844412716686654']\n",
    "        if d_type=='val':\n",
    "            self.sample = ['102844294671796427','102844224145685626','102844412717407552','102844235751390657','102844401156069033','102904869420860038','102910307641576395','102844341905404560','102844341906977427','102844212430075086','102844412711116088','102844401153578660','102844294667405508','102844412706659630']\n",
    "        if d_type=='test':\n",
    "            self.sample = ['102844212431058132','102844341902586509','102844401152267937','102844212430927059','102844412708953395','102844212429944013','102844341912679064','102844235753749959','102844341908026005','102844283023206486','102844224147717245','102844412704890154','102844212430599377','102844412711443769','102844235747982779']\n",
    "        if d_type == 'total':\n",
    "            self.sample = ['102844412722519367','102844212429550795','102844401151219358','102844401154430631','102844412717014335','102844401153971877','102844224148503678','102844412722847048','102844401152857762','102844412707380528','102844212431516886','102844283027925085','102844412716227901','102844412710001974','102844294670878922','102844294670551241','102844283023599703','102844412704496937','102844235751783874','102844401152071328','102844412709674293','102844401153447587','102844224148896895','102844235746868664','102979081290790284','102844283027531868','102844212431975640','102844401155937960','102844212429092040','102844341906649746','102844412706987311','102844412721339716','102844212430402768','102844341905011343','102844235753356742','102844235750997440','102844412709346612','102844412705217835','102844235752963525','102844412712164667','102844412705545516','102844341912220311','102844341907370644','102844235749424575','102844212429419722','102844294669568199','102844212431779031','102844294666422466','102844224146472059','102844212428895431','102844212429747404','102844235748703677','102844224146930812','102844212430730450','102844294674876621','102844341909598870','102844283020453971','102844294670026952','102844412723174729','102844341904683662','102844283025696858','102844235747261881','102844401154168486','102844235748310460','102844412711836986','102844412723567946','102844235749031358','102844294674286796','102844294666881219','102844412716686654','102844294671796427','102844224145685626','102844412717407552','102844235751390657','102844401156069033','102904869420860038','102910307641576395','102844341905404560','102844341906977427','102844212430075086','102844412711116088','102844401153578660','102844294667405508','102844412706659630','102844212431058132','102844341902586509','102844401152267937','102844212430927059','102844412708953395','102844212429944013','102844341912679064','102844235753749959','102844341908026005','102844283023206486','102844224147717245','102844412704890154','102844212430599377','102844412711443769','102844235747982779']\n",
    "\n",
    "        \n",
    "        #sampling 대상이 될 데이터 저장 (25% 구간때문에 진행)\n",
    "        self.WeightedSampling=[]\n",
    "        for i in self.sample:\n",
    "            self.WeightedSampling.extend(copy.copy(self.gt[str(i)]))\n",
    "        \n",
    "        sampling = np.array(self.WeightedSampling)\n",
    "        neg_idx = np.where(sampling == 0)[0] #general\n",
    "        pos_idx = np.where(sampling == 1)[0] #highlight\n",
    "        sampling = sampling.astype(np.float32)\n",
    "\n",
    "        #구간 구분하면서, 25% 만 추출 및 weightedsampling 에 저장\n",
    "        begin_pos = 0 \n",
    "        hl_frames = []\n",
    "        for it, cur_pos in enumerate(pos_idx):\n",
    "            if it+1 < len(pos_idx): \n",
    "                if((pos_idx[it+1] - cur_pos) > 1):#cur_pos와 cur_pos+1 간격이 1보다 크면, 즉 다른 구간이면\n",
    "                    begin = int((it+1 - begin_pos) * self.gt_range) + begin_pos\n",
    "                    hl_frames.extend( pos_idx[begin: it] ) #한구간의 하이라이트 25%만 사용하겠다.\n",
    "                    begin_pos = it+1\n",
    "\n",
    "        #sampling 비율 맞춰줌\n",
    "        sampling.fill(0)\n",
    "        sampling[neg_idx] = len(sampling) / float(len(neg_idx))\n",
    "       # self.WeightedSampling[pos_idx] = len(self.WeightedSampling) / float(len(pos_idx))\n",
    "        sampling[hl_frames] = len(sampling) / float(len(hl_frames))\n",
    "        self.WeightedSampling = sampling\n",
    "        \n",
    "        #sum : 각 경기당 데이터수 누적으로 저장 -->지금 어디 경기인지, 몇번째 프레임인지 확인할때 사용\n",
    "        self.sum=np.insert(np.cumsum([len(self.gt[str(i)]) for i in self.sample]),0,0)\n",
    "        print(\"data load fin\")\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.WeightedSampling)\n",
    "    def __getitem__(self,index):\n",
    "            global input_size\n",
    "            #누적 데이터수 - index 로 프레임과 경기를 찾음\n",
    "            vid=np.histogram(index,self.sum)\n",
    "            vid = np.where(vid[0]>0)[0][0]\n",
    "            vframe=index-self.sum[vid]\n",
    "            game_id=str(self.sample[vid])\n",
    "\n",
    "            window=[]#batch*7(window size)*3(highlight result)\n",
    "            for idx in range(7): #7 : window size\n",
    "                s_window=[]\n",
    "                if vframe+idx<len(self.data[game_id]):\n",
    "                    s_window=(self.data[game_id][vframe+idx])#vframe의 image\n",
    "                else:\n",
    "                    #s_window=[0,0,0]#padding value\n",
    "                    s_window=[0]*input_size # input_size\n",
    "                window.append(s_window)\n",
    "                \n",
    "\n",
    "\n",
    "            #label 값 받기\n",
    "            label=self.gt[str(game_id)][vframe]\n",
    "            \n",
    "            \n",
    "            return np.array(window),label,str(game_id)\n",
    "        \n",
    "            \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 경기별 score 평균\n",
    "def fmeasure2(frames,label):\n",
    "    average = [0,0,0,0,0]\n",
    "    for key in frames.keys():\n",
    "        TP = len(np.where((np.array(frames[key])==1)&(label[key]==1)==True)[0])\n",
    "        FP = len(np.where((np.array(frames[key])==1)&(label[key]==0)==True)[0])\n",
    "        TN = len(np.where((np.array(frames[key])==0)&(label[key]==0)==True)[0])\n",
    "        FN = len(np.where((np.array(frames[key])==0)&(label[key]==1)==True)[0])\n",
    "        precision = TP/(TP+FP)\n",
    "        recall = TP/(TP+FN)\n",
    "        accuracy = (TP+TN)/(TP+FN+FP+TN)\n",
    "        print(precision,recall,accuracy)\n",
    "        if precision==0 and recall == 0:\n",
    "            print('error')\n",
    "        else:\n",
    "            f1 = (2*precision*recall / (precision + recall))\n",
    "            print(key)\n",
    "            print('precision : {}, recall : {}, f1 : {}, accuracy : {}'.format(precision,recall,f1,accuracy))\n",
    "            average[0]+= precision\n",
    "            average[1]+= recall\n",
    "            average[2]+= f1\n",
    "            average[3]+= accuracy\n",
    "            average[4]+=1\n",
    "    print('==precision : {}, recall : {}, f1 : {}, accuracy : {}'.format(average[0]/average[4],average[1]/average[4],average[2]/average[4],average[3]/average[4]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 전체 데이터에 대해서 score\n",
    "def fmeasure(output, target):\n",
    "    _, pred = output.topk(1, 1, True, True)\n",
    "    pred = pred.view(-1,1)\n",
    "    target = target.view(-1,1)\n",
    "\n",
    "    #overlap = ((pred== 1) + (target == 1)).gt(1)\n",
    "    #overlap = overlap.view(-1,1)\n",
    "    TP = len(np.where((pred==1)&(target==1)==True)[0]) # True positive\n",
    "    FP = len(np.where((pred==1)&(target==0)==True)[0]) # Condition positive = TP + FN\n",
    "    TN = len(np.where((pred==0)&(target==0)==True)[0])\n",
    "    FN = len(np.where((pred==0)&(target==1)==True)[0])\n",
    "\n",
    "    \n",
    "    #overlap_len = overlap.data.long().sum()\n",
    "    pred_len = pred.data.long().sum()\n",
    "    gt_len   =  target.data.long().sum()\n",
    "\n",
    "    return TP,FP,TN,FN,pred_len, gt_len,pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset load\n",
    "train : undersampling 진행,  validation : 그냥 평등하게 sampling (sequential sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SampleSequentialSampler(sampler.Sampler):\n",
    "    \"\"\"Samples elements sequentially, always in the same order.\n",
    "    Arguments:\n",
    "        data_source (Dataset): dataset to sample from\n",
    "        offset (int): offset between the samples\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_source, offset=10):\n",
    "        self.num_samples = len(data_source) \n",
    "        self.offset = offset\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(np.arange(0, self.num_samples, self.offset ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(np.arange(0, self.num_samples, self.offset ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load fin\n",
      "data load fin\n"
     ]
    }
   ],
   "source": [
    "# train, validation dataset 준비\n",
    "train=data_ds('train')\n",
    "val=data_ds('val')\n",
    "\n",
    "# dataloader with sampling\n",
    "sampler1 = torch.utils.data.sampler.WeightedRandomSampler(weights=train.WeightedSampling.tolist(), num_samples=40000)\n",
    "train_loader=torch.utils.data.DataLoader(train,batch_size=32,sampler=sampler1)\n",
    "sampler2 =  SampleSequentialSampler(val, 30)\n",
    "val_loader=torch.utils.data.DataLoader(val,batch_size=32,sampler= sampler2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.13377275],\n",
       "        [0.02945322],\n",
       "        [0.        ],\n",
       "        [0.05890644],\n",
       "        [0.02945322],\n",
       "        [0.04108113],\n",
       "        [0.        ]]), 0.0, '102844401153971877')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model load 및 실험\n",
    "\n",
    "- weight_dir/train_result 에 모든 결과가 저장됨.\n",
    "- fmeasure (전체 데이터에 대한 score 로 weight를 저장..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_dir='./video_no_smoothing/'\n",
    "\n",
    "###### model load #####\n",
    "model=LSTM().cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "epoch 0 train_loss : 0.6937623023986816 , val_loss : 0.7295051217079163, val_acc : 0.17386363636363636\n",
      "1\n",
      "epoch 1 train_loss : 0.6937437057495117 , val_loss : 0.6864364743232727, val_acc : 0.8261363636363637\n",
      "2\n",
      "epoch 2 train_loss : 0.6935272216796875 , val_loss : 0.6723896265029907, val_acc : 0.8261363636363637\n",
      "3\n",
      "epoch 3 train_loss : 0.693342387676239 , val_loss : 0.6737912893295288, val_acc : 0.8261363636363637\n",
      "4\n",
      "epoch 4 train_loss : 0.6927903890609741 , val_loss : 0.7085973024368286, val_acc : 0.17386363636363636\n",
      "5\n",
      "epoch 5 train_loss : 0.6627776026725769 , val_loss : 0.5448241829872131, val_acc : 0.875\n",
      "6\n",
      "epoch 6 train_loss : 0.5881031155586243 , val_loss : 0.5220286250114441, val_acc : 0.8931818181818182\n",
      "7\n",
      "epoch 7 train_loss : 0.5785117745399475 , val_loss : 0.5099902153015137, val_acc : 0.8875\n",
      "8\n",
      "epoch 8 train_loss : 0.5685471296310425 , val_loss : 0.5162443518638611, val_acc : 0.8897727272727273\n",
      "9\n",
      "epoch 9 train_loss : 0.5634499788284302 , val_loss : 0.5143837928771973, val_acc : 0.865909090909091\n",
      "10\n",
      "epoch 10 train_loss : 0.5570893883705139 , val_loss : 0.5335668921470642, val_acc : 0.8852272727272728\n",
      "11\n",
      "epoch 11 train_loss : 0.5568934082984924 , val_loss : 0.5522896647453308, val_acc : 0.8943181818181818\n",
      "12\n",
      "epoch 12 train_loss : 0.5504440069198608 , val_loss : 0.5110070705413818, val_acc : 0.8920454545454546\n",
      "13\n",
      "epoch 13 train_loss : 0.5486326813697815 , val_loss : 0.5459982752799988, val_acc : 0.8920454545454546\n",
      "14\n",
      "epoch 14 train_loss : 0.5517928600311279 , val_loss : 0.5267215967178345, val_acc : 0.8909090909090909\n",
      "15\n",
      "epoch 15 train_loss : 0.5475262403488159 , val_loss : 0.4925471544265747, val_acc : 0.8988636363636363\n",
      "16\n",
      "epoch 16 train_loss : 0.5450739860534668 , val_loss : 0.44834452867507935, val_acc : 0.8909090909090909\n",
      "17\n",
      "epoch 17 train_loss : 0.5496467351913452 , val_loss : 0.5316076874732971, val_acc : 0.8931818181818182\n",
      "18\n",
      "epoch 18 train_loss : 0.5458340644836426 , val_loss : 0.5451698303222656, val_acc : 0.9011363636363636\n",
      "19\n",
      "epoch 19 train_loss : 0.5471158027648926 , val_loss : 0.5587776899337769, val_acc : 0.8909090909090909\n",
      "20\n",
      "epoch 20 train_loss : 0.5818617939949036 , val_loss : 0.5308083891868591, val_acc : 0.8977272727272727\n",
      "21\n",
      "epoch 21 train_loss : 0.5652276873588562 , val_loss : 0.5115970969200134, val_acc : 0.9\n",
      "22\n",
      "epoch 22 train_loss : 0.5454189777374268 , val_loss : 0.4675458073616028, val_acc : 0.8965909090909091\n",
      "23\n",
      "epoch 23 train_loss : 0.5390881896018982 , val_loss : 0.45855987071990967, val_acc : 0.9011363636363636\n",
      "24\n",
      "epoch 24 train_loss : 0.529617965221405 , val_loss : 0.47498002648353577, val_acc : 0.9022727272727272\n",
      "25\n",
      "epoch 25 train_loss : 0.534656822681427 , val_loss : 0.46722593903541565, val_acc : 0.9022727272727272\n",
      "26\n",
      "epoch 26 train_loss : 0.5403904914855957 , val_loss : 0.4848726987838745, val_acc : 0.8420454545454545\n",
      "27\n",
      "epoch 27 train_loss : 0.5353855490684509 , val_loss : 0.45684686303138733, val_acc : 0.9034090909090909\n",
      "28\n",
      "epoch 28 train_loss : 0.5336297154426575 , val_loss : 0.45738163590431213, val_acc : 0.9011363636363636\n",
      "29\n",
      "epoch 29 train_loss : 0.5323336124420166 , val_loss : 0.4492748975753784, val_acc : 0.9011363636363636\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(weight_dir):\n",
    "    os.makedirs(weight_dir)\n",
    "optimizer = torch.optim.SGD(model.parameters(), 0.01,momentum=0.9,weight_decay=1e-4)\n",
    "# scheduler = lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "with open(weight_dir+'train_result','a') as f:\n",
    "\n",
    "\n",
    "    best_losses=1000000\n",
    "    for epoch in range(30):\n",
    "        lr = adjust_learning_rate(optimizer, epoch)\n",
    "        train_loss=0\n",
    "        print(epoch)\n",
    "        model.train()\n",
    "        for i, (data,labels,game_id) in enumerate(train_loader):\n",
    "            inputs = Variable(data).float().cuda()\n",
    "            labels = Variable(labels).cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output,_=model(inputs)\n",
    "\n",
    "            loss=criterion(output,labels.long())\n",
    "            train_loss+=loss\n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.) #rnn 계열 gradient 장치\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        #validation\n",
    "        model.eval()\n",
    "        val_loss=0\n",
    "        acc=0\n",
    "        gt_sum=0\n",
    "        val_result ={}\n",
    "        with open(weight_dir+'train_result','a') as f:\n",
    "            with torch.no_grad():\n",
    "                for it, (data,labels,game_id) in enumerate(val_loader):\n",
    "                    inputs = Variable(data).float().cuda()\n",
    "                    labels = Variable(labels).cuda()\n",
    "                    output,_=model(inputs)\n",
    "                    loss=criterion(output,labels.long())\n",
    "                    val_loss+=loss\n",
    "                    TP,FP,TN,FN,pred_len, gt_len,pred=fmeasure(output.cpu(),labels.cpu())\n",
    "                    acc=acc+TP+TN\n",
    "                    gt_sum+=len(output)\n",
    "                val_acc=acc/gt_sum\n",
    "                print(\"epoch {} train_loss : {} , val_loss : {}, val_acc : {}\".format(epoch,train_loss/len(train_loader),val_loss/len(val_loader),val_acc))\n",
    "                f.write(\"epoch {} train_loss : {} , val_loss : {}, val_acc : {}\\n\".format(epoch,train_loss/len(train_loader),val_loss/len(val_loader),val_acc))\n",
    "                if best_losses>val_loss:\n",
    "                    best_losses=val_loss\n",
    "                    torch.save(model.state_dict(),'{}'.format(weight_dir+str(epoch)+\"train_best\"))#epoch 변화 확인위해\n",
    "                    torch.save(model.state_dict(),'{}'.format(weight_dir+\"train_best\"))#train best 덮어써짐\n",
    "\n",
    "                    f.write(\"epoch {} saved\\n\".format(epoch))\n",
    "                else:\n",
    "                    torch.save(model.state_dict(),'{}'.format(weight_dir+str(epoch)+\"train\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "- test set 로드\n",
    "- best weight load\n",
    "- weight_dir/train_result 에 결과 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load fin\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './video_no_smoothing/train_best'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-e0b683ffae4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#test dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'train_best'\u001b[0m \u001b[0;31m#best weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#model weight load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yes/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yes/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yes/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './video_no_smoothing/train_best'"
     ]
    }
   ],
   "source": [
    "#test\n",
    "\n",
    "\n",
    "test=data_ds('test')#test dataset\n",
    "test_loader=torch.utils.data.DataLoader(test,batch_size=32)#test dataloader\n",
    "dataset=weight_dir+'train_best' #best weight\n",
    "checkpoint=torch.load(dataset,map_location='cuda:0')#model weight load\n",
    "model.load_state_dict(checkpoint)\n",
    "model.cuda()\n",
    "\n",
    "#test exp\n",
    "model.eval()\n",
    "pred_sum = 0#model output\n",
    "gt_sum = 0#label\n",
    "tp_sum=0\n",
    "fp_sum=0\n",
    "fn_sum=0\n",
    "acc=0\n",
    "sum=0\n",
    "result={}\n",
    "with torch.no_grad():\n",
    "    for it, (data,labels,g) in enumerate(test_loader):\n",
    "        inputs = data.float().cuda()\n",
    "        labels = labels.cuda()\n",
    "        output,_=model(inputs)\n",
    "\n",
    "        TP,FP,TN,FN,pred_len, gt_len,pred=fmeasure(output.cpu(),labels.cpu())\n",
    "        for idx,game_id in enumerate(g):\n",
    "            if game_id not in result.keys():\n",
    "                result[game_id]=pred[idx].tolist()\n",
    "            else:\n",
    "                result[game_id]+=pred[idx].tolist()\n",
    "        print(TP,FP,TN,FN,pred_len, gt_len)\n",
    "        tp_sum += TP\n",
    "        fp_sum += FP\n",
    "        fn_sum += FN\n",
    "        pred_sum += pred_len\n",
    "        gt_sum += gt_len\n",
    "        acc=acc+TP+TN\n",
    "        sum+=len(output)\n",
    "    with open(weight_dir+'/train_result','a') as f:\n",
    "        if tp_sum>0 and fp_sum>0 and fn_sum>0:\n",
    "            precision = tp_sum/(tp_sum+fp_sum)\n",
    "            recall = tp_sum / (tp_sum+fn_sum)\n",
    "            f1 = (2*precision*recall / (precision + recall)) * 100\n",
    "            accuracy=acc/sum\n",
    "            print( tp_sum, fp_sum, fn_sum)\n",
    "            print('[{}/{}], prec:{}, recall:{}, f1:{}, acc: {}'.format(it, len(test_loader), precision, recall, f1,accuracy))\n",
    "            f.write('{}, prec:{}, recall:{}, f1:{}, acc : {}\\n'.format(dataset, precision, recall, f1,accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#이게 진짜 metric임 (경기별 평균)\n",
    "with open('./label/label.pickle',\"rb\") as f4:  \n",
    "    real_result=pickle.load(f4)\n",
    "fmeasure2(result,real_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature 추출 (마지막 LSTM feature)\n",
    "- test, train, validation 모두 진행해야함!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load fin\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for LSTM:\n\tWhile copying the parameter named \"_clf1.weight_ih_l0\", whose dimensions in the model are torch.Size([512, 1]) and whose dimensions in the checkpoint are torch.Size([512, 1]), an exception occured : ('CUDA error: invalid resource handle',).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-0bb19323cb68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'train_best'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yes/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1045\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1046\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for LSTM:\n\tWhile copying the parameter named \"_clf1.weight_ih_l0\", whose dimensions in the model are torch.Size([512, 1]) and whose dimensions in the checkpoint are torch.Size([512, 1]), an exception occured : ('CUDA error: invalid resource handle',)."
     ]
    }
   ],
   "source": [
    "save_dir = 'video_raw_feature_no_smoothing_lstm.pickle' #feature 어디에 저장할지\n",
    "\n",
    "test=data_ds('total')\n",
    "test_loader=torch.utils.data.DataLoader(test,batch_size=1)\n",
    "dataset=weight_dir+'train_best'\n",
    "checkpoint=torch.load(dataset,map_location='cuda:0')\n",
    "model.load_state_dict(checkpoint)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "result={}\n",
    "with torch.no_grad():\n",
    "    for it, (data,labels,g) in enumerate(test_loader):\n",
    "        inputs = data.float().cuda()\n",
    "        labels = labels.cuda()\n",
    "        _,output_f=model(inputs) #feature 만 추출\n",
    "\n",
    "        if g[0] not in result.keys():\n",
    "            print(g[0])\n",
    "            result[g[0]]=[(output_f[0]).tolist()]\n",
    "            \n",
    "        else:\n",
    "            result[g[0]]+=[(output_f[0]).tolist()]\n",
    "\n",
    "with open(weight_dir+save_dir,'wb') as f:\n",
    "    pickle.dump(result,f)       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.35791340470314026,\n",
       "  0.26698946952819824,\n",
       "  -0.09512882679700851,\n",
       "  -0.0047071343287825584,\n",
       "  0.1654966175556183,\n",
       "  0.14154905080795288,\n",
       "  -0.3338702917098999,\n",
       "  -0.12367624789476395,\n",
       "  0.18952010571956635,\n",
       "  -0.11053846031427383,\n",
       "  -0.2618904411792755,\n",
       "  -0.11754820495843887,\n",
       "  0.17547045648097992,\n",
       "  -0.03597959131002426,\n",
       "  0.4411907196044922,\n",
       "  0.014064689166843891,\n",
       "  0.01666095107793808,\n",
       "  0.1770620495080948,\n",
       "  0.3251374661922455,\n",
       "  0.003986166324466467,\n",
       "  0.2823714315891266,\n",
       "  -0.27818432450294495,\n",
       "  0.37367361783981323,\n",
       "  0.042778804898262024,\n",
       "  0.28152137994766235,\n",
       "  0.12793771922588348,\n",
       "  0.2897695004940033,\n",
       "  0.007131745107471943,\n",
       "  0.19353102147579193,\n",
       "  -0.2322344034910202,\n",
       "  -0.13017696142196655,\n",
       "  -0.04692450165748596,\n",
       "  0.21807120740413666,\n",
       "  0.21550297737121582,\n",
       "  -0.08499440550804138,\n",
       "  -0.3457574248313904,\n",
       "  -0.3608167767524719,\n",
       "  0.1919105052947998,\n",
       "  -0.18918801844120026,\n",
       "  0.13556288182735443,\n",
       "  0.24390022456645966,\n",
       "  0.23177573084831238,\n",
       "  0.31884250044822693,\n",
       "  -0.10725637525320053,\n",
       "  -0.2860272228717804,\n",
       "  -0.21971635520458221,\n",
       "  -0.29555580019950867,\n",
       "  0.3177889287471771,\n",
       "  -0.11162906140089035,\n",
       "  0.21935592591762543,\n",
       "  0.08060990273952484,\n",
       "  0.06216880679130554,\n",
       "  -0.11729424446821213,\n",
       "  0.21121692657470703,\n",
       "  -0.12110460549592972,\n",
       "  0.43867769837379456,\n",
       "  -0.2540470063686371,\n",
       "  -0.2394268661737442,\n",
       "  -0.0909498929977417,\n",
       "  0.20677007734775543,\n",
       "  -0.22091518342494965,\n",
       "  -0.3037639260292053,\n",
       "  0.30394017696380615,\n",
       "  0.153153657913208,\n",
       "  -0.35198014974594116,\n",
       "  -0.021952742710709572,\n",
       "  -0.23249979317188263,\n",
       "  -0.2581396996974945,\n",
       "  -0.2827187478542328,\n",
       "  0.36873558163642883,\n",
       "  0.3709706664085388,\n",
       "  -0.08481775969266891,\n",
       "  -0.2627639174461365,\n",
       "  0.4583421051502228,\n",
       "  -0.31300434470176697,\n",
       "  0.3708840310573578,\n",
       "  -0.10305589437484741,\n",
       "  0.3000417947769165,\n",
       "  -0.19622379541397095,\n",
       "  -0.2834450900554657,\n",
       "  0.11438899487257004,\n",
       "  -0.257830947637558,\n",
       "  -0.27232030034065247,\n",
       "  0.1493559032678604,\n",
       "  -0.26720958948135376,\n",
       "  -0.3887597322463989,\n",
       "  0.27934569120407104,\n",
       "  0.028316641226410866,\n",
       "  -0.20324905216693878,\n",
       "  0.237822026014328,\n",
       "  0.1887531876564026,\n",
       "  -0.4624069333076477,\n",
       "  0.042809564620256424,\n",
       "  -0.33468934893608093,\n",
       "  -0.1961463987827301,\n",
       "  -0.019646966829895973,\n",
       "  0.13418740034103394,\n",
       "  0.1526598483324051,\n",
       "  0.3831208050251007,\n",
       "  -0.07263857126235962,\n",
       "  0.030955949798226357,\n",
       "  -0.02819211035966873,\n",
       "  -0.11854786425828934,\n",
       "  -0.1984373927116394,\n",
       "  -0.25410041213035583,\n",
       "  -0.10539098083972931,\n",
       "  0.12409134209156036,\n",
       "  0.08357381075620651,\n",
       "  -0.19571514427661896,\n",
       "  0.19625496864318848,\n",
       "  0.18196646869182587,\n",
       "  0.3224068880081177,\n",
       "  0.20778512954711914,\n",
       "  0.23307889699935913,\n",
       "  0.2136409878730774,\n",
       "  0.309150367975235,\n",
       "  0.3247028887271881,\n",
       "  0.22155316174030304,\n",
       "  0.04499324783682823,\n",
       "  0.16452085971832275,\n",
       "  0.030625561252236366,\n",
       "  0.19001346826553345,\n",
       "  0.19127871096134186,\n",
       "  0.2723590135574341,\n",
       "  0.22710856795310974,\n",
       "  0.22784772515296936,\n",
       "  -0.14124195277690887,\n",
       "  0.3209955394268036],\n",
       " [-0.35791340470314026,\n",
       "  0.26698946952819824,\n",
       "  -0.09512882679700851,\n",
       "  -0.0047071343287825584,\n",
       "  0.1654966175556183,\n",
       "  0.14154905080795288,\n",
       "  -0.3338702917098999,\n",
       "  -0.12367624789476395,\n",
       "  0.18952010571956635,\n",
       "  -0.11053846031427383,\n",
       "  -0.2618904411792755,\n",
       "  -0.11754820495843887,\n",
       "  0.17547045648097992,\n",
       "  -0.03597959131002426,\n",
       "  0.4411907196044922,\n",
       "  0.014064689166843891,\n",
       "  0.01666095107793808,\n",
       "  0.1770620495080948,\n",
       "  0.3251374661922455,\n",
       "  0.003986166324466467,\n",
       "  0.2823714315891266,\n",
       "  -0.27818432450294495,\n",
       "  0.37367361783981323,\n",
       "  0.042778804898262024,\n",
       "  0.28152137994766235,\n",
       "  0.12793771922588348,\n",
       "  0.2897695004940033,\n",
       "  0.007131745107471943,\n",
       "  0.19353102147579193,\n",
       "  -0.2322344034910202,\n",
       "  -0.13017696142196655,\n",
       "  -0.04692450165748596,\n",
       "  0.21807120740413666,\n",
       "  0.21550297737121582,\n",
       "  -0.08499440550804138,\n",
       "  -0.3457574248313904,\n",
       "  -0.3608167767524719,\n",
       "  0.1919105052947998,\n",
       "  -0.18918801844120026,\n",
       "  0.13556288182735443,\n",
       "  0.24390022456645966,\n",
       "  0.23177573084831238,\n",
       "  0.31884250044822693,\n",
       "  -0.10725637525320053,\n",
       "  -0.2860272228717804,\n",
       "  -0.21971635520458221,\n",
       "  -0.29555580019950867,\n",
       "  0.3177889287471771,\n",
       "  -0.11162906140089035,\n",
       "  0.21935592591762543,\n",
       "  0.08060990273952484,\n",
       "  0.06216880679130554,\n",
       "  -0.11729424446821213,\n",
       "  0.21121692657470703,\n",
       "  -0.12110460549592972,\n",
       "  0.43867769837379456,\n",
       "  -0.2540470063686371,\n",
       "  -0.2394268661737442,\n",
       "  -0.0909498929977417,\n",
       "  0.20677007734775543,\n",
       "  -0.22091518342494965,\n",
       "  -0.3037639260292053,\n",
       "  0.30394017696380615,\n",
       "  0.153153657913208,\n",
       "  -0.35198014974594116,\n",
       "  -0.021952742710709572,\n",
       "  -0.23249979317188263,\n",
       "  -0.2581396996974945,\n",
       "  -0.2827187478542328,\n",
       "  0.36873558163642883,\n",
       "  0.3709706664085388,\n",
       "  -0.08481775969266891,\n",
       "  -0.2627639174461365,\n",
       "  0.4583421051502228,\n",
       "  -0.31300434470176697,\n",
       "  0.3708840310573578,\n",
       "  -0.10305589437484741,\n",
       "  0.3000417947769165,\n",
       "  -0.19622379541397095,\n",
       "  -0.2834450900554657,\n",
       "  0.11438899487257004,\n",
       "  -0.257830947637558,\n",
       "  -0.27232030034065247,\n",
       "  0.1493559032678604,\n",
       "  -0.26720958948135376,\n",
       "  -0.3887597322463989,\n",
       "  0.27934569120407104,\n",
       "  0.028316641226410866,\n",
       "  -0.20324905216693878,\n",
       "  0.237822026014328,\n",
       "  0.1887531876564026,\n",
       "  -0.4624069333076477,\n",
       "  0.042809564620256424,\n",
       "  -0.33468934893608093,\n",
       "  -0.1961463987827301,\n",
       "  -0.019646966829895973,\n",
       "  0.13418740034103394,\n",
       "  0.1526598483324051,\n",
       "  0.3831208050251007,\n",
       "  -0.07263857126235962,\n",
       "  0.030955949798226357,\n",
       "  -0.02819211035966873,\n",
       "  -0.11854786425828934,\n",
       "  -0.1984373927116394,\n",
       "  -0.25410041213035583,\n",
       "  -0.10539098083972931,\n",
       "  0.12409134209156036,\n",
       "  0.08357381075620651,\n",
       "  -0.19571514427661896,\n",
       "  0.19625496864318848,\n",
       "  0.18196646869182587,\n",
       "  0.3224068880081177,\n",
       "  0.20778512954711914,\n",
       "  0.23307889699935913,\n",
       "  0.2136409878730774,\n",
       "  0.309150367975235,\n",
       "  0.3247028887271881,\n",
       "  0.22155316174030304,\n",
       "  0.04499324783682823,\n",
       "  0.16452085971832275,\n",
       "  0.030625561252236366,\n",
       "  0.19001346826553345,\n",
       "  0.19127871096134186,\n",
       "  0.2723590135574341,\n",
       "  0.22710856795310974,\n",
       "  0.22784772515296936,\n",
       "  -0.14124195277690887,\n",
       "  0.3209955394268036],\n",
       " [-0.35791340470314026,\n",
       "  0.26698946952819824,\n",
       "  -0.09512882679700851,\n",
       "  -0.0047071343287825584,\n",
       "  0.1654966175556183,\n",
       "  0.14154905080795288,\n",
       "  -0.3338702917098999,\n",
       "  -0.12367624789476395,\n",
       "  0.18952010571956635,\n",
       "  -0.11053846031427383,\n",
       "  -0.2618904411792755,\n",
       "  -0.11754820495843887,\n",
       "  0.17547045648097992,\n",
       "  -0.03597959131002426,\n",
       "  0.4411907196044922,\n",
       "  0.014064689166843891,\n",
       "  0.01666095107793808,\n",
       "  0.1770620495080948,\n",
       "  0.3251374661922455,\n",
       "  0.003986166324466467,\n",
       "  0.2823714315891266,\n",
       "  -0.27818432450294495,\n",
       "  0.37367361783981323,\n",
       "  0.042778804898262024,\n",
       "  0.28152137994766235,\n",
       "  0.12793771922588348,\n",
       "  0.2897695004940033,\n",
       "  0.007131745107471943,\n",
       "  0.19353102147579193,\n",
       "  -0.2322344034910202,\n",
       "  -0.13017696142196655,\n",
       "  -0.04692450165748596,\n",
       "  0.21807120740413666,\n",
       "  0.21550297737121582,\n",
       "  -0.08499440550804138,\n",
       "  -0.3457574248313904,\n",
       "  -0.3608167767524719,\n",
       "  0.1919105052947998,\n",
       "  -0.18918801844120026,\n",
       "  0.13556288182735443,\n",
       "  0.24390022456645966,\n",
       "  0.23177573084831238,\n",
       "  0.31884250044822693,\n",
       "  -0.10725637525320053,\n",
       "  -0.2860272228717804,\n",
       "  -0.21971635520458221,\n",
       "  -0.29555580019950867,\n",
       "  0.3177889287471771,\n",
       "  -0.11162906140089035,\n",
       "  0.21935592591762543,\n",
       "  0.08060990273952484,\n",
       "  0.06216880679130554,\n",
       "  -0.11729424446821213,\n",
       "  0.21121692657470703,\n",
       "  -0.12110460549592972,\n",
       "  0.43867769837379456,\n",
       "  -0.2540470063686371,\n",
       "  -0.2394268661737442,\n",
       "  -0.0909498929977417,\n",
       "  0.20677007734775543,\n",
       "  -0.22091518342494965,\n",
       "  -0.3037639260292053,\n",
       "  0.30394017696380615,\n",
       "  0.153153657913208,\n",
       "  -0.35198014974594116,\n",
       "  -0.021952742710709572,\n",
       "  -0.23249979317188263,\n",
       "  -0.2581396996974945,\n",
       "  -0.2827187478542328,\n",
       "  0.36873558163642883,\n",
       "  0.3709706664085388,\n",
       "  -0.08481775969266891,\n",
       "  -0.2627639174461365,\n",
       "  0.4583421051502228,\n",
       "  -0.31300434470176697,\n",
       "  0.3708840310573578,\n",
       "  -0.10305589437484741,\n",
       "  0.3000417947769165,\n",
       "  -0.19622379541397095,\n",
       "  -0.2834450900554657,\n",
       "  0.11438899487257004,\n",
       "  -0.257830947637558,\n",
       "  -0.27232030034065247,\n",
       "  0.1493559032678604,\n",
       "  -0.26720958948135376,\n",
       "  -0.3887597322463989,\n",
       "  0.27934569120407104,\n",
       "  0.028316641226410866,\n",
       "  -0.20324905216693878,\n",
       "  0.237822026014328,\n",
       "  0.1887531876564026,\n",
       "  -0.4624069333076477,\n",
       "  0.042809564620256424,\n",
       "  -0.33468934893608093,\n",
       "  -0.1961463987827301,\n",
       "  -0.019646966829895973,\n",
       "  0.13418740034103394,\n",
       "  0.1526598483324051,\n",
       "  0.3831208050251007,\n",
       "  -0.07263857126235962,\n",
       "  0.030955949798226357,\n",
       "  -0.02819211035966873,\n",
       "  -0.11854786425828934,\n",
       "  -0.1984373927116394,\n",
       "  -0.25410041213035583,\n",
       "  -0.10539098083972931,\n",
       "  0.12409134209156036,\n",
       "  0.08357381075620651,\n",
       "  -0.19571514427661896,\n",
       "  0.19625496864318848,\n",
       "  0.18196646869182587,\n",
       "  0.3224068880081177,\n",
       "  0.20778512954711914,\n",
       "  0.23307889699935913,\n",
       "  0.2136409878730774,\n",
       "  0.309150367975235,\n",
       "  0.3247028887271881,\n",
       "  0.22155316174030304,\n",
       "  0.04499324783682823,\n",
       "  0.16452085971832275,\n",
       "  0.030625561252236366,\n",
       "  0.19001346826553345,\n",
       "  0.19127871096134186,\n",
       "  0.2723590135574341,\n",
       "  0.22710856795310974,\n",
       "  0.22784772515296936,\n",
       "  -0.14124195277690887,\n",
       "  0.3209955394268036],\n",
       " [-0.35791340470314026,\n",
       "  0.26698946952819824,\n",
       "  -0.09512882679700851,\n",
       "  -0.0047071343287825584,\n",
       "  0.1654966175556183,\n",
       "  0.14154905080795288,\n",
       "  -0.3338702917098999,\n",
       "  -0.12367624789476395,\n",
       "  0.18952010571956635,\n",
       "  -0.11053846031427383,\n",
       "  -0.2618904411792755,\n",
       "  -0.11754820495843887,\n",
       "  0.17547045648097992,\n",
       "  -0.03597959131002426,\n",
       "  0.4411907196044922,\n",
       "  0.014064689166843891,\n",
       "  0.01666095107793808,\n",
       "  0.1770620495080948,\n",
       "  0.3251374661922455,\n",
       "  0.003986166324466467,\n",
       "  0.2823714315891266,\n",
       "  -0.27818432450294495,\n",
       "  0.37367361783981323,\n",
       "  0.042778804898262024,\n",
       "  0.28152137994766235,\n",
       "  0.12793771922588348,\n",
       "  0.2897695004940033,\n",
       "  0.007131745107471943,\n",
       "  0.19353102147579193,\n",
       "  -0.2322344034910202,\n",
       "  -0.13017696142196655,\n",
       "  -0.04692450165748596,\n",
       "  0.21807120740413666,\n",
       "  0.21550297737121582,\n",
       "  -0.08499440550804138,\n",
       "  -0.3457574248313904,\n",
       "  -0.3608167767524719,\n",
       "  0.1919105052947998,\n",
       "  -0.18918801844120026,\n",
       "  0.13556288182735443,\n",
       "  0.24390022456645966,\n",
       "  0.23177573084831238,\n",
       "  0.31884250044822693,\n",
       "  -0.10725637525320053,\n",
       "  -0.2860272228717804,\n",
       "  -0.21971635520458221,\n",
       "  -0.29555580019950867,\n",
       "  0.3177889287471771,\n",
       "  -0.11162906140089035,\n",
       "  0.21935592591762543,\n",
       "  0.08060990273952484,\n",
       "  0.06216880679130554,\n",
       "  -0.11729424446821213,\n",
       "  0.21121692657470703,\n",
       "  -0.12110460549592972,\n",
       "  0.43867769837379456,\n",
       "  -0.2540470063686371,\n",
       "  -0.2394268661737442,\n",
       "  -0.0909498929977417,\n",
       "  0.20677007734775543,\n",
       "  -0.22091518342494965,\n",
       "  -0.3037639260292053,\n",
       "  0.30394017696380615,\n",
       "  0.153153657913208,\n",
       "  -0.35198014974594116,\n",
       "  -0.021952742710709572,\n",
       "  -0.23249979317188263,\n",
       "  -0.2581396996974945,\n",
       "  -0.2827187478542328,\n",
       "  0.36873558163642883,\n",
       "  0.3709706664085388,\n",
       "  -0.08481775969266891,\n",
       "  -0.2627639174461365,\n",
       "  0.4583421051502228,\n",
       "  -0.31300434470176697,\n",
       "  0.3708840310573578,\n",
       "  -0.10305589437484741,\n",
       "  0.3000417947769165,\n",
       "  -0.19622379541397095,\n",
       "  -0.2834450900554657,\n",
       "  0.11438899487257004,\n",
       "  -0.257830947637558,\n",
       "  -0.27232030034065247,\n",
       "  0.1493559032678604,\n",
       "  -0.26720958948135376,\n",
       "  -0.3887597322463989,\n",
       "  0.27934569120407104,\n",
       "  0.028316641226410866,\n",
       "  -0.20324905216693878,\n",
       "  0.237822026014328,\n",
       "  0.1887531876564026,\n",
       "  -0.4624069333076477,\n",
       "  0.042809564620256424,\n",
       "  -0.33468934893608093,\n",
       "  -0.1961463987827301,\n",
       "  -0.019646966829895973,\n",
       "  0.13418740034103394,\n",
       "  0.1526598483324051,\n",
       "  0.3831208050251007,\n",
       "  -0.07263857126235962,\n",
       "  0.030955949798226357,\n",
       "  -0.02819211035966873,\n",
       "  -0.11854786425828934,\n",
       "  -0.1984373927116394,\n",
       "  -0.25410041213035583,\n",
       "  -0.10539098083972931,\n",
       "  0.12409134209156036,\n",
       "  0.08357381075620651,\n",
       "  -0.19571514427661896,\n",
       "  0.19625496864318848,\n",
       "  0.18196646869182587,\n",
       "  0.3224068880081177,\n",
       "  0.20778512954711914,\n",
       "  0.23307889699935913,\n",
       "  0.2136409878730774,\n",
       "  0.309150367975235,\n",
       "  0.3247028887271881,\n",
       "  0.22155316174030304,\n",
       "  0.04499324783682823,\n",
       "  0.16452085971832275,\n",
       "  0.030625561252236366,\n",
       "  0.19001346826553345,\n",
       "  0.19127871096134186,\n",
       "  0.2723590135574341,\n",
       "  0.22710856795310974,\n",
       "  0.22784772515296936,\n",
       "  -0.14124195277690887,\n",
       "  0.3209955394268036],\n",
       " [-0.35791340470314026,\n",
       "  0.26698946952819824,\n",
       "  -0.09512882679700851,\n",
       "  -0.0047071343287825584,\n",
       "  0.1654966175556183,\n",
       "  0.14154905080795288,\n",
       "  -0.3338702917098999,\n",
       "  -0.12367624789476395,\n",
       "  0.18952010571956635,\n",
       "  -0.11053846031427383,\n",
       "  -0.2618904411792755,\n",
       "  -0.11754820495843887,\n",
       "  0.17547045648097992,\n",
       "  -0.03597959131002426,\n",
       "  0.4411907196044922,\n",
       "  0.014064689166843891,\n",
       "  0.01666095107793808,\n",
       "  0.1770620495080948,\n",
       "  0.3251374661922455,\n",
       "  0.003986166324466467,\n",
       "  0.2823714315891266,\n",
       "  -0.27818432450294495,\n",
       "  0.37367361783981323,\n",
       "  0.042778804898262024,\n",
       "  0.28152137994766235,\n",
       "  0.12793771922588348,\n",
       "  0.2897695004940033,\n",
       "  0.007131745107471943,\n",
       "  0.19353102147579193,\n",
       "  -0.2322344034910202,\n",
       "  -0.13017696142196655,\n",
       "  -0.04692450165748596,\n",
       "  0.21807120740413666,\n",
       "  0.21550297737121582,\n",
       "  -0.08499440550804138,\n",
       "  -0.3457574248313904,\n",
       "  -0.3608167767524719,\n",
       "  0.1919105052947998,\n",
       "  -0.18918801844120026,\n",
       "  0.13556288182735443,\n",
       "  0.24390022456645966,\n",
       "  0.23177573084831238,\n",
       "  0.31884250044822693,\n",
       "  -0.10725637525320053,\n",
       "  -0.2860272228717804,\n",
       "  -0.21971635520458221,\n",
       "  -0.29555580019950867,\n",
       "  0.3177889287471771,\n",
       "  -0.11162906140089035,\n",
       "  0.21935592591762543,\n",
       "  0.08060990273952484,\n",
       "  0.06216880679130554,\n",
       "  -0.11729424446821213,\n",
       "  0.21121692657470703,\n",
       "  -0.12110460549592972,\n",
       "  0.43867769837379456,\n",
       "  -0.2540470063686371,\n",
       "  -0.2394268661737442,\n",
       "  -0.0909498929977417,\n",
       "  0.20677007734775543,\n",
       "  -0.22091518342494965,\n",
       "  -0.3037639260292053,\n",
       "  0.30394017696380615,\n",
       "  0.153153657913208,\n",
       "  -0.35198014974594116,\n",
       "  -0.021952742710709572,\n",
       "  -0.23249979317188263,\n",
       "  -0.2581396996974945,\n",
       "  -0.2827187478542328,\n",
       "  0.36873558163642883,\n",
       "  0.3709706664085388,\n",
       "  -0.08481775969266891,\n",
       "  -0.2627639174461365,\n",
       "  0.4583421051502228,\n",
       "  -0.31300434470176697,\n",
       "  0.3708840310573578,\n",
       "  -0.10305589437484741,\n",
       "  0.3000417947769165,\n",
       "  -0.19622379541397095,\n",
       "  -0.2834450900554657,\n",
       "  0.11438899487257004,\n",
       "  -0.257830947637558,\n",
       "  -0.27232030034065247,\n",
       "  0.1493559032678604,\n",
       "  -0.26720958948135376,\n",
       "  -0.3887597322463989,\n",
       "  0.27934569120407104,\n",
       "  0.028316641226410866,\n",
       "  -0.20324905216693878,\n",
       "  0.237822026014328,\n",
       "  0.1887531876564026,\n",
       "  -0.4624069333076477,\n",
       "  0.042809564620256424,\n",
       "  -0.33468934893608093,\n",
       "  -0.1961463987827301,\n",
       "  -0.019646966829895973,\n",
       "  0.13418740034103394,\n",
       "  0.1526598483324051,\n",
       "  0.3831208050251007,\n",
       "  -0.07263857126235962,\n",
       "  0.030955949798226357,\n",
       "  -0.02819211035966873,\n",
       "  -0.11854786425828934,\n",
       "  -0.1984373927116394,\n",
       "  -0.25410041213035583,\n",
       "  -0.10539098083972931,\n",
       "  0.12409134209156036,\n",
       "  0.08357381075620651,\n",
       "  -0.19571514427661896,\n",
       "  0.19625496864318848,\n",
       "  0.18196646869182587,\n",
       "  0.3224068880081177,\n",
       "  0.20778512954711914,\n",
       "  0.23307889699935913,\n",
       "  0.2136409878730774,\n",
       "  0.309150367975235,\n",
       "  0.3247028887271881,\n",
       "  0.22155316174030304,\n",
       "  0.04499324783682823,\n",
       "  0.16452085971832275,\n",
       "  0.030625561252236366,\n",
       "  0.19001346826553345,\n",
       "  0.19127871096134186,\n",
       "  0.2723590135574341,\n",
       "  0.22710856795310974,\n",
       "  0.22784772515296936,\n",
       "  -0.14124195277690887,\n",
       "  0.3209955394268036],\n",
       " [-0.35791340470314026,\n",
       "  0.26698946952819824,\n",
       "  -0.09512882679700851,\n",
       "  -0.0047071343287825584,\n",
       "  0.1654966175556183,\n",
       "  0.14154905080795288,\n",
       "  -0.3338702917098999,\n",
       "  -0.12367624789476395,\n",
       "  0.18952010571956635,\n",
       "  -0.11053846031427383,\n",
       "  -0.2618904411792755,\n",
       "  -0.11754820495843887,\n",
       "  0.17547045648097992,\n",
       "  -0.03597959131002426,\n",
       "  0.4411907196044922,\n",
       "  0.014064689166843891,\n",
       "  0.01666095107793808,\n",
       "  0.1770620495080948,\n",
       "  0.3251374661922455,\n",
       "  0.003986166324466467,\n",
       "  0.2823714315891266,\n",
       "  -0.27818432450294495,\n",
       "  0.37367361783981323,\n",
       "  0.042778804898262024,\n",
       "  0.28152137994766235,\n",
       "  0.12793771922588348,\n",
       "  0.2897695004940033,\n",
       "  0.007131745107471943,\n",
       "  0.19353102147579193,\n",
       "  -0.2322344034910202,\n",
       "  -0.13017696142196655,\n",
       "  -0.04692450165748596,\n",
       "  0.21807120740413666,\n",
       "  0.21550297737121582,\n",
       "  -0.08499440550804138,\n",
       "  -0.3457574248313904,\n",
       "  -0.3608167767524719,\n",
       "  0.1919105052947998,\n",
       "  -0.18918801844120026,\n",
       "  0.13556288182735443,\n",
       "  0.24390022456645966,\n",
       "  0.23177573084831238,\n",
       "  0.31884250044822693,\n",
       "  -0.10725637525320053,\n",
       "  -0.2860272228717804,\n",
       "  -0.21971635520458221,\n",
       "  -0.29555580019950867,\n",
       "  0.3177889287471771,\n",
       "  -0.11162906140089035,\n",
       "  0.21935592591762543,\n",
       "  0.08060990273952484,\n",
       "  0.06216880679130554,\n",
       "  -0.11729424446821213,\n",
       "  0.21121692657470703,\n",
       "  -0.12110460549592972,\n",
       "  0.43867769837379456,\n",
       "  -0.2540470063686371,\n",
       "  -0.2394268661737442,\n",
       "  -0.0909498929977417,\n",
       "  0.20677007734775543,\n",
       "  -0.22091518342494965,\n",
       "  -0.3037639260292053,\n",
       "  0.30394017696380615,\n",
       "  0.153153657913208,\n",
       "  -0.35198014974594116,\n",
       "  -0.021952742710709572,\n",
       "  -0.23249979317188263,\n",
       "  -0.2581396996974945,\n",
       "  -0.2827187478542328,\n",
       "  0.36873558163642883,\n",
       "  0.3709706664085388,\n",
       "  -0.08481775969266891,\n",
       "  -0.2627639174461365,\n",
       "  0.4583421051502228,\n",
       "  -0.31300434470176697,\n",
       "  0.3708840310573578,\n",
       "  -0.10305589437484741,\n",
       "  0.3000417947769165,\n",
       "  -0.19622379541397095,\n",
       "  -0.2834450900554657,\n",
       "  0.11438899487257004,\n",
       "  -0.257830947637558,\n",
       "  -0.27232030034065247,\n",
       "  0.1493559032678604,\n",
       "  -0.26720958948135376,\n",
       "  -0.3887597322463989,\n",
       "  0.27934569120407104,\n",
       "  0.028316641226410866,\n",
       "  -0.20324905216693878,\n",
       "  0.237822026014328,\n",
       "  0.1887531876564026,\n",
       "  -0.4624069333076477,\n",
       "  0.042809564620256424,\n",
       "  -0.33468934893608093,\n",
       "  -0.1961463987827301,\n",
       "  -0.019646966829895973,\n",
       "  0.13418740034103394,\n",
       "  0.1526598483324051,\n",
       "  0.3831208050251007,\n",
       "  -0.07263857126235962,\n",
       "  0.030955949798226357,\n",
       "  -0.02819211035966873,\n",
       "  -0.11854786425828934,\n",
       "  -0.1984373927116394,\n",
       "  -0.25410041213035583,\n",
       "  -0.10539098083972931,\n",
       "  0.12409134209156036,\n",
       "  0.08357381075620651,\n",
       "  -0.19571514427661896,\n",
       "  0.19625496864318848,\n",
       "  0.18196646869182587,\n",
       "  0.3224068880081177,\n",
       "  0.20778512954711914,\n",
       "  0.23307889699935913,\n",
       "  0.2136409878730774,\n",
       "  0.309150367975235,\n",
       "  0.3247028887271881,\n",
       "  0.22155316174030304,\n",
       "  0.04499324783682823,\n",
       "  0.16452085971832275,\n",
       "  0.030625561252236366,\n",
       "  0.19001346826553345,\n",
       "  0.19127871096134186,\n",
       "  0.2723590135574341,\n",
       "  0.22710856795310974,\n",
       "  0.22784772515296936,\n",
       "  -0.14124195277690887,\n",
       "  0.3209955394268036],\n",
       " [-0.35791340470314026,\n",
       "  0.26698946952819824,\n",
       "  -0.09512882679700851,\n",
       "  -0.0047071343287825584,\n",
       "  0.1654966175556183,\n",
       "  0.14154905080795288,\n",
       "  -0.3338702917098999,\n",
       "  -0.12367624789476395,\n",
       "  0.18952010571956635,\n",
       "  -0.11053846031427383,\n",
       "  -0.2618904411792755,\n",
       "  -0.11754820495843887,\n",
       "  0.17547045648097992,\n",
       "  -0.03597959131002426,\n",
       "  0.4411907196044922,\n",
       "  0.014064689166843891,\n",
       "  0.01666095107793808,\n",
       "  0.1770620495080948,\n",
       "  0.3251374661922455,\n",
       "  0.003986166324466467,\n",
       "  0.2823714315891266,\n",
       "  -0.27818432450294495,\n",
       "  0.37367361783981323,\n",
       "  0.042778804898262024,\n",
       "  0.28152137994766235,\n",
       "  0.12793771922588348,\n",
       "  0.2897695004940033,\n",
       "  0.007131745107471943,\n",
       "  0.19353102147579193,\n",
       "  -0.2322344034910202,\n",
       "  -0.13017696142196655,\n",
       "  -0.04692450165748596,\n",
       "  0.21807120740413666,\n",
       "  0.21550297737121582,\n",
       "  -0.08499440550804138,\n",
       "  -0.3457574248313904,\n",
       "  -0.3608167767524719,\n",
       "  0.1919105052947998,\n",
       "  -0.18918801844120026,\n",
       "  0.13556288182735443,\n",
       "  0.24390022456645966,\n",
       "  0.23177573084831238,\n",
       "  0.31884250044822693,\n",
       "  -0.10725637525320053,\n",
       "  -0.2860272228717804,\n",
       "  -0.21971635520458221,\n",
       "  -0.29555580019950867,\n",
       "  0.3177889287471771,\n",
       "  -0.11162906140089035,\n",
       "  0.21935592591762543,\n",
       "  0.08060990273952484,\n",
       "  0.06216880679130554,\n",
       "  -0.11729424446821213,\n",
       "  0.21121692657470703,\n",
       "  -0.12110460549592972,\n",
       "  0.43867769837379456,\n",
       "  -0.2540470063686371,\n",
       "  -0.2394268661737442,\n",
       "  -0.0909498929977417,\n",
       "  0.20677007734775543,\n",
       "  -0.22091518342494965,\n",
       "  -0.3037639260292053,\n",
       "  0.30394017696380615,\n",
       "  0.153153657913208,\n",
       "  -0.35198014974594116,\n",
       "  -0.021952742710709572,\n",
       "  -0.23249979317188263,\n",
       "  -0.2581396996974945,\n",
       "  -0.2827187478542328,\n",
       "  0.36873558163642883,\n",
       "  0.3709706664085388,\n",
       "  -0.08481775969266891,\n",
       "  -0.2627639174461365,\n",
       "  0.4583421051502228,\n",
       "  -0.31300434470176697,\n",
       "  0.3708840310573578,\n",
       "  -0.10305589437484741,\n",
       "  0.3000417947769165,\n",
       "  -0.19622379541397095,\n",
       "  -0.2834450900554657,\n",
       "  0.11438899487257004,\n",
       "  -0.257830947637558,\n",
       "  -0.27232030034065247,\n",
       "  0.1493559032678604,\n",
       "  -0.26720958948135376,\n",
       "  -0.3887597322463989,\n",
       "  0.27934569120407104,\n",
       "  0.028316641226410866,\n",
       "  -0.20324905216693878,\n",
       "  0.237822026014328,\n",
       "  0.1887531876564026,\n",
       "  -0.4624069333076477,\n",
       "  0.042809564620256424,\n",
       "  -0.33468934893608093,\n",
       "  -0.1961463987827301,\n",
       "  -0.019646966829895973,\n",
       "  0.13418740034103394,\n",
       "  0.1526598483324051,\n",
       "  0.3831208050251007,\n",
       "  -0.07263857126235962,\n",
       "  0.030955949798226357,\n",
       "  -0.02819211035966873,\n",
       "  -0.11854786425828934,\n",
       "  -0.1984373927116394,\n",
       "  -0.25410041213035583,\n",
       "  -0.10539098083972931,\n",
       "  0.12409134209156036,\n",
       "  0.08357381075620651,\n",
       "  -0.19571514427661896,\n",
       "  0.19625496864318848,\n",
       "  0.18196646869182587,\n",
       "  0.3224068880081177,\n",
       "  0.20778512954711914,\n",
       "  0.23307889699935913,\n",
       "  0.2136409878730774,\n",
       "  0.309150367975235,\n",
       "  0.3247028887271881,\n",
       "  0.22155316174030304,\n",
       "  0.04499324783682823,\n",
       "  0.16452085971832275,\n",
       "  0.030625561252236366,\n",
       "  0.19001346826553345,\n",
       "  0.19127871096134186,\n",
       "  0.2723590135574341,\n",
       "  0.22710856795310974,\n",
       "  0.22784772515296936,\n",
       "  -0.14124195277690887,\n",
       "  0.3209955394268036],\n",
       " [-0.35791340470314026,\n",
       "  0.26698946952819824,\n",
       "  -0.09512882679700851,\n",
       "  -0.0047071343287825584,\n",
       "  0.1654966175556183,\n",
       "  0.14154905080795288,\n",
       "  -0.3338702917098999,\n",
       "  -0.12367624789476395,\n",
       "  0.18952010571956635,\n",
       "  -0.11053846031427383,\n",
       "  -0.2618904411792755,\n",
       "  -0.11754820495843887,\n",
       "  0.17547045648097992,\n",
       "  -0.03597959131002426,\n",
       "  0.4411907196044922,\n",
       "  0.014064689166843891,\n",
       "  0.01666095107793808,\n",
       "  0.1770620495080948,\n",
       "  0.3251374661922455,\n",
       "  0.003986166324466467,\n",
       "  0.2823714315891266,\n",
       "  -0.27818432450294495,\n",
       "  0.37367361783981323,\n",
       "  0.042778804898262024,\n",
       "  0.28152137994766235,\n",
       "  0.12793771922588348,\n",
       "  0.2897695004940033,\n",
       "  0.007131745107471943,\n",
       "  0.19353102147579193,\n",
       "  -0.2322344034910202,\n",
       "  -0.13017696142196655,\n",
       "  -0.04692450165748596,\n",
       "  0.21807120740413666,\n",
       "  0.21550297737121582,\n",
       "  -0.08499440550804138,\n",
       "  -0.3457574248313904,\n",
       "  -0.3608167767524719,\n",
       "  0.1919105052947998,\n",
       "  -0.18918801844120026,\n",
       "  0.13556288182735443,\n",
       "  0.24390022456645966,\n",
       "  0.23177573084831238,\n",
       "  0.31884250044822693,\n",
       "  -0.10725637525320053,\n",
       "  -0.2860272228717804,\n",
       "  -0.21971635520458221,\n",
       "  -0.29555580019950867,\n",
       "  0.3177889287471771,\n",
       "  -0.11162906140089035,\n",
       "  0.21935592591762543,\n",
       "  0.08060990273952484,\n",
       "  0.06216880679130554,\n",
       "  -0.11729424446821213,\n",
       "  0.21121692657470703,\n",
       "  -0.12110460549592972,\n",
       "  0.43867769837379456,\n",
       "  -0.2540470063686371,\n",
       "  -0.2394268661737442,\n",
       "  -0.0909498929977417,\n",
       "  0.20677007734775543,\n",
       "  -0.22091518342494965,\n",
       "  -0.3037639260292053,\n",
       "  0.30394017696380615,\n",
       "  0.153153657913208,\n",
       "  -0.35198014974594116,\n",
       "  -0.021952742710709572,\n",
       "  -0.23249979317188263,\n",
       "  -0.2581396996974945,\n",
       "  -0.2827187478542328,\n",
       "  0.36873558163642883,\n",
       "  0.3709706664085388,\n",
       "  -0.08481775969266891,\n",
       "  -0.2627639174461365,\n",
       "  0.4583421051502228,\n",
       "  -0.31300434470176697,\n",
       "  0.3708840310573578,\n",
       "  -0.10305589437484741,\n",
       "  0.3000417947769165,\n",
       "  -0.19622379541397095,\n",
       "  -0.2834450900554657,\n",
       "  0.11438899487257004,\n",
       "  -0.257830947637558,\n",
       "  -0.27232030034065247,\n",
       "  0.1493559032678604,\n",
       "  -0.26720958948135376,\n",
       "  -0.3887597322463989,\n",
       "  0.27934569120407104,\n",
       "  0.028316641226410866,\n",
       "  -0.20324905216693878,\n",
       "  0.237822026014328,\n",
       "  0.1887531876564026,\n",
       "  -0.4624069333076477,\n",
       "  0.042809564620256424,\n",
       "  -0.33468934893608093,\n",
       "  -0.1961463987827301,\n",
       "  -0.019646966829895973,\n",
       "  0.13418740034103394,\n",
       "  0.1526598483324051,\n",
       "  0.3831208050251007,\n",
       "  -0.07263857126235962,\n",
       "  0.030955949798226357,\n",
       "  -0.02819211035966873,\n",
       "  -0.11854786425828934,\n",
       "  -0.1984373927116394,\n",
       "  -0.25410041213035583,\n",
       "  -0.10539098083972931,\n",
       "  0.12409134209156036,\n",
       "  0.08357381075620651,\n",
       "  -0.19571514427661896,\n",
       "  0.19625496864318848,\n",
       "  0.18196646869182587,\n",
       "  0.3224068880081177,\n",
       "  0.20778512954711914,\n",
       "  0.23307889699935913,\n",
       "  0.2136409878730774,\n",
       "  0.309150367975235,\n",
       "  0.3247028887271881,\n",
       "  0.22155316174030304,\n",
       "  0.04499324783682823,\n",
       "  0.16452085971832275,\n",
       "  0.030625561252236366,\n",
       "  0.19001346826553345,\n",
       "  0.19127871096134186,\n",
       "  0.2723590135574341,\n",
       "  0.22710856795310974,\n",
       "  0.22784772515296936,\n",
       "  -0.14124195277690887,\n",
       "  0.3209955394268036],\n",
       " [-0.35791340470314026,\n",
       "  0.26698946952819824,\n",
       "  -0.09512882679700851,\n",
       "  -0.0047071343287825584,\n",
       "  0.1654966175556183,\n",
       "  0.14154905080795288,\n",
       "  -0.3338702917098999,\n",
       "  -0.12367624789476395,\n",
       "  0.18952010571956635,\n",
       "  -0.11053846031427383,\n",
       "  -0.2618904411792755,\n",
       "  -0.11754820495843887,\n",
       "  0.17547045648097992,\n",
       "  -0.03597959131002426,\n",
       "  0.4411907196044922,\n",
       "  0.014064689166843891,\n",
       "  0.01666095107793808,\n",
       "  0.1770620495080948,\n",
       "  0.3251374661922455,\n",
       "  0.003986166324466467,\n",
       "  0.2823714315891266,\n",
       "  -0.27818432450294495,\n",
       "  0.37367361783981323,\n",
       "  0.042778804898262024,\n",
       "  0.28152137994766235,\n",
       "  0.12793771922588348,\n",
       "  0.2897695004940033,\n",
       "  0.007131745107471943,\n",
       "  0.19353102147579193,\n",
       "  -0.2322344034910202,\n",
       "  -0.13017696142196655,\n",
       "  -0.04692450165748596,\n",
       "  0.21807120740413666,\n",
       "  0.21550297737121582,\n",
       "  -0.08499440550804138,\n",
       "  -0.3457574248313904,\n",
       "  -0.3608167767524719,\n",
       "  0.1919105052947998,\n",
       "  -0.18918801844120026,\n",
       "  0.13556288182735443,\n",
       "  0.24390022456645966,\n",
       "  0.23177573084831238,\n",
       "  0.31884250044822693,\n",
       "  -0.10725637525320053,\n",
       "  -0.2860272228717804,\n",
       "  -0.21971635520458221,\n",
       "  -0.29555580019950867,\n",
       "  0.3177889287471771,\n",
       "  -0.11162906140089035,\n",
       "  0.21935592591762543,\n",
       "  0.08060990273952484,\n",
       "  0.06216880679130554,\n",
       "  -0.11729424446821213,\n",
       "  0.21121692657470703,\n",
       "  -0.12110460549592972,\n",
       "  0.43867769837379456,\n",
       "  -0.2540470063686371,\n",
       "  -0.2394268661737442,\n",
       "  -0.0909498929977417,\n",
       "  0.20677007734775543,\n",
       "  -0.22091518342494965,\n",
       "  -0.3037639260292053,\n",
       "  0.30394017696380615,\n",
       "  0.153153657913208,\n",
       "  -0.35198014974594116,\n",
       "  -0.021952742710709572,\n",
       "  -0.23249979317188263,\n",
       "  -0.2581396996974945,\n",
       "  -0.2827187478542328,\n",
       "  0.36873558163642883,\n",
       "  0.3709706664085388,\n",
       "  -0.08481775969266891,\n",
       "  -0.2627639174461365,\n",
       "  0.4583421051502228,\n",
       "  -0.31300434470176697,\n",
       "  0.3708840310573578,\n",
       "  -0.10305589437484741,\n",
       "  0.3000417947769165,\n",
       "  -0.19622379541397095,\n",
       "  -0.2834450900554657,\n",
       "  0.11438899487257004,\n",
       "  -0.257830947637558,\n",
       "  -0.27232030034065247,\n",
       "  0.1493559032678604,\n",
       "  -0.26720958948135376,\n",
       "  -0.3887597322463989,\n",
       "  0.27934569120407104,\n",
       "  0.028316641226410866,\n",
       "  -0.20324905216693878,\n",
       "  0.237822026014328,\n",
       "  0.1887531876564026,\n",
       "  -0.4624069333076477,\n",
       "  0.042809564620256424,\n",
       "  -0.33468934893608093,\n",
       "  -0.1961463987827301,\n",
       "  -0.019646966829895973,\n",
       "  0.13418740034103394,\n",
       "  0.1526598483324051,\n",
       "  0.3831208050251007,\n",
       "  -0.07263857126235962,\n",
       "  0.030955949798226357,\n",
       "  -0.02819211035966873,\n",
       "  -0.11854786425828934,\n",
       "  -0.1984373927116394,\n",
       "  -0.25410041213035583,\n",
       "  -0.10539098083972931,\n",
       "  0.12409134209156036,\n",
       "  0.08357381075620651,\n",
       "  -0.19571514427661896,\n",
       "  0.19625496864318848,\n",
       "  0.18196646869182587,\n",
       "  0.3224068880081177,\n",
       "  0.20778512954711914,\n",
       "  0.23307889699935913,\n",
       "  0.2136409878730774,\n",
       "  0.309150367975235,\n",
       "  0.3247028887271881,\n",
       "  0.22155316174030304,\n",
       "  0.04499324783682823,\n",
       "  0.16452085971832275,\n",
       "  0.030625561252236366,\n",
       "  0.19001346826553345,\n",
       "  0.19127871096134186,\n",
       "  0.2723590135574341,\n",
       "  0.22710856795310974,\n",
       "  0.22784772515296936,\n",
       "  -0.14124195277690887,\n",
       "  0.3209955394268036],\n",
       " [-0.35791340470314026,\n",
       "  0.26698946952819824,\n",
       "  -0.09512882679700851,\n",
       "  -0.0047071343287825584,\n",
       "  0.1654966175556183,\n",
       "  0.14154905080795288,\n",
       "  -0.3338702917098999,\n",
       "  -0.12367624789476395,\n",
       "  0.18952010571956635,\n",
       "  -0.11053846031427383,\n",
       "  -0.2618904411792755,\n",
       "  -0.11754820495843887,\n",
       "  0.17547045648097992,\n",
       "  -0.03597959131002426,\n",
       "  0.4411907196044922,\n",
       "  0.014064689166843891,\n",
       "  0.01666095107793808,\n",
       "  0.1770620495080948,\n",
       "  0.3251374661922455,\n",
       "  0.003986166324466467,\n",
       "  0.2823714315891266,\n",
       "  -0.27818432450294495,\n",
       "  0.37367361783981323,\n",
       "  0.042778804898262024,\n",
       "  0.28152137994766235,\n",
       "  0.12793771922588348,\n",
       "  0.2897695004940033,\n",
       "  0.007131745107471943,\n",
       "  0.19353102147579193,\n",
       "  -0.2322344034910202,\n",
       "  -0.13017696142196655,\n",
       "  -0.04692450165748596,\n",
       "  0.21807120740413666,\n",
       "  0.21550297737121582,\n",
       "  -0.08499440550804138,\n",
       "  -0.3457574248313904,\n",
       "  -0.3608167767524719,\n",
       "  0.1919105052947998,\n",
       "  -0.18918801844120026,\n",
       "  0.13556288182735443,\n",
       "  0.24390022456645966,\n",
       "  0.23177573084831238,\n",
       "  0.31884250044822693,\n",
       "  -0.10725637525320053,\n",
       "  -0.2860272228717804,\n",
       "  -0.21971635520458221,\n",
       "  -0.29555580019950867,\n",
       "  0.3177889287471771,\n",
       "  -0.11162906140089035,\n",
       "  0.21935592591762543,\n",
       "  0.08060990273952484,\n",
       "  0.06216880679130554,\n",
       "  -0.11729424446821213,\n",
       "  0.21121692657470703,\n",
       "  -0.12110460549592972,\n",
       "  0.43867769837379456,\n",
       "  -0.2540470063686371,\n",
       "  -0.2394268661737442,\n",
       "  -0.0909498929977417,\n",
       "  0.20677007734775543,\n",
       "  -0.22091518342494965,\n",
       "  -0.3037639260292053,\n",
       "  0.30394017696380615,\n",
       "  0.153153657913208,\n",
       "  -0.35198014974594116,\n",
       "  -0.021952742710709572,\n",
       "  -0.23249979317188263,\n",
       "  -0.2581396996974945,\n",
       "  -0.2827187478542328,\n",
       "  0.36873558163642883,\n",
       "  0.3709706664085388,\n",
       "  -0.08481775969266891,\n",
       "  -0.2627639174461365,\n",
       "  0.4583421051502228,\n",
       "  -0.31300434470176697,\n",
       "  0.3708840310573578,\n",
       "  -0.10305589437484741,\n",
       "  0.3000417947769165,\n",
       "  -0.19622379541397095,\n",
       "  -0.2834450900554657,\n",
       "  0.11438899487257004,\n",
       "  -0.257830947637558,\n",
       "  -0.27232030034065247,\n",
       "  0.1493559032678604,\n",
       "  -0.26720958948135376,\n",
       "  -0.3887597322463989,\n",
       "  0.27934569120407104,\n",
       "  0.028316641226410866,\n",
       "  -0.20324905216693878,\n",
       "  0.237822026014328,\n",
       "  0.1887531876564026,\n",
       "  -0.4624069333076477,\n",
       "  0.042809564620256424,\n",
       "  -0.33468934893608093,\n",
       "  -0.1961463987827301,\n",
       "  -0.019646966829895973,\n",
       "  0.13418740034103394,\n",
       "  0.1526598483324051,\n",
       "  0.3831208050251007,\n",
       "  -0.07263857126235962,\n",
       "  0.030955949798226357,\n",
       "  -0.02819211035966873,\n",
       "  -0.11854786425828934,\n",
       "  -0.1984373927116394,\n",
       "  -0.25410041213035583,\n",
       "  -0.10539098083972931,\n",
       "  0.12409134209156036,\n",
       "  0.08357381075620651,\n",
       "  -0.19571514427661896,\n",
       "  0.19625496864318848,\n",
       "  0.18196646869182587,\n",
       "  0.3224068880081177,\n",
       "  0.20778512954711914,\n",
       "  0.23307889699935913,\n",
       "  0.2136409878730774,\n",
       "  0.309150367975235,\n",
       "  0.3247028887271881,\n",
       "  0.22155316174030304,\n",
       "  0.04499324783682823,\n",
       "  0.16452085971832275,\n",
       "  0.030625561252236366,\n",
       "  0.19001346826553345,\n",
       "  0.19127871096134186,\n",
       "  0.2723590135574341,\n",
       "  0.22710856795310974,\n",
       "  0.22784772515296936,\n",
       "  -0.14124195277690887,\n",
       "  0.3209955394268036]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['102844412722519367'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyein",
   "language": "python",
   "name": "hyein"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
