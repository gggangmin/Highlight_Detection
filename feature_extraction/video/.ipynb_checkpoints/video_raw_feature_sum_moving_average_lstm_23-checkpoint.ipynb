{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import string\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "import math\n",
    "import torch.utils.data as data\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy\n",
    "import torch.utils.data.sampler as sampler\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/video_statistic_features_one2.pickle','rb') as f2:\n",
    "    temp_data = pickle.load(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    if epoch<20:\n",
    "        lr = 0.01\n",
    "    else:\n",
    "        lr = 0.001\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model\n",
    "output, feature를 한번에 뽑음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size=1\n",
    "hidden_size=128\n",
    "num_layers=3\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._clf1 = nn.LSTM(input_size, hidden_size,3,batch_first=True)\n",
    "        self._lin = nn.Sequential(nn.Linear(hidden_size, hidden_size),\n",
    "                                 nn.Linear(hidden_size,2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=x.cuda()\n",
    "        hidden = Variable(torch.zeros(num_layers,x.size(0),hidden_size)).cuda() # (num_layers * num_directions, batch, hidden_size)\n",
    "        cell = Variable(torch.zeros(num_layers,x.size(0),hidden_size)).cuda() # (num_layers * num_directions, batch, hidden_size)        out,hidden = self._clf1(x,h0)\n",
    "        out,hidden = self._clf1(x,(hidden,cell))#batch*7*3\n",
    "        ft = out[:,-1,:]\n",
    "        out = self._lin(out[:,-1,:])\n",
    "        return out,ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 준비\n",
    "1. d_type : train, val, test 를 받음\n",
    "2. 7초 window size shift 진행 --> *** 이부분 수정해야함.***\n",
    "3. 하이라이트 구간 25%추출 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "class data_ds(data.Dataset):\n",
    "    def __init__(self,d_type):\n",
    "        self.gt_range =  1-0.25\n",
    "        self.d_type=d_type\n",
    "        \n",
    "        #label load\n",
    "        with open('label/label.pickle','rb') as f1:  \n",
    "            self.gt=pickle.load(f1)\n",
    "        \n",
    "        #원본 데이터로드\n",
    "        with open('./data/video_raw_feature_sum_smooth.pickle','rb') as f2:\n",
    "            temp_data = pickle.load(f2)\n",
    "            if temp_data['102844412722519367'][0] is not list:\n",
    "                for i in temp_data:\n",
    "                    temp_data[i] = np.array(temp_data[i])[:,np.newaxis]\n",
    "            self.data= temp_data\n",
    "        if d_type=='train':\n",
    "            self.sample = ['102844412722519367','102844212429550795','102844401151219358','102844401154430631','102844412717014335','102844401153971877','102844224148503678','102844412722847048','102844401152857762','102844412707380528','102844212431516886','102844283027925085','102844412716227901','102844412710001974','102844294670878922','102844294670551241','102844283023599703','102844412704496937','102844235751783874','102844401152071328','102844412709674293','102844401153447587','102844224148896895','102844235746868664','102979081290790284','102844283027531868','102844212431975640','102844401155937960','102844212429092040','102844341906649746','102844412706987311','102844412721339716','102844212430402768','102844341905011343','102844235753356742','102844235750997440','102844412709346612','102844412705217835','102844235752963525','102844412712164667','102844412705545516','102844341912220311','102844341907370644','102844235749424575','102844212429419722','102844294669568199','102844212431779031','102844294666422466','102844224146472059','102844212428895431','102844212429747404','102844235748703677','102844224146930812','102844212430730450','102844294674876621','102844341909598870','102844283020453971','102844294670026952','102844412723174729','102844341904683662','102844283025696858','102844235747261881','102844401154168486','102844235748310460','102844412711836986','102844412723567946','102844235749031358','102844294674286796','102844294666881219','102844412716686654']\n",
    "        if d_type=='val':\n",
    "            self.sample = ['102844294671796427','102844224145685626','102844412717407552','102844235751390657','102844401156069033','102904869420860038','102910307641576395','102844341905404560','102844341906977427','102844212430075086','102844412711116088','102844401153578660','102844294667405508','102844412706659630']\n",
    "        if d_type=='test':\n",
    "            self.sample = ['102844212431058132','102844341902586509','102844401152267937','102844212430927059','102844412708953395','102844212429944013','102844341912679064','102844235753749959','102844341908026005','102844283023206486','102844224147717245','102844412704890154','102844212430599377','102844412711443769','102844235747982779']\n",
    "        if d_type == 'total':\n",
    "            self.sample = ['102844412722519367','102844212429550795','102844401151219358','102844401154430631','102844412717014335','102844401153971877','102844224148503678','102844412722847048','102844401152857762','102844412707380528','102844212431516886','102844283027925085','102844412716227901','102844412710001974','102844294670878922','102844294670551241','102844283023599703','102844412704496937','102844235751783874','102844401152071328','102844412709674293','102844401153447587','102844224148896895','102844235746868664','102979081290790284','102844283027531868','102844212431975640','102844401155937960','102844212429092040','102844341906649746','102844412706987311','102844412721339716','102844212430402768','102844341905011343','102844235753356742','102844235750997440','102844412709346612','102844412705217835','102844235752963525','102844412712164667','102844412705545516','102844341912220311','102844341907370644','102844235749424575','102844212429419722','102844294669568199','102844212431779031','102844294666422466','102844224146472059','102844212428895431','102844212429747404','102844235748703677','102844224146930812','102844212430730450','102844294674876621','102844341909598870','102844283020453971','102844294670026952','102844412723174729','102844341904683662','102844283025696858','102844235747261881','102844401154168486','102844235748310460','102844412711836986','102844412723567946','102844235749031358','102844294674286796','102844294666881219','102844412716686654','102844294671796427','102844224145685626','102844412717407552','102844235751390657','102844401156069033','102904869420860038','102910307641576395','102844341905404560','102844341906977427','102844212430075086','102844412711116088','102844401153578660','102844294667405508','102844412706659630','102844212431058132','102844341902586509','102844401152267937','102844212430927059','102844412708953395','102844212429944013','102844341912679064','102844235753749959','102844341908026005','102844283023206486','102844224147717245','102844412704890154','102844212430599377','102844412711443769','102844235747982779']\n",
    "\n",
    "        \n",
    "        #sampling 대상이 될 데이터 저장 (25% 구간때문에 진행)\n",
    "        self.WeightedSampling=[]\n",
    "        for i in self.sample:\n",
    "            self.WeightedSampling.extend(copy.copy(self.gt[str(i)]))\n",
    "        \n",
    "        sampling = np.array(self.WeightedSampling)\n",
    "        neg_idx = np.where(sampling == 0)[0] #general\n",
    "        pos_idx = np.where(sampling == 1)[0] #highlight\n",
    "        sampling = sampling.astype(np.float32)\n",
    "\n",
    "        #구간 구분하면서, 25% 만 추출 및 weightedsampling 에 저장\n",
    "        begin_pos = 0 \n",
    "        hl_frames = []\n",
    "        for it, cur_pos in enumerate(pos_idx):\n",
    "            if it+1 < len(pos_idx): \n",
    "                if((pos_idx[it+1] - cur_pos) > 1):#cur_pos와 cur_pos+1 간격이 1보다 크면, 즉 다른 구간이면\n",
    "                    begin = int((it+1 - begin_pos) * self.gt_range) + begin_pos\n",
    "                    hl_frames.extend( pos_idx[begin: it] ) #한구간의 하이라이트 25%만 사용하겠다.\n",
    "                    begin_pos = it+1\n",
    "\n",
    "        #sampling 비율 맞춰줌\n",
    "        sampling.fill(0)\n",
    "        sampling[neg_idx] = len(sampling) / float(len(neg_idx))\n",
    "       # self.WeightedSampling[pos_idx] = len(self.WeightedSampling) / float(len(pos_idx))\n",
    "        sampling[hl_frames] = len(sampling) / float(len(hl_frames))\n",
    "        self.WeightedSampling = sampling\n",
    "        \n",
    "        #sum : 각 경기당 데이터수 누적으로 저장 -->지금 어디 경기인지, 몇번째 프레임인지 확인할때 사용\n",
    "        self.sum=np.insert(np.cumsum([len(self.gt[str(i)]) for i in self.sample]),0,0)\n",
    "        print(\"data load fin\")\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.WeightedSampling)\n",
    "    def __getitem__(self,index):\n",
    "            global input_size\n",
    "            #누적 데이터수 - index 로 프레임과 경기를 찾음\n",
    "            vid=np.histogram(index,self.sum)\n",
    "            vid = np.where(vid[0]>0)[0][0]\n",
    "            vframe=index-self.sum[vid]\n",
    "            game_id=str(self.sample[vid])\n",
    "\n",
    "            window=[]#batch*7(window size)*3(highlight result)\n",
    "            for idx in range(7): #7 : window size\n",
    "                s_window=[]\n",
    "                if vframe+idx<len(self.data[game_id]):\n",
    "                    s_window=(self.data[game_id][vframe+idx])#vframe의 image\n",
    "                else:\n",
    "                    #s_window=[0,0,0]#padding value\n",
    "                    s_window=[0]*input_size # input_size\n",
    "                window.append(s_window)\n",
    "                \n",
    "\n",
    "\n",
    "            #label 값 받기\n",
    "            label=self.gt[str(game_id)][vframe]\n",
    "            \n",
    "            \n",
    "            return np.array(window),label,str(game_id)\n",
    "        \n",
    "            \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 경기별 score 평균\n",
    "def fmeasure2(frames,label):\n",
    "    average = [0,0,0,0,0]\n",
    "    for key in frames.keys():\n",
    "        TP = len(np.where((np.array(frames[key])==1)&(label[key]==1)==True)[0])\n",
    "        FP = len(np.where((np.array(frames[key])==1)&(label[key]==0)==True)[0])\n",
    "        TN = len(np.where((np.array(frames[key])==0)&(label[key]==0)==True)[0])\n",
    "        FN = len(np.where((np.array(frames[key])==0)&(label[key]==1)==True)[0])\n",
    "        precision = TP/(TP+FP)\n",
    "        recall = TP/(TP+FN)\n",
    "        accuracy = (TP+TN)/(TP+FN+FP+TN)\n",
    "        print(precision,recall,accuracy)\n",
    "        if precision==0 and recall == 0:\n",
    "            print('error')\n",
    "        else:\n",
    "            f1 = (2*precision*recall / (precision + recall))\n",
    "            print(key)\n",
    "            print('precision : {}, recall : {}, f1 : {}, accuracy : {}'.format(precision,recall,f1,accuracy))\n",
    "            average[0]+= precision\n",
    "            average[1]+= recall\n",
    "            average[2]+= f1\n",
    "            average[3]+= accuracy\n",
    "            average[4]+=1\n",
    "    print('==precision : {}, recall : {}, f1 : {}, accuracy : {}'.format(average[0]/average[4],average[1]/average[4],average[2]/average[4],average[3]/average[4]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 전체 데이터에 대해서 score\n",
    "def fmeasure(output, target):\n",
    "    _, pred = output.topk(1, 1, True, True)\n",
    "    pred = pred.view(-1,1)\n",
    "    target = target.view(-1,1)\n",
    "\n",
    "    #overlap = ((pred== 1) + (target == 1)).gt(1)\n",
    "    #overlap = overlap.view(-1,1)\n",
    "    TP = len(np.where((pred==1)&(target==1)==True)[0]) # True positive\n",
    "    FP = len(np.where((pred==1)&(target==0)==True)[0]) # Condition positive = TP + FN\n",
    "    TN = len(np.where((pred==0)&(target==0)==True)[0])\n",
    "    FN = len(np.where((pred==0)&(target==1)==True)[0])\n",
    "\n",
    "    \n",
    "    #overlap_len = overlap.data.long().sum()\n",
    "    pred_len = pred.data.long().sum()\n",
    "    gt_len   =  target.data.long().sum()\n",
    "\n",
    "    return TP,FP,TN,FN,pred_len, gt_len,pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset load\n",
    "train : undersampling 진행,  validation : 그냥 평등하게 sampling (sequential sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SampleSequentialSampler(sampler.Sampler):\n",
    "    \"\"\"Samples elements sequentially, always in the same order.\n",
    "    Arguments:\n",
    "        data_source (Dataset): dataset to sample from\n",
    "        offset (int): offset between the samples\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_source, offset=10):\n",
    "        self.num_samples = len(data_source) \n",
    "        self.offset = offset\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(np.arange(0, self.num_samples, self.offset ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(np.arange(0, self.num_samples, self.offset ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load fin\n",
      "data load fin\n"
     ]
    }
   ],
   "source": [
    "# train, validation dataset 준비\n",
    "train=data_ds('train')\n",
    "val=data_ds('val')\n",
    "\n",
    "# dataloader with sampling\n",
    "sampler1 = torch.utils.data.sampler.WeightedRandomSampler(weights=train.WeightedSampling.tolist(), num_samples=40000)\n",
    "train_loader=torch.utils.data.DataLoader(train,batch_size=32,sampler=sampler1)\n",
    "sampler2 =  SampleSequentialSampler(val, 30)\n",
    "val_loader=torch.utils.data.DataLoader(val,batch_size=32,sampler= sampler2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.13377275],\n",
       "        [0.02945322],\n",
       "        [0.        ],\n",
       "        [0.05890644],\n",
       "        [0.02945322],\n",
       "        [0.04108113],\n",
       "        [0.        ]]), 0.0, '102844401153971877')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model load 및 실험\n",
    "\n",
    "- weight_dir/train_result 에 모든 결과가 저장됨.\n",
    "- fmeasure (전체 데이터에 대한 score 로 weight를 저장..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_dir='./video_raw_feature_sum_smooth/'\n",
    "\n",
    "###### model load #####\n",
    "model=LSTM().cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "epoch 0 train_loss : 0.6938788294792175 , val_loss : 0.7181130051612854, val_acc : 0.17386363636363636\n",
      "1\n",
      "epoch 1 train_loss : 0.693651020526886 , val_loss : 0.7255598306655884, val_acc : 0.17386363636363636\n",
      "2\n",
      "epoch 2 train_loss : 0.6935136318206787 , val_loss : 0.6851707100868225, val_acc : 0.8261363636363637\n",
      "3\n",
      "epoch 3 train_loss : 0.6933275461196899 , val_loss : 0.7127496004104614, val_acc : 0.17386363636363636\n",
      "4\n",
      "epoch 4 train_loss : 0.6903792023658752 , val_loss : 0.653791606426239, val_acc : 0.8772727272727273\n",
      "5\n",
      "epoch 5 train_loss : 0.5183555483818054 , val_loss : 0.3356100618839264, val_acc : 0.8988636363636363\n",
      "6\n",
      "epoch 6 train_loss : 0.4504523277282715 , val_loss : 0.43927615880966187, val_acc : 0.8738636363636364\n",
      "7\n",
      "epoch 7 train_loss : 0.44008591771125793 , val_loss : 0.41315117478370667, val_acc : 0.8920454545454546\n",
      "8\n",
      "epoch 8 train_loss : 0.43841463327407837 , val_loss : 0.444257527589798, val_acc : 0.8920454545454546\n",
      "9\n",
      "epoch 9 train_loss : 0.43118393421173096 , val_loss : 0.41671547293663025, val_acc : 0.8954545454545455\n",
      "10\n",
      "epoch 10 train_loss : 0.4369605779647827 , val_loss : 0.3929658532142639, val_acc : 0.8863636363636364\n",
      "11\n",
      "epoch 11 train_loss : 0.4329077899456024 , val_loss : 0.44492045044898987, val_acc : 0.8545454545454545\n",
      "12\n",
      "epoch 12 train_loss : 0.43660253286361694 , val_loss : 0.42623236775398254, val_acc : 0.8772727272727273\n",
      "13\n",
      "epoch 13 train_loss : 0.4338347017765045 , val_loss : 0.43486419320106506, val_acc : 0.8727272727272727\n",
      "14\n",
      "epoch 14 train_loss : 0.4381958246231079 , val_loss : 0.4057762920856476, val_acc : 0.8909090909090909\n",
      "15\n",
      "epoch 15 train_loss : 0.43382856249809265 , val_loss : 0.4186330735683441, val_acc : 0.8727272727272727\n",
      "16\n",
      "epoch 16 train_loss : 0.43584144115448 , val_loss : 0.42518946528434753, val_acc : 0.8852272727272728\n",
      "17\n",
      "epoch 17 train_loss : 0.4293307065963745 , val_loss : 0.4358748197555542, val_acc : 0.865909090909091\n",
      "18\n",
      "epoch 18 train_loss : 0.4306707978248596 , val_loss : 0.3935644328594208, val_acc : 0.8920454545454546\n",
      "19\n",
      "epoch 19 train_loss : 0.4307897984981537 , val_loss : 0.45226624608039856, val_acc : 0.8920454545454546\n",
      "20\n",
      "epoch 20 train_loss : 0.42721205949783325 , val_loss : 0.41493386030197144, val_acc : 0.8886363636363637\n",
      "21\n",
      "epoch 21 train_loss : 0.42072007060050964 , val_loss : 0.412023663520813, val_acc : 0.8761363636363636\n",
      "22\n",
      "epoch 22 train_loss : 0.4256736636161804 , val_loss : 0.40301865339279175, val_acc : 0.8829545454545454\n",
      "23\n",
      "epoch 23 train_loss : 0.42852783203125 , val_loss : 0.41624805331230164, val_acc : 0.8761363636363636\n",
      "24\n",
      "epoch 24 train_loss : 0.4211779236793518 , val_loss : 0.4063287675380707, val_acc : 0.8806818181818182\n",
      "25\n",
      "epoch 25 train_loss : 0.4242219626903534 , val_loss : 0.4064303934574127, val_acc : 0.8829545454545454\n",
      "26\n",
      "epoch 26 train_loss : 0.423952579498291 , val_loss : 0.42449381947517395, val_acc : 0.8727272727272727\n",
      "27\n",
      "epoch 27 train_loss : 0.42588934302330017 , val_loss : 0.41365936398506165, val_acc : 0.875\n",
      "28\n",
      "epoch 28 train_loss : 0.4230327010154724 , val_loss : 0.41429197788238525, val_acc : 0.8772727272727273\n",
      "29\n",
      "epoch 29 train_loss : 0.4257599413394928 , val_loss : 0.42153459787368774, val_acc : 0.8727272727272727\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(weight_dir):\n",
    "    os.makedirs(weight_dir)\n",
    "optimizer = torch.optim.SGD(model.parameters(), 0.01,momentum=0.9,weight_decay=1e-4)\n",
    "# scheduler = lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "with open(weight_dir+'train_result','a') as f:\n",
    "\n",
    "\n",
    "    best_losses=1000000\n",
    "    for epoch in range(30):\n",
    "        lr = adjust_learning_rate(optimizer, epoch)\n",
    "        train_loss=0\n",
    "        print(epoch)\n",
    "        model.train()\n",
    "        for i, (data,labels,game_id) in enumerate(train_loader):\n",
    "            inputs = Variable(data).float().cuda()\n",
    "            labels = Variable(labels).cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output,_=model(inputs)\n",
    "\n",
    "            loss=criterion(output,labels.long())\n",
    "            train_loss+=loss\n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.) #rnn 계열 gradient 장치\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        #validation\n",
    "        model.eval()\n",
    "        val_loss=0\n",
    "        acc=0\n",
    "        gt_sum=0\n",
    "        val_result ={}\n",
    "        with open(weight_dir+'train_result','a') as f:\n",
    "            with torch.no_grad():\n",
    "                for it, (data,labels,game_id) in enumerate(val_loader):\n",
    "                    inputs = Variable(data).float().cuda()\n",
    "                    labels = Variable(labels).cuda()\n",
    "                    output,_=model(inputs)\n",
    "                    loss=criterion(output,labels.long())\n",
    "                    val_loss+=loss\n",
    "                    TP,FP,TN,FN,pred_len, gt_len,pred=fmeasure(output.cpu(),labels.cpu())\n",
    "                    acc=acc+TP+TN\n",
    "                    gt_sum+=len(output)\n",
    "                val_acc=acc/gt_sum\n",
    "                print(\"epoch {} train_loss : {} , val_loss : {}, val_acc : {}\".format(epoch,train_loss/len(train_loader),val_loss/len(val_loader),val_acc))\n",
    "                f.write(\"epoch {} train_loss : {} , val_loss : {}, val_acc : {}\\n\".format(epoch,train_loss/len(train_loader),val_loss/len(val_loader),val_acc))\n",
    "                if best_losses>val_loss:\n",
    "                    best_losses=val_loss\n",
    "                    torch.save(model.state_dict(),'{}'.format(weight_dir+str(epoch)+\"train_best\"))#epoch 변화 확인위해\n",
    "                    torch.save(model.state_dict(),'{}'.format(weight_dir+\"train_best\"))#train best 덮어써짐\n",
    "\n",
    "                    f.write(\"epoch {} saved\\n\".format(epoch))\n",
    "                else:\n",
    "                    torch.save(model.state_dict(),'{}'.format(weight_dir+str(epoch)+\"train\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "- test set 로드\n",
    "- best weight load\n",
    "- weight_dir/train_result 에 결과 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load fin\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "5 2 25 0 tensor(7) tensor(5)\n",
      "9 3 20 0 tensor(12) tensor(9)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "9 10 13 0 tensor(19) tensor(9)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 16 16 tensor(0) tensor(16)\n",
      "17 2 13 0 tensor(19) tensor(17)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "11 8 12 1 tensor(19) tensor(12)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 12 20 0 tensor(12) tensor(0)\n",
      "0 8 24 0 tensor(8) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "7 0 25 0 tensor(7) tensor(7)\n",
      "11 1 13 7 tensor(12) tensor(18)\n",
      "1 3 21 7 tensor(4) tensor(8)\n",
      "0 19 13 0 tensor(19) tensor(0)\n",
      "13 3 16 0 tensor(16) tensor(13)\n",
      "6 19 7 0 tensor(25) tensor(6)\n",
      "5 4 23 0 tensor(9) tensor(5)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "13 0 19 0 tensor(13) tensor(13)\n",
      "3 10 19 0 tensor(13) tensor(3)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "7 9 16 0 tensor(16) tensor(7)\n",
      "0 3 29 0 tensor(3) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "16 3 12 1 tensor(19) tensor(17)\n",
      "7 17 4 4 tensor(24) tensor(11)\n",
      "0 0 30 2 tensor(0) tensor(2)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "10 0 19 3 tensor(10) tensor(13)\n",
      "23 3 6 0 tensor(26) tensor(23)\n",
      "10 20 2 0 tensor(30) tensor(10)\n",
      "0 20 12 0 tensor(20) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 28 4 0 tensor(28) tensor(0)\n",
      "30 2 0 0 tensor(32) tensor(30)\n",
      "0 12 20 0 tensor(12) tensor(0)\n",
      "11 3 18 0 tensor(14) tensor(11)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "10 0 20 2 tensor(10) tensor(12)\n",
      "9 3 20 0 tensor(12) tensor(9)\n",
      "19 0 13 0 tensor(19) tensor(19)\n",
      "9 7 16 0 tensor(16) tensor(9)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "15 0 10 7 tensor(15) tensor(22)\n",
      "17 3 12 0 tensor(20) tensor(17)\n",
      "21 0 4 7 tensor(21) tensor(28)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "17 2 13 0 tensor(19) tensor(17)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "18 4 10 0 tensor(22) tensor(18)\n",
      "7 12 13 0 tensor(19) tensor(7)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 4 28 0 tensor(4) tensor(0)\n",
      "0 15 17 0 tensor(15) tensor(0)\n",
      "0 8 24 0 tensor(8) tensor(0)\n",
      "0 11 21 0 tensor(11) tensor(0)\n",
      "8 22 2 0 tensor(30) tensor(8)\n",
      "10 3 19 0 tensor(13) tensor(10)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 11 21 0 tensor(11) tensor(0)\n",
      "0 8 24 0 tensor(8) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 19 13 0 tensor(19) tensor(0)\n",
      "10 5 17 0 tensor(15) tensor(10)\n",
      "18 4 10 0 tensor(22) tensor(18)\n",
      "5 13 14 0 tensor(18) tensor(5)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "16 5 11 0 tensor(21) tensor(16)\n",
      "25 0 7 0 tensor(25) tensor(25)\n",
      "0 19 13 0 tensor(19) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "11 2 19 0 tensor(13) tensor(11)\n",
      "18 1 13 0 tensor(19) tensor(18)\n",
      "9 8 6 9 tensor(17) tensor(18)\n",
      "0 3 29 0 tensor(3) tensor(0)\n",
      "0 19 13 0 tensor(19) tensor(0)\n",
      "0 4 28 0 tensor(4) tensor(0)\n",
      "0 15 17 0 tensor(15) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 3 29 0 tensor(3) tensor(0)\n",
      "11 16 2 3 tensor(27) tensor(14)\n",
      "32 0 0 0 tensor(32) tensor(32)\n",
      "26 0 6 0 tensor(26) tensor(26)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "7 0 25 0 tensor(7) tensor(7)\n",
      "15 3 14 0 tensor(18) tensor(15)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "5 3 24 0 tensor(8) tensor(5)\n",
      "4 7 21 0 tensor(11) tensor(4)\n",
      "17 1 14 0 tensor(18) tensor(17)\n",
      "5 2 25 0 tensor(7) tensor(5)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "9 1 22 0 tensor(10) tensor(9)\n",
      "13 5 14 0 tensor(18) tensor(13)\n",
      "10 6 16 0 tensor(16) tensor(10)\n",
      "0 19 13 0 tensor(19) tensor(0)\n",
      "11 6 15 0 tensor(17) tensor(11)\n",
      "12 5 15 0 tensor(17) tensor(12)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 25 7 0 tensor(25) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "3 16 13 0 tensor(19) tensor(3)\n",
      "17 11 1 3 tensor(28) tensor(20)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "8 5 19 0 tensor(13) tensor(8)\n",
      "0 6 26 0 tensor(6) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "6 4 22 0 tensor(10) tensor(6)\n",
      "5 4 23 0 tensor(9) tensor(5)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "24 5 0 3 tensor(29) tensor(27)\n",
      "25 7 0 0 tensor(32) tensor(25)\n",
      "0 15 17 0 tensor(15) tensor(0)\n",
      "0 8 24 0 tensor(8) tensor(0)\n",
      "12 20 0 0 tensor(32) tensor(12)\n",
      "19 9 4 0 tensor(28) tensor(19)\n",
      "6 22 4 0 tensor(28) tensor(6)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "7 4 21 0 tensor(11) tensor(7)\n",
      "21 3 6 2 tensor(24) tensor(23)\n",
      "32 0 0 0 tensor(32) tensor(32)\n",
      "8 1 23 0 tensor(9) tensor(8)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "2 5 25 0 tensor(7) tensor(2)\n",
      "29 0 3 0 tensor(29) tensor(29)\n",
      "11 8 8 5 tensor(19) tensor(16)\n",
      "15 0 16 1 tensor(15) tensor(16)\n",
      "19 13 0 0 tensor(32) tensor(19)\n",
      "11 0 21 0 tensor(11) tensor(11)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 31 1 tensor(0) tensor(1)\n",
      "18 1 13 0 tensor(19) tensor(18)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "2 0 27 3 tensor(2) tensor(5)\n",
      "17 0 5 10 tensor(17) tensor(27)\n",
      "12 7 13 0 tensor(19) tensor(12)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "15 0 5 12 tensor(15) tensor(27)\n",
      "21 11 0 0 tensor(32) tensor(21)\n",
      "0 15 17 0 tensor(15) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "15 4 13 0 tensor(19) tensor(15)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "13 8 11 0 tensor(21) tensor(13)\n",
      "14 12 6 0 tensor(26) tensor(14)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "8 7 17 0 tensor(15) tensor(8)\n",
      "0 17 15 0 tensor(17) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 19 13 0 tensor(19) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "8 0 21 3 tensor(8) tensor(11)\n",
      "16 3 13 0 tensor(19) tensor(16)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "24 3 5 0 tensor(27) tensor(24)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "1 0 26 5 tensor(1) tensor(6)\n",
      "16 2 14 0 tensor(18) tensor(16)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "10 7 15 0 tensor(17) tensor(10)\n",
      "0 2 30 0 tensor(2) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 10 22 0 tensor(10) tensor(0)\n",
      "10 9 6 7 tensor(19) tensor(17)\n",
      "13 0 19 0 tensor(13) tensor(13)\n",
      "10 9 13 0 tensor(19) tensor(10)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 30 2 0 tensor(30) tensor(0)\n",
      "0 19 13 0 tensor(19) tensor(0)\n",
      "14 6 12 0 tensor(20) tensor(14)\n",
      "8 0 21 3 tensor(8) tensor(11)\n",
      "10 6 16 0 tensor(16) tensor(10)\n",
      "2 1 29 0 tensor(3) tensor(2)\n",
      "0 2 30 0 tensor(2) tensor(0)\n",
      "0 17 15 0 tensor(17) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "15 0 17 0 tensor(15) tensor(15)\n",
      "9 2 21 0 tensor(11) tensor(9)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 5 27 0 tensor(5) tensor(0)\n",
      "0 14 18 0 tensor(14) tensor(0)\n",
      "3 8 21 0 tensor(11) tensor(3)\n",
      "22 0 0 10 tensor(22) tensor(32)\n",
      "19 2 10 1 tensor(21) tensor(20)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "24 0 3 5 tensor(24) tensor(29)\n",
      "12 3 17 0 tensor(15) tensor(12)\n",
      "0 16 16 0 tensor(16) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 19 13 0 tensor(19) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "19 0 11 2 tensor(19) tensor(21)\n",
      "12 6 0 14 tensor(18) tensor(26)\n",
      "0 1 31 0 tensor(1) tensor(0)\n",
      "0 0 30 2 tensor(0) tensor(2)\n",
      "17 2 10 3 tensor(19) tensor(20)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "5 0 27 0 tensor(5) tensor(5)\n",
      "24 2 6 0 tensor(26) tensor(24)\n",
      "4 0 25 3 tensor(4) tensor(7)\n",
      "10 6 16 0 tensor(16) tensor(10)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "19 0 8 5 tensor(19) tensor(24)\n",
      "0 0 15 17 tensor(0) tensor(17)\n",
      "21 0 7 4 tensor(21) tensor(25)\n",
      "32 0 0 0 tensor(32) tensor(32)\n",
      "6 0 26 0 tensor(6) tensor(6)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 27 5 tensor(0) tensor(5)\n",
      "1 0 10 21 tensor(1) tensor(22)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "1 0 13 18 tensor(1) tensor(19)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "11 0 4 17 tensor(11) tensor(28)\n",
      "8 10 14 0 tensor(18) tensor(8)\n",
      "4 8 7 13 tensor(12) tensor(17)\n",
      "0 0 28 4 tensor(0) tensor(4)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 19 13 0 tensor(19) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "8 10 14 0 tensor(18) tensor(8)\n",
      "0 7 25 0 tensor(7) tensor(0)\n",
      "0 13 19 0 tensor(13) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "11 18 1 2 tensor(29) tensor(13)\n",
      "0 3 29 0 tensor(3) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "6 13 13 0 tensor(19) tensor(6)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 17 15 0 tensor(17) tensor(0)\n",
      "0 2 30 0 tensor(2) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "7 0 15 10 tensor(7) tensor(17)\n",
      "12 3 16 1 tensor(15) tensor(13)\n",
      "0 16 16 0 tensor(16) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "10 0 18 4 tensor(10) tensor(14)\n",
      "5 22 5 0 tensor(27) tensor(5)\n",
      "5 8 19 0 tensor(13) tensor(5)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "4 18 10 0 tensor(22) tensor(4)\n",
      "17 10 5 0 tensor(27) tensor(17)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 11 21 0 tensor(11) tensor(0)\n",
      "0 8 19 5 tensor(8) tensor(5)\n",
      "19 0 11 2 tensor(19) tensor(21)\n",
      "22 0 1 9 tensor(22) tensor(31)\n",
      "9 0 18 5 tensor(9) tensor(14)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "17 1 8 6 tensor(18) tensor(23)\n",
      "0 1 31 0 tensor(1) tensor(0)\n",
      "0 0 24 8 tensor(0) tensor(8)\n",
      "1 0 8 23 tensor(1) tensor(24)\n",
      "1 0 21 10 tensor(1) tensor(11)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "15 2 15 0 tensor(17) tensor(15)\n",
      "4 0 20 8 tensor(4) tensor(12)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "9 10 13 0 tensor(19) tensor(9)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 30 2 tensor(0) tensor(2)\n",
      "17 0 0 15 tensor(17) tensor(32)\n",
      "0 2 30 0 tensor(2) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 9 23 0 tensor(9) tensor(0)\n",
      "0 29 3 0 tensor(29) tensor(0)\n",
      "8 11 13 0 tensor(19) tensor(8)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "1 0 17 14 tensor(1) tensor(15)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 20 12 0 tensor(20) tensor(0)\n",
      "0 6 26 0 tensor(6) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "14 3 15 0 tensor(17) tensor(14)\n",
      "4 0 25 3 tensor(4) tensor(7)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "7 12 12 1 tensor(19) tensor(8)\n",
      "16 3 11 2 tensor(19) tensor(18)\n",
      "0 1 31 0 tensor(1) tensor(0)\n",
      "0 0 30 2 tensor(0) tensor(2)\n",
      "1 2 9 20 tensor(3) tensor(21)\n",
      "28 0 3 1 tensor(28) tensor(29)\n",
      "0 20 12 0 tensor(20) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "19 0 0 13 tensor(19) tensor(32)\n",
      "12 7 13 0 tensor(19) tensor(12)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 1 31 0 tensor(1) tensor(0)\n",
      "25 7 0 0 tensor(32) tensor(25)\n",
      "26 0 0 6 tensor(26) tensor(32)\n",
      "12 0 20 0 tensor(12) tensor(12)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "18 1 10 3 tensor(19) tensor(21)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "1 0 23 8 tensor(1) tensor(9)\n",
      "30 1 1 0 tensor(31) tensor(30)\n",
      "0 1 31 0 tensor(1) tensor(0)\n",
      "15 3 14 0 tensor(18) tensor(15)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "10 3 19 0 tensor(13) tensor(10)\n",
      "4 3 25 0 tensor(7) tensor(4)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "8 6 18 0 tensor(14) tensor(8)\n",
      "0 25 7 0 tensor(25) tensor(0)\n",
      "0 15 17 0 tensor(15) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 19 13 0 tensor(19) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "10 9 13 0 tensor(19) tensor(10)\n",
      "0 3 29 0 tensor(3) tensor(0)\n",
      "0 28 4 0 tensor(28) tensor(0)\n",
      "0 25 7 0 tensor(25) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "12 8 12 0 tensor(20) tensor(12)\n",
      "2 6 24 0 tensor(8) tensor(2)\n",
      "6 21 5 0 tensor(27) tensor(6)\n",
      "0 3 29 0 tensor(3) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "22 0 9 1 tensor(22) tensor(23)\n",
      "1 0 30 1 tensor(1) tensor(2)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "7 12 13 0 tensor(19) tensor(7)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "5 6 21 0 tensor(11) tensor(5)\n",
      "8 14 9 1 tensor(22) tensor(9)\n",
      "0 7 14 11 tensor(7) tensor(11)\n",
      "0 0 29 3 tensor(0) tensor(3)\n",
      "1 0 25 6 tensor(1) tensor(7)\n",
      "32 0 0 0 tensor(32) tensor(32)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "16 1 15 0 tensor(17) tensor(16)\n",
      "1 1 30 0 tensor(2) tensor(1)\n",
      "6 4 22 0 tensor(10) tensor(6)\n",
      "6 3 23 0 tensor(9) tensor(6)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "10 10 12 0 tensor(20) tensor(10)\n",
      "30 0 0 2 tensor(30) tensor(32)\n",
      "4 4 24 0 tensor(8) tensor(4)\n",
      "15 1 16 0 tensor(16) tensor(15)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "10 4 18 0 tensor(14) tensor(10)\n",
      "3 2 27 0 tensor(5) tensor(3)\n",
      "0 19 13 0 tensor(19) tensor(0)\n",
      "4 0 24 4 tensor(4) tensor(8)\n",
      "27 0 3 2 tensor(27) tensor(29)\n",
      "0 25 7 0 tensor(25) tensor(0)\n",
      "0 1 31 0 tensor(1) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 4 21 0 tensor(11) tensor(7)\n",
      "28 1 3 0 tensor(29) tensor(28)\n",
      "9 10 13 0 tensor(19) tensor(9)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 19 13 0 tensor(19) tensor(0)\n",
      "11 8 13 0 tensor(19) tensor(11)\n",
      "0 19 13 0 tensor(19) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "8 0 23 1 tensor(8) tensor(9)\n",
      "9 2 21 0 tensor(11) tensor(9)\n",
      "0 5 27 0 tensor(5) tensor(0)\n",
      "4 10 18 0 tensor(14) tensor(4)\n",
      "0 24 8 0 tensor(24) tensor(0)\n",
      "10 9 13 0 tensor(19) tensor(10)\n",
      "12 7 13 0 tensor(19) tensor(12)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "3 0 29 0 tensor(3) tensor(3)\n",
      "23 9 0 0 tensor(32) tensor(23)\n",
      "9 14 9 0 tensor(23) tensor(9)\n",
      "0 8 24 0 tensor(8) tensor(0)\n",
      "0 5 27 0 tensor(5) tensor(0)\n",
      "0 14 11 7 tensor(14) tensor(7)\n",
      "0 0 26 6 tensor(0) tensor(6)\n",
      "12 4 16 0 tensor(16) tensor(12)\n",
      "20 2 10 0 tensor(22) tensor(20)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "9 3 20 0 tensor(12) tensor(9)\n",
      "7 14 11 0 tensor(21) tensor(7)\n",
      "0 5 25 2 tensor(5) tensor(2)\n",
      "0 0 10 22 tensor(0) tensor(22)\n",
      "25 0 6 1 tensor(25) tensor(26)\n",
      "8 0 24 0 tensor(8) tensor(8)\n",
      "22 0 9 1 tensor(22) tensor(23)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "17 2 11 2 tensor(19) tensor(19)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 3 29 0 tensor(3) tensor(0)\n",
      "7 9 16 0 tensor(16) tensor(7)\n",
      "12 3 17 0 tensor(15) tensor(12)\n",
      "2 2 28 0 tensor(4) tensor(2)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "19 3 10 0 tensor(22) tensor(19)\n",
      "0 0 30 2 tensor(0) tensor(2)\n",
      "1 0 13 18 tensor(1) tensor(19)\n",
      "0 9 23 0 tensor(9) tensor(0)\n",
      "0 32 0 0 tensor(32) tensor(0)\n",
      "0 6 26 0 tensor(6) tensor(0)\n",
      "10 0 3 19 tensor(10) tensor(29)\n",
      "9 0 19 4 tensor(9) tensor(13)\n",
      "4 15 13 0 tensor(19) tensor(4)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 1 31 0 tensor(1) tensor(0)\n",
      "16 9 0 7 tensor(25) tensor(23)\n",
      "2 17 2 11 tensor(19) tensor(13)\n",
      "0 2 30 0 tensor(2) tensor(0)\n",
      "13 0 13 6 tensor(13) tensor(19)\n",
      "24 5 3 0 tensor(29) tensor(24)\n",
      "3 6 23 0 tensor(9) tensor(3)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "1 0 13 18 tensor(1) tensor(19)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 1 31 0 tensor(1) tensor(0)\n",
      "9 7 16 0 tensor(16) tensor(9)\n",
      "10 3 16 3 tensor(13) tensor(13)\n",
      "16 3 8 5 tensor(19) tensor(21)\n",
      "12 0 15 5 tensor(12) tensor(17)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 17 15 0 tensor(17) tensor(0)\n",
      "11 7 14 0 tensor(18) tensor(11)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "3 6 23 0 tensor(9) tensor(3)\n",
      "15 0 17 0 tensor(15) tensor(15)\n",
      "9 10 13 0 tensor(19) tensor(9)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "16 0 7 9 tensor(16) tensor(25)\n",
      "16 0 9 7 tensor(16) tensor(23)\n",
      "0 24 8 0 tensor(24) tensor(0)\n",
      "1 3 23 5 tensor(4) tensor(6)\n",
      "1 1 21 9 tensor(2) tensor(10)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "21 0 8 3 tensor(21) tensor(24)\n",
      "1 10 10 11 tensor(11) tensor(12)\n",
      "4 17 11 0 tensor(21) tensor(4)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "10 0 16 6 tensor(10) tensor(16)\n",
      "9 0 17 6 tensor(9) tensor(15)\n",
      "0 0 20 12 tensor(0) tensor(12)\n",
      "29 0 0 3 tensor(29) tensor(32)\n",
      "12 1 19 0 tensor(13) tensor(12)\n",
      "1 8 23 0 tensor(9) tensor(1)\n",
      "5 5 22 0 tensor(10) tensor(5)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "4 0 11 17 tensor(4) tensor(21)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 1 31 0 tensor(1) tensor(0)\n",
      "24 8 0 0 tensor(32) tensor(24)\n",
      "4 3 25 0 tensor(7) tensor(4)\n",
      "3 7 22 0 tensor(10) tensor(3)\n",
      "28 0 3 1 tensor(28) tensor(29)\n",
      "0 0 21 11 tensor(0) tensor(11)\n",
      "0 0 30 2 tensor(0) tensor(2)\n",
      "4 0 25 3 tensor(4) tensor(7)\n",
      "13 0 19 0 tensor(13) tensor(13)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "1 2 29 0 tensor(3) tensor(1)\n",
      "20 7 5 0 tensor(27) tensor(20)\n",
      "21 3 7 1 tensor(24) tensor(22)\n",
      "12 2 18 0 tensor(14) tensor(12)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "16 3 8 5 tensor(19) tensor(21)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "11 4 17 0 tensor(15) tensor(11)\n",
      "0 4 28 0 tensor(4) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "9 1 22 0 tensor(10) tensor(9)\n",
      "7 2 23 0 tensor(9) tensor(7)\n",
      "0 2 30 0 tensor(2) tensor(0)\n",
      "0 17 15 0 tensor(17) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "10 4 18 0 tensor(14) tensor(10)\n",
      "18 3 0 11 tensor(21) tensor(29)\n",
      "22 4 6 0 tensor(26) tensor(22)\n",
      "0 20 12 0 tensor(20) tensor(0)\n",
      "0 18 14 0 tensor(18) tensor(0)\n",
      "0 15 17 0 tensor(15) tensor(0)\n",
      "0 31 1 0 tensor(31) tensor(0)\n",
      "16 3 13 0 tensor(19) tensor(16)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "16 2 7 7 tensor(18) tensor(23)\n",
      "0 16 16 0 tensor(16) tensor(0)\n",
      "0 6 26 0 tensor(6) tensor(0)\n",
      "0 17 15 0 tensor(17) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 7 25 0 tensor(7) tensor(0)\n",
      "0 12 20 0 tensor(12) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 3 29 0 tensor(3) tensor(0)\n",
      "31 1 0 0 tensor(32) tensor(31)\n",
      "17 0 0 15 tensor(17) tensor(32)\n",
      "28 4 0 0 tensor(32) tensor(28)\n",
      "2 5 25 0 tensor(7) tensor(2)\n",
      "0 9 23 0 tensor(9) tensor(0)\n",
      "8 24 0 0 tensor(32) tensor(8)\n",
      "32 0 0 0 tensor(32) tensor(32)\n",
      "21 0 11 0 tensor(21) tensor(21)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "7 3 22 0 tensor(10) tensor(7)\n",
      "3 6 23 0 tensor(9) tensor(3)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "12 0 20 0 tensor(12) tensor(12)\n",
      "5 2 25 0 tensor(7) tensor(5)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "1 0 7 24 tensor(1) tensor(25)\n",
      "2 0 13 17 tensor(2) tensor(19)\n",
      "13 6 13 0 tensor(19) tensor(13)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "5 0 15 12 tensor(5) tensor(17)\n",
      "23 0 8 1 tensor(23) tensor(24)\n",
      "0 24 8 0 tensor(24) tensor(0)\n",
      "9 10 4 9 tensor(19) tensor(18)\n",
      "13 6 11 2 tensor(19) tensor(15)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 5 27 0 tensor(5) tensor(0)\n",
      "31 1 0 0 tensor(32) tensor(31)\n",
      "0 18 14 0 tensor(18) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "17 2 11 2 tensor(19) tensor(19)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 24 8 tensor(0) tensor(8)\n",
      "7 2 4 19 tensor(9) tensor(26)\n",
      "6 7 19 0 tensor(13) tensor(6)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 13 19 0 tensor(13) tensor(0)\n",
      "0 6 26 0 tensor(6) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 2 30 0 tensor(2) tensor(0)\n",
      "0 32 0 0 tensor(32) tensor(0)\n",
      "7 16 9 0 tensor(23) tensor(7)\n",
      "3 0 25 4 tensor(3) tensor(7)\n",
      "18 11 3 0 tensor(29) tensor(18)\n",
      "30 1 1 0 tensor(31) tensor(30)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "18 1 13 0 tensor(19) tensor(18)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "8 11 13 0 tensor(19) tensor(8)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "13 6 13 0 tensor(19) tensor(13)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 3 29 0 tensor(3) tensor(0)\n",
      "0 16 16 0 tensor(16) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "2 13 17 0 tensor(15) tensor(2)\n",
      "2 5 25 0 tensor(7) tensor(2)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "21 1 10 0 tensor(22) tensor(21)\n",
      "5 2 25 0 tensor(7) tensor(5)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 4 28 0 tensor(4) tensor(0)\n",
      "0 17 15 0 tensor(17) tensor(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 17 15 0 tensor(17) tensor(0)\n",
      "4 5 23 0 tensor(9) tensor(4)\n",
      "4 6 22 0 tensor(10) tensor(4)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 19 13 0 tensor(19) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "28 1 3 0 tensor(29) tensor(28)\n",
      "5 15 12 0 tensor(20) tensor(5)\n",
      "0 6 26 0 tensor(6) tensor(0)\n",
      "23 1 8 0 tensor(24) tensor(23)\n",
      "14 7 11 0 tensor(21) tensor(14)\n",
      "6 8 18 0 tensor(14) tensor(6)\n",
      "1 0 25 6 tensor(1) tensor(7)\n",
      "29 3 0 0 tensor(32) tensor(29)\n",
      "0 2 30 0 tensor(2) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 9 23 0 tensor(9) tensor(0)\n",
      "18 10 0 4 tensor(28) tensor(22)\n",
      "7 11 14 0 tensor(18) tensor(7)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "6 13 13 0 tensor(19) tensor(6)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "6 0 20 6 tensor(6) tensor(12)\n",
      "20 6 5 1 tensor(26) tensor(21)\n",
      "8 12 12 0 tensor(20) tensor(8)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "6 0 16 10 tensor(6) tensor(16)\n",
      "26 0 6 0 tensor(26) tensor(26)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 10 22 tensor(0) tensor(22)\n",
      "20 0 3 9 tensor(20) tensor(29)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "7 5 20 0 tensor(12) tensor(7)\n",
      "2 5 25 0 tensor(7) tensor(2)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "17 2 13 0 tensor(19) tensor(17)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 19 13 0 tensor(19) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "7 7 18 0 tensor(14) tensor(7)\n",
      "0 5 27 0 tensor(5) tensor(0)\n",
      "0 19 13 0 tensor(19) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 5 27 0 tensor(5) tensor(0)\n",
      "0 14 18 0 tensor(14) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "2 6 24 0 tensor(8) tensor(2)\n",
      "6 5 21 0 tensor(11) tensor(6)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 19 13 0 tensor(19) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 16 16 0 tensor(16) tensor(0)\n",
      "23 3 6 0 tensor(26) tensor(23)\n",
      "11 9 12 0 tensor(20) tensor(11)\n",
      "2 6 24 0 tensor(8) tensor(2)\n",
      "5 6 21 0 tensor(11) tensor(5)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 19 13 0 tensor(19) tensor(0)\n",
      "0 19 13 0 tensor(19) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "12 5 15 0 tensor(17) tensor(12)\n",
      "0 2 30 0 tensor(2) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 1 31 0 tensor(1) tensor(0)\n",
      "8 10 14 0 tensor(18) tensor(8)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "6 2 24 0 tensor(8) tensor(6)\n",
      "12 18 2 0 tensor(30) tensor(12)\n",
      "0 3 29 0 tensor(3) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 1 31 0 tensor(1) tensor(0)\n",
      "31 1 0 0 tensor(32) tensor(31)\n",
      "0 10 22 0 tensor(10) tensor(0)\n",
      "0 13 19 0 tensor(13) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "9 2 21 0 tensor(11) tensor(9)\n",
      "25 0 7 0 tensor(25) tensor(25)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "18 1 13 0 tensor(19) tensor(18)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "4 0 19 9 tensor(4) tensor(13)\n",
      "21 0 10 1 tensor(21) tensor(22)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 31 1 tensor(0) tensor(1)\n",
      "0 0 6 26 tensor(0) tensor(26)\n",
      "2 14 16 0 tensor(16) tensor(2)\n",
      "0 3 29 0 tensor(3) tensor(0)\n",
      "15 2 15 0 tensor(17) tensor(15)\n",
      "26 0 0 6 tensor(26) tensor(32)\n",
      "6 3 23 0 tensor(9) tensor(6)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 8 24 0 tensor(8) tensor(0)\n",
      "12 11 2 7 tensor(23) tensor(19)\n",
      "13 0 15 4 tensor(13) tensor(17)\n",
      "0 16 16 0 tensor(16) tensor(0)\n",
      "7 13 12 0 tensor(20) tensor(7)\n",
      "0 2 30 0 tensor(2) tensor(0)\n",
      "0 0 29 3 tensor(0) tensor(3)\n",
      "5 3 4 20 tensor(8) tensor(25)\n",
      "12 9 11 0 tensor(21) tensor(12)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "8 5 19 0 tensor(13) tensor(8)\n",
      "12 13 7 0 tensor(25) tensor(12)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "4 4 24 0 tensor(8) tensor(4)\n",
      "4 7 21 0 tensor(11) tensor(4)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "23 2 7 0 tensor(25) tensor(23)\n",
      "21 6 5 0 tensor(27) tensor(21)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "8 6 18 0 tensor(14) tensor(8)\n",
      "17 15 0 0 tensor(32) tensor(17)\n",
      "26 1 0 5 tensor(27) tensor(31)\n",
      "0 0 28 4 tensor(0) tensor(4)\n",
      "3 6 23 0 tensor(9) tensor(3)\n",
      "30 0 0 2 tensor(30) tensor(32)\n",
      "0 0 29 3 tensor(0) tensor(3)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "10 0 13 9 tensor(10) tensor(19)\n",
      "32 0 0 0 tensor(32) tensor(32)\n",
      "25 5 2 0 tensor(30) tensor(25)\n",
      "7 3 19 3 tensor(10) tensor(10)\n",
      "19 0 9 4 tensor(19) tensor(23)\n",
      "8 11 13 0 tensor(19) tensor(8)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "10 0 11 11 tensor(10) tensor(21)\n",
      "13 3 16 0 tensor(16) tensor(13)\n",
      "13 2 9 8 tensor(15) tensor(21)\n",
      "0 5 27 0 tensor(5) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "3 4 25 0 tensor(7) tensor(3)\n",
      "23 4 5 0 tensor(27) tensor(23)\n",
      "27 5 0 0 tensor(32) tensor(27)\n",
      "3 0 21 8 tensor(3) tensor(11)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "9 0 19 4 tensor(9) tensor(13)\n",
      "32 0 0 0 tensor(32) tensor(32)\n",
      "9 0 23 0 tensor(9) tensor(9)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "18 0 14 0 tensor(18) tensor(18)\n",
      "0 1 31 0 tensor(1) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "29 0 1 2 tensor(29) tensor(31)\n",
      "16 3 8 5 tensor(19) tensor(21)\n",
      "9 0 20 3 tensor(9) tensor(12)\n",
      "8 2 22 0 tensor(10) tensor(8)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "9 10 13 0 tensor(19) tensor(9)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "8 6 18 0 tensor(14) tensor(8)\n",
      "14 0 17 1 tensor(14) tensor(15)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "23 0 6 3 tensor(23) tensor(26)\n",
      "31 1 0 0 tensor(32) tensor(31)\n",
      "0 4 28 0 tensor(4) tensor(0)\n",
      "0 19 13 0 tensor(19) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "16 0 10 6 tensor(16) tensor(22)\n",
      "0 3 29 0 tensor(3) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "15 0 14 3 tensor(15) tensor(18)\n",
      "18 7 4 3 tensor(25) tensor(21)\n",
      "12 9 11 0 tensor(21) tensor(12)\n",
      "12 0 18 2 tensor(12) tensor(14)\n",
      "20 12 0 0 tensor(32) tensor(20)\n",
      "5 14 13 0 tensor(19) tensor(5)\n",
      "13 8 11 0 tensor(21) tensor(13)\n",
      "14 5 13 0 tensor(19) tensor(14)\n",
      "6 4 22 0 tensor(10) tensor(6)\n",
      "9 0 22 1 tensor(9) tensor(10)\n",
      "1 0 29 2 tensor(1) tensor(3)\n",
      "16 14 2 0 tensor(30) tensor(16)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 31 1 tensor(0) tensor(1)\n",
      "17 14 0 1 tensor(31) tensor(18)\n",
      "0 1 31 0 tensor(1) tensor(0)\n",
      "9 10 13 0 tensor(19) tensor(9)\n",
      "19 0 2 11 tensor(19) tensor(30)\n",
      "6 2 23 1 tensor(8) tensor(7)\n",
      "32 0 0 0 tensor(32) tensor(32)\n",
      "23 3 0 6 tensor(26) tensor(29)\n",
      "0 3 29 0 tensor(3) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "20 3 7 2 tensor(23) tensor(22)\n",
      "0 29 3 0 tensor(29) tensor(0)\n",
      "5 17 10 0 tensor(22) tensor(5)\n",
      "0 6 26 0 tensor(6) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 25 7 tensor(0) tensor(7)\n",
      "15 4 3 10 tensor(19) tensor(25)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "18 0 11 3 tensor(18) tensor(21)\n",
      "31 1 0 0 tensor(32) tensor(31)\n",
      "0 24 8 0 tensor(24) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "9 10 13 0 tensor(19) tensor(9)\n",
      "3 11 18 0 tensor(14) tensor(3)\n",
      "5 0 24 3 tensor(5) tensor(8)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 19 13 0 tensor(19) tensor(0)\n",
      "0 7 25 0 tensor(7) tensor(0)\n",
      "4 8 9 11 tensor(12) tensor(15)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 31 1 tensor(0) tensor(1)\n",
      "30 0 0 2 tensor(30) tensor(32)\n",
      "1 24 7 0 tensor(25) tensor(1)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "12 7 13 0 tensor(19) tensor(12)\n",
      "20 0 12 0 tensor(20) tensor(20)\n",
      "22 3 7 0 tensor(25) tensor(22)\n",
      "0 0 5 12 tensor(0) tensor(12)\n",
      "5069 3585 1165\n",
      "[1100/1101], prec:0.5857406979431476, recall:0.8131215912736606, f1:68.09511015583021, acc: 0.865121958145214\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "\n",
    "\n",
    "test=data_ds('test')#test dataset\n",
    "test_loader=torch.utils.data.DataLoader(test,batch_size=32)#test dataloader\n",
    "dataset=weight_dir+'train_best' #best weight\n",
    "checkpoint=torch.load(dataset,map_location='cuda:0')#model weight load\n",
    "model.load_state_dict(checkpoint)\n",
    "model.cuda()\n",
    "\n",
    "#test exp\n",
    "model.eval()\n",
    "pred_sum = 0#model output\n",
    "gt_sum = 0#label\n",
    "tp_sum=0\n",
    "fp_sum=0\n",
    "fn_sum=0\n",
    "acc=0\n",
    "sum=0\n",
    "result={}\n",
    "with torch.no_grad():\n",
    "    for it, (data,labels,g) in enumerate(test_loader):\n",
    "        inputs = data.float().cuda()\n",
    "        labels = labels.cuda()\n",
    "        output,_=model(inputs)\n",
    "\n",
    "        TP,FP,TN,FN,pred_len, gt_len,pred=fmeasure(output.cpu(),labels.cpu())\n",
    "        for idx,game_id in enumerate(g):\n",
    "            if game_id not in result.keys():\n",
    "                result[game_id]=pred[idx].tolist()\n",
    "            else:\n",
    "                result[game_id]+=pred[idx].tolist()\n",
    "        print(TP,FP,TN,FN,pred_len, gt_len)\n",
    "        tp_sum += TP\n",
    "        fp_sum += FP\n",
    "        fn_sum += FN\n",
    "        pred_sum += pred_len\n",
    "        gt_sum += gt_len\n",
    "        acc=acc+TP+TN\n",
    "        sum+=len(output)\n",
    "    with open(weight_dir+'/train_result','a') as f:\n",
    "        if tp_sum>0 and fp_sum>0 and fn_sum>0:\n",
    "            precision = tp_sum/(tp_sum+fp_sum)\n",
    "            recall = tp_sum / (tp_sum+fn_sum)\n",
    "            f1 = (2*precision*recall / (precision + recall)) * 100\n",
    "            accuracy=acc/sum\n",
    "            print( tp_sum, fp_sum, fn_sum)\n",
    "            print('[{}/{}], prec:{}, recall:{}, f1:{}, acc: {}'.format(it, len(test_loader), precision, recall, f1,accuracy))\n",
    "            f.write('{}, prec:{}, recall:{}, f1:{}, acc : {}\\n'.format(dataset, precision, recall, f1,accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5775401069518716 0.8503937007874016 0.8806818181818182\n",
      "102844212431058132\n",
      "precision : 0.5775401069518716, recall : 0.8503937007874016, f1 : 0.6878980891719746, accuracy : 0.8806818181818182\n",
      "0.5052410901467506 0.9525691699604744 0.8840037418147801\n",
      "102844341902586509\n",
      "precision : 0.5052410901467506, recall : 0.9525691699604744, f1 : 0.6602739726027398, accuracy : 0.8840037418147801\n",
      "0.609105180533752 0.9651741293532339 0.8851026649191787\n",
      "102844401152267937\n",
      "precision : 0.609105180533752, recall : 0.9651741293532339, f1 : 0.7468719923002887, accuracy : 0.8851026649191787\n",
      "0.6373117033603708 0.8333333333333334 0.8887720220878254\n",
      "102844212430927059\n",
      "precision : 0.6373117033603708, recall : 0.8333333333333334, f1 : 0.7222586999343401, accuracy : 0.8887720220878254\n",
      "0.425531914893617 0.5797101449275363 0.8385214007782101\n",
      "102844412708953395\n",
      "precision : 0.425531914893617, recall : 0.5797101449275363, f1 : 0.4907975460122699, accuracy : 0.8385214007782101\n",
      "0.6171875 0.6388140161725068 0.8626588465298143\n",
      "102844212429944013\n",
      "precision : 0.6171875, recall : 0.6388140161725068, f1 : 0.6278145695364238, accuracy : 0.8626588465298143\n",
      "0.46715328467153283 0.8495575221238938 0.8695876288659794\n",
      "102844341912679064\n",
      "precision : 0.46715328467153283, recall : 0.8495575221238938, f1 : 0.6028257456828886, accuracy : 0.8695876288659794\n",
      "0.5607779578606159 0.8804071246819338 0.8564981949458483\n",
      "102844235753749959\n",
      "precision : 0.5607779578606159, recall : 0.8804071246819338, f1 : 0.6851485148514852, accuracy : 0.8564981949458483\n",
      "0.6300417246175244 0.6874051593323217 0.8381899211518684\n",
      "102844341908026005\n",
      "precision : 0.6300417246175244, recall : 0.6874051593323217, f1 : 0.6574746008708273, accuracy : 0.8381899211518684\n",
      "0.5588752196836555 0.8907563025210085 0.8417030567685589\n",
      "102844283023206486\n",
      "precision : 0.5588752196836555, recall : 0.8907563025210085, f1 : 0.6868250539956803, accuracy : 0.8417030567685589\n",
      "0.5506493506493506 0.6838709677419355 0.8522355507088332\n",
      "102844224147717245\n",
      "precision : 0.5506493506493506, recall : 0.6838709677419355, f1 : 0.6100719424460431, accuracy : 0.8522355507088332\n",
      "0.5490196078431373 0.9120521172638436 0.876144578313253\n",
      "102844412704890154\n",
      "precision : 0.5490196078431373, recall : 0.9120521172638436, f1 : 0.6854345165238678, accuracy : 0.876144578313253\n",
      "0.44372294372294374 0.8686440677966102 0.8766595289079229\n",
      "102844212430599377\n",
      "precision : 0.44372294372294374, recall : 0.8686440677966102, f1 : 0.5873925501432665, accuracy : 0.8766595289079229\n",
      "0.7377049180327869 0.7964601769911505 0.8702830188679245\n",
      "102844412711443769\n",
      "precision : 0.7377049180327869, recall : 0.7964601769911505, f1 : 0.7659574468085107, accuracy : 0.8702830188679245\n",
      "0.655099894847529 0.8593103448275862 0.842375366568915\n",
      "102844235747982779\n",
      "precision : 0.655099894847529, recall : 0.8593103448275862, f1 : 0.743436754176611, accuracy : 0.842375366568915\n",
      "==precision : 0.5683308265210292, recall : 0.8165638851876514, f1 : 0.6640321330038146, accuracy : 0.8642278226273822\n"
     ]
    }
   ],
   "source": [
    "with open('./label/label.pickle',\"rb\") as f4:  \n",
    "    real_result=pickle.load(f4)\n",
    "fmeasure2(result,real_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature 추출 (마지막 LSTM feature)\n",
    "- test, train, validation 모두 진행해야함!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load fin\n",
      "102844412722519367\n",
      "102844212429550795\n",
      "102844401151219358\n",
      "102844401154430631\n",
      "102844412717014335\n",
      "102844401153971877\n",
      "102844224148503678\n",
      "102844412722847048\n",
      "102844401152857762\n",
      "102844412707380528\n",
      "102844212431516886\n",
      "102844283027925085\n",
      "102844412716227901\n",
      "102844412710001974\n",
      "102844294670878922\n",
      "102844294670551241\n",
      "102844283023599703\n",
      "102844412704496937\n",
      "102844235751783874\n",
      "102844401152071328\n",
      "102844412709674293\n",
      "102844401153447587\n",
      "102844224148896895\n",
      "102844235746868664\n",
      "102979081290790284\n",
      "102844283027531868\n",
      "102844212431975640\n",
      "102844401155937960\n",
      "102844212429092040\n",
      "102844341906649746\n",
      "102844412706987311\n",
      "102844412721339716\n",
      "102844212430402768\n",
      "102844341905011343\n",
      "102844235753356742\n",
      "102844235750997440\n",
      "102844412709346612\n",
      "102844412705217835\n",
      "102844235752963525\n",
      "102844412712164667\n",
      "102844412705545516\n",
      "102844341912220311\n",
      "102844341907370644\n",
      "102844235749424575\n",
      "102844212429419722\n",
      "102844294669568199\n",
      "102844212431779031\n",
      "102844294666422466\n",
      "102844224146472059\n",
      "102844212428895431\n",
      "102844212429747404\n",
      "102844235748703677\n",
      "102844224146930812\n",
      "102844212430730450\n",
      "102844294674876621\n",
      "102844341909598870\n",
      "102844283020453971\n",
      "102844294670026952\n",
      "102844412723174729\n",
      "102844341904683662\n",
      "102844283025696858\n",
      "102844235747261881\n",
      "102844401154168486\n",
      "102844235748310460\n",
      "102844412711836986\n",
      "102844412723567946\n",
      "102844235749031358\n",
      "102844294674286796\n",
      "102844294666881219\n",
      "102844412716686654\n",
      "102844294671796427\n",
      "102844224145685626\n",
      "102844412717407552\n",
      "102844235751390657\n",
      "102844401156069033\n",
      "102904869420860038\n",
      "102910307641576395\n",
      "102844341905404560\n",
      "102844341906977427\n",
      "102844212430075086\n",
      "102844412711116088\n",
      "102844401153578660\n",
      "102844294667405508\n",
      "102844412706659630\n",
      "102844212431058132\n",
      "102844341902586509\n",
      "102844401152267937\n",
      "102844212430927059\n",
      "102844412708953395\n",
      "102844212429944013\n",
      "102844341912679064\n",
      "102844235753749959\n",
      "102844341908026005\n",
      "102844283023206486\n",
      "102844224147717245\n",
      "102844412704890154\n",
      "102844212430599377\n",
      "102844412711443769\n",
      "102844235747982779\n"
     ]
    }
   ],
   "source": [
    "save_dir = 'video_feature.pickle' #feature 어디에 저장할지\n",
    "\n",
    "test=data_ds('total')\n",
    "test_loader=torch.utils.data.DataLoader(test,batch_size=1)\n",
    "dataset=weight_dir+'train_best'\n",
    "checkpoint=torch.load(dataset,map_location='cuda:0')\n",
    "model.load_state_dict(checkpoint)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "result={}\n",
    "with torch.no_grad():\n",
    "    for it, (data,labels,g) in enumerate(test_loader):\n",
    "        inputs = data.float().cuda()\n",
    "        labels = labels.cuda()\n",
    "        _,output_f=model(inputs) #feature 만 추출\n",
    "\n",
    "        if g[0] not in result.keys():\n",
    "            print(g[0])\n",
    "            result[g[0]]=[(output_f[0]).tolist()]\n",
    "            \n",
    "        else:\n",
    "            result[g[0]]+=[(output_f[0]).tolist()]\n",
    "\n",
    "with open(weight_dir+save_dir,'wb') as f:\n",
    "    pickle.dump(result,f)       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.35791340470314026,\n",
       "  0.26698946952819824,\n",
       "  -0.09512882679700851,\n",
       "  -0.0047071343287825584,\n",
       "  0.1654966175556183,\n",
       "  0.14154905080795288,\n",
       "  -0.3338702917098999,\n",
       "  -0.12367624789476395,\n",
       "  0.18952010571956635,\n",
       "  -0.11053846031427383,\n",
       "  -0.2618904411792755,\n",
       "  -0.11754820495843887,\n",
       "  0.17547045648097992,\n",
       "  -0.03597959131002426,\n",
       "  0.4411907196044922,\n",
       "  0.014064689166843891,\n",
       "  0.01666095107793808,\n",
       "  0.1770620495080948,\n",
       "  0.3251374661922455,\n",
       "  0.003986166324466467,\n",
       "  0.2823714315891266,\n",
       "  -0.27818432450294495,\n",
       "  0.37367361783981323,\n",
       "  0.042778804898262024,\n",
       "  0.28152137994766235,\n",
       "  0.12793771922588348,\n",
       "  0.2897695004940033,\n",
       "  0.007131745107471943,\n",
       "  0.19353102147579193,\n",
       "  -0.2322344034910202,\n",
       "  -0.13017696142196655,\n",
       "  -0.04692450165748596,\n",
       "  0.21807120740413666,\n",
       "  0.21550297737121582,\n",
       "  -0.08499440550804138,\n",
       "  -0.3457574248313904,\n",
       "  -0.3608167767524719,\n",
       "  0.1919105052947998,\n",
       "  -0.18918801844120026,\n",
       "  0.13556288182735443,\n",
       "  0.24390022456645966,\n",
       "  0.23177573084831238,\n",
       "  0.31884250044822693,\n",
       "  -0.10725637525320053,\n",
       "  -0.2860272228717804,\n",
       "  -0.21971635520458221,\n",
       "  -0.29555580019950867,\n",
       "  0.3177889287471771,\n",
       "  -0.11162906140089035,\n",
       "  0.21935592591762543,\n",
       "  0.08060990273952484,\n",
       "  0.06216880679130554,\n",
       "  -0.11729424446821213,\n",
       "  0.21121692657470703,\n",
       "  -0.12110460549592972,\n",
       "  0.43867769837379456,\n",
       "  -0.2540470063686371,\n",
       "  -0.2394268661737442,\n",
       "  -0.0909498929977417,\n",
       "  0.20677007734775543,\n",
       "  -0.22091518342494965,\n",
       "  -0.3037639260292053,\n",
       "  0.30394017696380615,\n",
       "  0.153153657913208,\n",
       "  -0.35198014974594116,\n",
       "  -0.021952742710709572,\n",
       "  -0.23249979317188263,\n",
       "  -0.2581396996974945,\n",
       "  -0.2827187478542328,\n",
       "  0.36873558163642883,\n",
       "  0.3709706664085388,\n",
       "  -0.08481775969266891,\n",
       "  -0.2627639174461365,\n",
       "  0.4583421051502228,\n",
       "  -0.31300434470176697,\n",
       "  0.3708840310573578,\n",
       "  -0.10305589437484741,\n",
       "  0.3000417947769165,\n",
       "  -0.19622379541397095,\n",
       "  -0.2834450900554657,\n",
       "  0.11438899487257004,\n",
       "  -0.257830947637558,\n",
       "  -0.27232030034065247,\n",
       "  0.1493559032678604,\n",
       "  -0.26720958948135376,\n",
       "  -0.3887597322463989,\n",
       "  0.27934569120407104,\n",
       "  0.028316641226410866,\n",
       "  -0.20324905216693878,\n",
       "  0.237822026014328,\n",
       "  0.1887531876564026,\n",
       "  -0.4624069333076477,\n",
       "  0.042809564620256424,\n",
       "  -0.33468934893608093,\n",
       "  -0.1961463987827301,\n",
       "  -0.019646966829895973,\n",
       "  0.13418740034103394,\n",
       "  0.1526598483324051,\n",
       "  0.3831208050251007,\n",
       "  -0.07263857126235962,\n",
       "  0.030955949798226357,\n",
       "  -0.02819211035966873,\n",
       "  -0.11854786425828934,\n",
       "  -0.1984373927116394,\n",
       "  -0.25410041213035583,\n",
       "  -0.10539098083972931,\n",
       "  0.12409134209156036,\n",
       "  0.08357381075620651,\n",
       "  -0.19571514427661896,\n",
       "  0.19625496864318848,\n",
       "  0.18196646869182587,\n",
       "  0.3224068880081177,\n",
       "  0.20778512954711914,\n",
       "  0.23307889699935913,\n",
       "  0.2136409878730774,\n",
       "  0.309150367975235,\n",
       "  0.3247028887271881,\n",
       "  0.22155316174030304,\n",
       "  0.04499324783682823,\n",
       "  0.16452085971832275,\n",
       "  0.030625561252236366,\n",
       "  0.19001346826553345,\n",
       "  0.19127871096134186,\n",
       "  0.2723590135574341,\n",
       "  0.22710856795310974,\n",
       "  0.22784772515296936,\n",
       "  -0.14124195277690887,\n",
       "  0.3209955394268036],\n",
       " [-0.35791340470314026,\n",
       "  0.26698946952819824,\n",
       "  -0.09512882679700851,\n",
       "  -0.0047071343287825584,\n",
       "  0.1654966175556183,\n",
       "  0.14154905080795288,\n",
       "  -0.3338702917098999,\n",
       "  -0.12367624789476395,\n",
       "  0.18952010571956635,\n",
       "  -0.11053846031427383,\n",
       "  -0.2618904411792755,\n",
       "  -0.11754820495843887,\n",
       "  0.17547045648097992,\n",
       "  -0.03597959131002426,\n",
       "  0.4411907196044922,\n",
       "  0.014064689166843891,\n",
       "  0.01666095107793808,\n",
       "  0.1770620495080948,\n",
       "  0.3251374661922455,\n",
       "  0.003986166324466467,\n",
       "  0.2823714315891266,\n",
       "  -0.27818432450294495,\n",
       "  0.37367361783981323,\n",
       "  0.042778804898262024,\n",
       "  0.28152137994766235,\n",
       "  0.12793771922588348,\n",
       "  0.2897695004940033,\n",
       "  0.007131745107471943,\n",
       "  0.19353102147579193,\n",
       "  -0.2322344034910202,\n",
       "  -0.13017696142196655,\n",
       "  -0.04692450165748596,\n",
       "  0.21807120740413666,\n",
       "  0.21550297737121582,\n",
       "  -0.08499440550804138,\n",
       "  -0.3457574248313904,\n",
       "  -0.3608167767524719,\n",
       "  0.1919105052947998,\n",
       "  -0.18918801844120026,\n",
       "  0.13556288182735443,\n",
       "  0.24390022456645966,\n",
       "  0.23177573084831238,\n",
       "  0.31884250044822693,\n",
       "  -0.10725637525320053,\n",
       "  -0.2860272228717804,\n",
       "  -0.21971635520458221,\n",
       "  -0.29555580019950867,\n",
       "  0.3177889287471771,\n",
       "  -0.11162906140089035,\n",
       "  0.21935592591762543,\n",
       "  0.08060990273952484,\n",
       "  0.06216880679130554,\n",
       "  -0.11729424446821213,\n",
       "  0.21121692657470703,\n",
       "  -0.12110460549592972,\n",
       "  0.43867769837379456,\n",
       "  -0.2540470063686371,\n",
       "  -0.2394268661737442,\n",
       "  -0.0909498929977417,\n",
       "  0.20677007734775543,\n",
       "  -0.22091518342494965,\n",
       "  -0.3037639260292053,\n",
       "  0.30394017696380615,\n",
       "  0.153153657913208,\n",
       "  -0.35198014974594116,\n",
       "  -0.021952742710709572,\n",
       "  -0.23249979317188263,\n",
       "  -0.2581396996974945,\n",
       "  -0.2827187478542328,\n",
       "  0.36873558163642883,\n",
       "  0.3709706664085388,\n",
       "  -0.08481775969266891,\n",
       "  -0.2627639174461365,\n",
       "  0.4583421051502228,\n",
       "  -0.31300434470176697,\n",
       "  0.3708840310573578,\n",
       "  -0.10305589437484741,\n",
       "  0.3000417947769165,\n",
       "  -0.19622379541397095,\n",
       "  -0.2834450900554657,\n",
       "  0.11438899487257004,\n",
       "  -0.257830947637558,\n",
       "  -0.27232030034065247,\n",
       "  0.1493559032678604,\n",
       "  -0.26720958948135376,\n",
       "  -0.3887597322463989,\n",
       "  0.27934569120407104,\n",
       "  0.028316641226410866,\n",
       "  -0.20324905216693878,\n",
       "  0.237822026014328,\n",
       "  0.1887531876564026,\n",
       "  -0.4624069333076477,\n",
       "  0.042809564620256424,\n",
       "  -0.33468934893608093,\n",
       "  -0.1961463987827301,\n",
       "  -0.019646966829895973,\n",
       "  0.13418740034103394,\n",
       "  0.1526598483324051,\n",
       "  0.3831208050251007,\n",
       "  -0.07263857126235962,\n",
       "  0.030955949798226357,\n",
       "  -0.02819211035966873,\n",
       "  -0.11854786425828934,\n",
       "  -0.1984373927116394,\n",
       "  -0.25410041213035583,\n",
       "  -0.10539098083972931,\n",
       "  0.12409134209156036,\n",
       "  0.08357381075620651,\n",
       "  -0.19571514427661896,\n",
       "  0.19625496864318848,\n",
       "  0.18196646869182587,\n",
       "  0.3224068880081177,\n",
       "  0.20778512954711914,\n",
       "  0.23307889699935913,\n",
       "  0.2136409878730774,\n",
       "  0.309150367975235,\n",
       "  0.3247028887271881,\n",
       "  0.22155316174030304,\n",
       "  0.04499324783682823,\n",
       "  0.16452085971832275,\n",
       "  0.030625561252236366,\n",
       "  0.19001346826553345,\n",
       "  0.19127871096134186,\n",
       "  0.2723590135574341,\n",
       "  0.22710856795310974,\n",
       "  0.22784772515296936,\n",
       "  -0.14124195277690887,\n",
       "  0.3209955394268036],\n",
       " [-0.35791340470314026,\n",
       "  0.26698946952819824,\n",
       "  -0.09512882679700851,\n",
       "  -0.0047071343287825584,\n",
       "  0.1654966175556183,\n",
       "  0.14154905080795288,\n",
       "  -0.3338702917098999,\n",
       "  -0.12367624789476395,\n",
       "  0.18952010571956635,\n",
       "  -0.11053846031427383,\n",
       "  -0.2618904411792755,\n",
       "  -0.11754820495843887,\n",
       "  0.17547045648097992,\n",
       "  -0.03597959131002426,\n",
       "  0.4411907196044922,\n",
       "  0.014064689166843891,\n",
       "  0.01666095107793808,\n",
       "  0.1770620495080948,\n",
       "  0.3251374661922455,\n",
       "  0.003986166324466467,\n",
       "  0.2823714315891266,\n",
       "  -0.27818432450294495,\n",
       "  0.37367361783981323,\n",
       "  0.042778804898262024,\n",
       "  0.28152137994766235,\n",
       "  0.12793771922588348,\n",
       "  0.2897695004940033,\n",
       "  0.007131745107471943,\n",
       "  0.19353102147579193,\n",
       "  -0.2322344034910202,\n",
       "  -0.13017696142196655,\n",
       "  -0.04692450165748596,\n",
       "  0.21807120740413666,\n",
       "  0.21550297737121582,\n",
       "  -0.08499440550804138,\n",
       "  -0.3457574248313904,\n",
       "  -0.3608167767524719,\n",
       "  0.1919105052947998,\n",
       "  -0.18918801844120026,\n",
       "  0.13556288182735443,\n",
       "  0.24390022456645966,\n",
       "  0.23177573084831238,\n",
       "  0.31884250044822693,\n",
       "  -0.10725637525320053,\n",
       "  -0.2860272228717804,\n",
       "  -0.21971635520458221,\n",
       "  -0.29555580019950867,\n",
       "  0.3177889287471771,\n",
       "  -0.11162906140089035,\n",
       "  0.21935592591762543,\n",
       "  0.08060990273952484,\n",
       "  0.06216880679130554,\n",
       "  -0.11729424446821213,\n",
       "  0.21121692657470703,\n",
       "  -0.12110460549592972,\n",
       "  0.43867769837379456,\n",
       "  -0.2540470063686371,\n",
       "  -0.2394268661737442,\n",
       "  -0.0909498929977417,\n",
       "  0.20677007734775543,\n",
       "  -0.22091518342494965,\n",
       "  -0.3037639260292053,\n",
       "  0.30394017696380615,\n",
       "  0.153153657913208,\n",
       "  -0.35198014974594116,\n",
       "  -0.021952742710709572,\n",
       "  -0.23249979317188263,\n",
       "  -0.2581396996974945,\n",
       "  -0.2827187478542328,\n",
       "  0.36873558163642883,\n",
       "  0.3709706664085388,\n",
       "  -0.08481775969266891,\n",
       "  -0.2627639174461365,\n",
       "  0.4583421051502228,\n",
       "  -0.31300434470176697,\n",
       "  0.3708840310573578,\n",
       "  -0.10305589437484741,\n",
       "  0.3000417947769165,\n",
       "  -0.19622379541397095,\n",
       "  -0.2834450900554657,\n",
       "  0.11438899487257004,\n",
       "  -0.257830947637558,\n",
       "  -0.27232030034065247,\n",
       "  0.1493559032678604,\n",
       "  -0.26720958948135376,\n",
       "  -0.3887597322463989,\n",
       "  0.27934569120407104,\n",
       "  0.028316641226410866,\n",
       "  -0.20324905216693878,\n",
       "  0.237822026014328,\n",
       "  0.1887531876564026,\n",
       "  -0.4624069333076477,\n",
       "  0.042809564620256424,\n",
       "  -0.33468934893608093,\n",
       "  -0.1961463987827301,\n",
       "  -0.019646966829895973,\n",
       "  0.13418740034103394,\n",
       "  0.1526598483324051,\n",
       "  0.3831208050251007,\n",
       "  -0.07263857126235962,\n",
       "  0.030955949798226357,\n",
       "  -0.02819211035966873,\n",
       "  -0.11854786425828934,\n",
       "  -0.1984373927116394,\n",
       "  -0.25410041213035583,\n",
       "  -0.10539098083972931,\n",
       "  0.12409134209156036,\n",
       "  0.08357381075620651,\n",
       "  -0.19571514427661896,\n",
       "  0.19625496864318848,\n",
       "  0.18196646869182587,\n",
       "  0.3224068880081177,\n",
       "  0.20778512954711914,\n",
       "  0.23307889699935913,\n",
       "  0.2136409878730774,\n",
       "  0.309150367975235,\n",
       "  0.3247028887271881,\n",
       "  0.22155316174030304,\n",
       "  0.04499324783682823,\n",
       "  0.16452085971832275,\n",
       "  0.030625561252236366,\n",
       "  0.19001346826553345,\n",
       "  0.19127871096134186,\n",
       "  0.2723590135574341,\n",
       "  0.22710856795310974,\n",
       "  0.22784772515296936,\n",
       "  -0.14124195277690887,\n",
       "  0.3209955394268036],\n",
       " [-0.35791340470314026,\n",
       "  0.26698946952819824,\n",
       "  -0.09512882679700851,\n",
       "  -0.0047071343287825584,\n",
       "  0.1654966175556183,\n",
       "  0.14154905080795288,\n",
       "  -0.3338702917098999,\n",
       "  -0.12367624789476395,\n",
       "  0.18952010571956635,\n",
       "  -0.11053846031427383,\n",
       "  -0.2618904411792755,\n",
       "  -0.11754820495843887,\n",
       "  0.17547045648097992,\n",
       "  -0.03597959131002426,\n",
       "  0.4411907196044922,\n",
       "  0.014064689166843891,\n",
       "  0.01666095107793808,\n",
       "  0.1770620495080948,\n",
       "  0.3251374661922455,\n",
       "  0.003986166324466467,\n",
       "  0.2823714315891266,\n",
       "  -0.27818432450294495,\n",
       "  0.37367361783981323,\n",
       "  0.042778804898262024,\n",
       "  0.28152137994766235,\n",
       "  0.12793771922588348,\n",
       "  0.2897695004940033,\n",
       "  0.007131745107471943,\n",
       "  0.19353102147579193,\n",
       "  -0.2322344034910202,\n",
       "  -0.13017696142196655,\n",
       "  -0.04692450165748596,\n",
       "  0.21807120740413666,\n",
       "  0.21550297737121582,\n",
       "  -0.08499440550804138,\n",
       "  -0.3457574248313904,\n",
       "  -0.3608167767524719,\n",
       "  0.1919105052947998,\n",
       "  -0.18918801844120026,\n",
       "  0.13556288182735443,\n",
       "  0.24390022456645966,\n",
       "  0.23177573084831238,\n",
       "  0.31884250044822693,\n",
       "  -0.10725637525320053,\n",
       "  -0.2860272228717804,\n",
       "  -0.21971635520458221,\n",
       "  -0.29555580019950867,\n",
       "  0.3177889287471771,\n",
       "  -0.11162906140089035,\n",
       "  0.21935592591762543,\n",
       "  0.08060990273952484,\n",
       "  0.06216880679130554,\n",
       "  -0.11729424446821213,\n",
       "  0.21121692657470703,\n",
       "  -0.12110460549592972,\n",
       "  0.43867769837379456,\n",
       "  -0.2540470063686371,\n",
       "  -0.2394268661737442,\n",
       "  -0.0909498929977417,\n",
       "  0.20677007734775543,\n",
       "  -0.22091518342494965,\n",
       "  -0.3037639260292053,\n",
       "  0.30394017696380615,\n",
       "  0.153153657913208,\n",
       "  -0.35198014974594116,\n",
       "  -0.021952742710709572,\n",
       "  -0.23249979317188263,\n",
       "  -0.2581396996974945,\n",
       "  -0.2827187478542328,\n",
       "  0.36873558163642883,\n",
       "  0.3709706664085388,\n",
       "  -0.08481775969266891,\n",
       "  -0.2627639174461365,\n",
       "  0.4583421051502228,\n",
       "  -0.31300434470176697,\n",
       "  0.3708840310573578,\n",
       "  -0.10305589437484741,\n",
       "  0.3000417947769165,\n",
       "  -0.19622379541397095,\n",
       "  -0.2834450900554657,\n",
       "  0.11438899487257004,\n",
       "  -0.257830947637558,\n",
       "  -0.27232030034065247,\n",
       "  0.1493559032678604,\n",
       "  -0.26720958948135376,\n",
       "  -0.3887597322463989,\n",
       "  0.27934569120407104,\n",
       "  0.028316641226410866,\n",
       "  -0.20324905216693878,\n",
       "  0.237822026014328,\n",
       "  0.1887531876564026,\n",
       "  -0.4624069333076477,\n",
       "  0.042809564620256424,\n",
       "  -0.33468934893608093,\n",
       "  -0.1961463987827301,\n",
       "  -0.019646966829895973,\n",
       "  0.13418740034103394,\n",
       "  0.1526598483324051,\n",
       "  0.3831208050251007,\n",
       "  -0.07263857126235962,\n",
       "  0.030955949798226357,\n",
       "  -0.02819211035966873,\n",
       "  -0.11854786425828934,\n",
       "  -0.1984373927116394,\n",
       "  -0.25410041213035583,\n",
       "  -0.10539098083972931,\n",
       "  0.12409134209156036,\n",
       "  0.08357381075620651,\n",
       "  -0.19571514427661896,\n",
       "  0.19625496864318848,\n",
       "  0.18196646869182587,\n",
       "  0.3224068880081177,\n",
       "  0.20778512954711914,\n",
       "  0.23307889699935913,\n",
       "  0.2136409878730774,\n",
       "  0.309150367975235,\n",
       "  0.3247028887271881,\n",
       "  0.22155316174030304,\n",
       "  0.04499324783682823,\n",
       "  0.16452085971832275,\n",
       "  0.030625561252236366,\n",
       "  0.19001346826553345,\n",
       "  0.19127871096134186,\n",
       "  0.2723590135574341,\n",
       "  0.22710856795310974,\n",
       "  0.22784772515296936,\n",
       "  -0.14124195277690887,\n",
       "  0.3209955394268036],\n",
       " [-0.35791340470314026,\n",
       "  0.26698946952819824,\n",
       "  -0.09512882679700851,\n",
       "  -0.0047071343287825584,\n",
       "  0.1654966175556183,\n",
       "  0.14154905080795288,\n",
       "  -0.3338702917098999,\n",
       "  -0.12367624789476395,\n",
       "  0.18952010571956635,\n",
       "  -0.11053846031427383,\n",
       "  -0.2618904411792755,\n",
       "  -0.11754820495843887,\n",
       "  0.17547045648097992,\n",
       "  -0.03597959131002426,\n",
       "  0.4411907196044922,\n",
       "  0.014064689166843891,\n",
       "  0.01666095107793808,\n",
       "  0.1770620495080948,\n",
       "  0.3251374661922455,\n",
       "  0.003986166324466467,\n",
       "  0.2823714315891266,\n",
       "  -0.27818432450294495,\n",
       "  0.37367361783981323,\n",
       "  0.042778804898262024,\n",
       "  0.28152137994766235,\n",
       "  0.12793771922588348,\n",
       "  0.2897695004940033,\n",
       "  0.007131745107471943,\n",
       "  0.19353102147579193,\n",
       "  -0.2322344034910202,\n",
       "  -0.13017696142196655,\n",
       "  -0.04692450165748596,\n",
       "  0.21807120740413666,\n",
       "  0.21550297737121582,\n",
       "  -0.08499440550804138,\n",
       "  -0.3457574248313904,\n",
       "  -0.3608167767524719,\n",
       "  0.1919105052947998,\n",
       "  -0.18918801844120026,\n",
       "  0.13556288182735443,\n",
       "  0.24390022456645966,\n",
       "  0.23177573084831238,\n",
       "  0.31884250044822693,\n",
       "  -0.10725637525320053,\n",
       "  -0.2860272228717804,\n",
       "  -0.21971635520458221,\n",
       "  -0.29555580019950867,\n",
       "  0.3177889287471771,\n",
       "  -0.11162906140089035,\n",
       "  0.21935592591762543,\n",
       "  0.08060990273952484,\n",
       "  0.06216880679130554,\n",
       "  -0.11729424446821213,\n",
       "  0.21121692657470703,\n",
       "  -0.12110460549592972,\n",
       "  0.43867769837379456,\n",
       "  -0.2540470063686371,\n",
       "  -0.2394268661737442,\n",
       "  -0.0909498929977417,\n",
       "  0.20677007734775543,\n",
       "  -0.22091518342494965,\n",
       "  -0.3037639260292053,\n",
       "  0.30394017696380615,\n",
       "  0.153153657913208,\n",
       "  -0.35198014974594116,\n",
       "  -0.021952742710709572,\n",
       "  -0.23249979317188263,\n",
       "  -0.2581396996974945,\n",
       "  -0.2827187478542328,\n",
       "  0.36873558163642883,\n",
       "  0.3709706664085388,\n",
       "  -0.08481775969266891,\n",
       "  -0.2627639174461365,\n",
       "  0.4583421051502228,\n",
       "  -0.31300434470176697,\n",
       "  0.3708840310573578,\n",
       "  -0.10305589437484741,\n",
       "  0.3000417947769165,\n",
       "  -0.19622379541397095,\n",
       "  -0.2834450900554657,\n",
       "  0.11438899487257004,\n",
       "  -0.257830947637558,\n",
       "  -0.27232030034065247,\n",
       "  0.1493559032678604,\n",
       "  -0.26720958948135376,\n",
       "  -0.3887597322463989,\n",
       "  0.27934569120407104,\n",
       "  0.028316641226410866,\n",
       "  -0.20324905216693878,\n",
       "  0.237822026014328,\n",
       "  0.1887531876564026,\n",
       "  -0.4624069333076477,\n",
       "  0.042809564620256424,\n",
       "  -0.33468934893608093,\n",
       "  -0.1961463987827301,\n",
       "  -0.019646966829895973,\n",
       "  0.13418740034103394,\n",
       "  0.1526598483324051,\n",
       "  0.3831208050251007,\n",
       "  -0.07263857126235962,\n",
       "  0.030955949798226357,\n",
       "  -0.02819211035966873,\n",
       "  -0.11854786425828934,\n",
       "  -0.1984373927116394,\n",
       "  -0.25410041213035583,\n",
       "  -0.10539098083972931,\n",
       "  0.12409134209156036,\n",
       "  0.08357381075620651,\n",
       "  -0.19571514427661896,\n",
       "  0.19625496864318848,\n",
       "  0.18196646869182587,\n",
       "  0.3224068880081177,\n",
       "  0.20778512954711914,\n",
       "  0.23307889699935913,\n",
       "  0.2136409878730774,\n",
       "  0.309150367975235,\n",
       "  0.3247028887271881,\n",
       "  0.22155316174030304,\n",
       "  0.04499324783682823,\n",
       "  0.16452085971832275,\n",
       "  0.030625561252236366,\n",
       "  0.19001346826553345,\n",
       "  0.19127871096134186,\n",
       "  0.2723590135574341,\n",
       "  0.22710856795310974,\n",
       "  0.22784772515296936,\n",
       "  -0.14124195277690887,\n",
       "  0.3209955394268036],\n",
       " [-0.35791340470314026,\n",
       "  0.26698946952819824,\n",
       "  -0.09512882679700851,\n",
       "  -0.0047071343287825584,\n",
       "  0.1654966175556183,\n",
       "  0.14154905080795288,\n",
       "  -0.3338702917098999,\n",
       "  -0.12367624789476395,\n",
       "  0.18952010571956635,\n",
       "  -0.11053846031427383,\n",
       "  -0.2618904411792755,\n",
       "  -0.11754820495843887,\n",
       "  0.17547045648097992,\n",
       "  -0.03597959131002426,\n",
       "  0.4411907196044922,\n",
       "  0.014064689166843891,\n",
       "  0.01666095107793808,\n",
       "  0.1770620495080948,\n",
       "  0.3251374661922455,\n",
       "  0.003986166324466467,\n",
       "  0.2823714315891266,\n",
       "  -0.27818432450294495,\n",
       "  0.37367361783981323,\n",
       "  0.042778804898262024,\n",
       "  0.28152137994766235,\n",
       "  0.12793771922588348,\n",
       "  0.2897695004940033,\n",
       "  0.007131745107471943,\n",
       "  0.19353102147579193,\n",
       "  -0.2322344034910202,\n",
       "  -0.13017696142196655,\n",
       "  -0.04692450165748596,\n",
       "  0.21807120740413666,\n",
       "  0.21550297737121582,\n",
       "  -0.08499440550804138,\n",
       "  -0.3457574248313904,\n",
       "  -0.3608167767524719,\n",
       "  0.1919105052947998,\n",
       "  -0.18918801844120026,\n",
       "  0.13556288182735443,\n",
       "  0.24390022456645966,\n",
       "  0.23177573084831238,\n",
       "  0.31884250044822693,\n",
       "  -0.10725637525320053,\n",
       "  -0.2860272228717804,\n",
       "  -0.21971635520458221,\n",
       "  -0.29555580019950867,\n",
       "  0.3177889287471771,\n",
       "  -0.11162906140089035,\n",
       "  0.21935592591762543,\n",
       "  0.08060990273952484,\n",
       "  0.06216880679130554,\n",
       "  -0.11729424446821213,\n",
       "  0.21121692657470703,\n",
       "  -0.12110460549592972,\n",
       "  0.43867769837379456,\n",
       "  -0.2540470063686371,\n",
       "  -0.2394268661737442,\n",
       "  -0.0909498929977417,\n",
       "  0.20677007734775543,\n",
       "  -0.22091518342494965,\n",
       "  -0.3037639260292053,\n",
       "  0.30394017696380615,\n",
       "  0.153153657913208,\n",
       "  -0.35198014974594116,\n",
       "  -0.021952742710709572,\n",
       "  -0.23249979317188263,\n",
       "  -0.2581396996974945,\n",
       "  -0.2827187478542328,\n",
       "  0.36873558163642883,\n",
       "  0.3709706664085388,\n",
       "  -0.08481775969266891,\n",
       "  -0.2627639174461365,\n",
       "  0.4583421051502228,\n",
       "  -0.31300434470176697,\n",
       "  0.3708840310573578,\n",
       "  -0.10305589437484741,\n",
       "  0.3000417947769165,\n",
       "  -0.19622379541397095,\n",
       "  -0.2834450900554657,\n",
       "  0.11438899487257004,\n",
       "  -0.257830947637558,\n",
       "  -0.27232030034065247,\n",
       "  0.1493559032678604,\n",
       "  -0.26720958948135376,\n",
       "  -0.3887597322463989,\n",
       "  0.27934569120407104,\n",
       "  0.028316641226410866,\n",
       "  -0.20324905216693878,\n",
       "  0.237822026014328,\n",
       "  0.1887531876564026,\n",
       "  -0.4624069333076477,\n",
       "  0.042809564620256424,\n",
       "  -0.33468934893608093,\n",
       "  -0.1961463987827301,\n",
       "  -0.019646966829895973,\n",
       "  0.13418740034103394,\n",
       "  0.1526598483324051,\n",
       "  0.3831208050251007,\n",
       "  -0.07263857126235962,\n",
       "  0.030955949798226357,\n",
       "  -0.02819211035966873,\n",
       "  -0.11854786425828934,\n",
       "  -0.1984373927116394,\n",
       "  -0.25410041213035583,\n",
       "  -0.10539098083972931,\n",
       "  0.12409134209156036,\n",
       "  0.08357381075620651,\n",
       "  -0.19571514427661896,\n",
       "  0.19625496864318848,\n",
       "  0.18196646869182587,\n",
       "  0.3224068880081177,\n",
       "  0.20778512954711914,\n",
       "  0.23307889699935913,\n",
       "  0.2136409878730774,\n",
       "  0.309150367975235,\n",
       "  0.3247028887271881,\n",
       "  0.22155316174030304,\n",
       "  0.04499324783682823,\n",
       "  0.16452085971832275,\n",
       "  0.030625561252236366,\n",
       "  0.19001346826553345,\n",
       "  0.19127871096134186,\n",
       "  0.2723590135574341,\n",
       "  0.22710856795310974,\n",
       "  0.22784772515296936,\n",
       "  -0.14124195277690887,\n",
       "  0.3209955394268036],\n",
       " [-0.35791340470314026,\n",
       "  0.26698946952819824,\n",
       "  -0.09512882679700851,\n",
       "  -0.0047071343287825584,\n",
       "  0.1654966175556183,\n",
       "  0.14154905080795288,\n",
       "  -0.3338702917098999,\n",
       "  -0.12367624789476395,\n",
       "  0.18952010571956635,\n",
       "  -0.11053846031427383,\n",
       "  -0.2618904411792755,\n",
       "  -0.11754820495843887,\n",
       "  0.17547045648097992,\n",
       "  -0.03597959131002426,\n",
       "  0.4411907196044922,\n",
       "  0.014064689166843891,\n",
       "  0.01666095107793808,\n",
       "  0.1770620495080948,\n",
       "  0.3251374661922455,\n",
       "  0.003986166324466467,\n",
       "  0.2823714315891266,\n",
       "  -0.27818432450294495,\n",
       "  0.37367361783981323,\n",
       "  0.042778804898262024,\n",
       "  0.28152137994766235,\n",
       "  0.12793771922588348,\n",
       "  0.2897695004940033,\n",
       "  0.007131745107471943,\n",
       "  0.19353102147579193,\n",
       "  -0.2322344034910202,\n",
       "  -0.13017696142196655,\n",
       "  -0.04692450165748596,\n",
       "  0.21807120740413666,\n",
       "  0.21550297737121582,\n",
       "  -0.08499440550804138,\n",
       "  -0.3457574248313904,\n",
       "  -0.3608167767524719,\n",
       "  0.1919105052947998,\n",
       "  -0.18918801844120026,\n",
       "  0.13556288182735443,\n",
       "  0.24390022456645966,\n",
       "  0.23177573084831238,\n",
       "  0.31884250044822693,\n",
       "  -0.10725637525320053,\n",
       "  -0.2860272228717804,\n",
       "  -0.21971635520458221,\n",
       "  -0.29555580019950867,\n",
       "  0.3177889287471771,\n",
       "  -0.11162906140089035,\n",
       "  0.21935592591762543,\n",
       "  0.08060990273952484,\n",
       "  0.06216880679130554,\n",
       "  -0.11729424446821213,\n",
       "  0.21121692657470703,\n",
       "  -0.12110460549592972,\n",
       "  0.43867769837379456,\n",
       "  -0.2540470063686371,\n",
       "  -0.2394268661737442,\n",
       "  -0.0909498929977417,\n",
       "  0.20677007734775543,\n",
       "  -0.22091518342494965,\n",
       "  -0.3037639260292053,\n",
       "  0.30394017696380615,\n",
       "  0.153153657913208,\n",
       "  -0.35198014974594116,\n",
       "  -0.021952742710709572,\n",
       "  -0.23249979317188263,\n",
       "  -0.2581396996974945,\n",
       "  -0.2827187478542328,\n",
       "  0.36873558163642883,\n",
       "  0.3709706664085388,\n",
       "  -0.08481775969266891,\n",
       "  -0.2627639174461365,\n",
       "  0.4583421051502228,\n",
       "  -0.31300434470176697,\n",
       "  0.3708840310573578,\n",
       "  -0.10305589437484741,\n",
       "  0.3000417947769165,\n",
       "  -0.19622379541397095,\n",
       "  -0.2834450900554657,\n",
       "  0.11438899487257004,\n",
       "  -0.257830947637558,\n",
       "  -0.27232030034065247,\n",
       "  0.1493559032678604,\n",
       "  -0.26720958948135376,\n",
       "  -0.3887597322463989,\n",
       "  0.27934569120407104,\n",
       "  0.028316641226410866,\n",
       "  -0.20324905216693878,\n",
       "  0.237822026014328,\n",
       "  0.1887531876564026,\n",
       "  -0.4624069333076477,\n",
       "  0.042809564620256424,\n",
       "  -0.33468934893608093,\n",
       "  -0.1961463987827301,\n",
       "  -0.019646966829895973,\n",
       "  0.13418740034103394,\n",
       "  0.1526598483324051,\n",
       "  0.3831208050251007,\n",
       "  -0.07263857126235962,\n",
       "  0.030955949798226357,\n",
       "  -0.02819211035966873,\n",
       "  -0.11854786425828934,\n",
       "  -0.1984373927116394,\n",
       "  -0.25410041213035583,\n",
       "  -0.10539098083972931,\n",
       "  0.12409134209156036,\n",
       "  0.08357381075620651,\n",
       "  -0.19571514427661896,\n",
       "  0.19625496864318848,\n",
       "  0.18196646869182587,\n",
       "  0.3224068880081177,\n",
       "  0.20778512954711914,\n",
       "  0.23307889699935913,\n",
       "  0.2136409878730774,\n",
       "  0.309150367975235,\n",
       "  0.3247028887271881,\n",
       "  0.22155316174030304,\n",
       "  0.04499324783682823,\n",
       "  0.16452085971832275,\n",
       "  0.030625561252236366,\n",
       "  0.19001346826553345,\n",
       "  0.19127871096134186,\n",
       "  0.2723590135574341,\n",
       "  0.22710856795310974,\n",
       "  0.22784772515296936,\n",
       "  -0.14124195277690887,\n",
       "  0.3209955394268036],\n",
       " [-0.35791340470314026,\n",
       "  0.26698946952819824,\n",
       "  -0.09512882679700851,\n",
       "  -0.0047071343287825584,\n",
       "  0.1654966175556183,\n",
       "  0.14154905080795288,\n",
       "  -0.3338702917098999,\n",
       "  -0.12367624789476395,\n",
       "  0.18952010571956635,\n",
       "  -0.11053846031427383,\n",
       "  -0.2618904411792755,\n",
       "  -0.11754820495843887,\n",
       "  0.17547045648097992,\n",
       "  -0.03597959131002426,\n",
       "  0.4411907196044922,\n",
       "  0.014064689166843891,\n",
       "  0.01666095107793808,\n",
       "  0.1770620495080948,\n",
       "  0.3251374661922455,\n",
       "  0.003986166324466467,\n",
       "  0.2823714315891266,\n",
       "  -0.27818432450294495,\n",
       "  0.37367361783981323,\n",
       "  0.042778804898262024,\n",
       "  0.28152137994766235,\n",
       "  0.12793771922588348,\n",
       "  0.2897695004940033,\n",
       "  0.007131745107471943,\n",
       "  0.19353102147579193,\n",
       "  -0.2322344034910202,\n",
       "  -0.13017696142196655,\n",
       "  -0.04692450165748596,\n",
       "  0.21807120740413666,\n",
       "  0.21550297737121582,\n",
       "  -0.08499440550804138,\n",
       "  -0.3457574248313904,\n",
       "  -0.3608167767524719,\n",
       "  0.1919105052947998,\n",
       "  -0.18918801844120026,\n",
       "  0.13556288182735443,\n",
       "  0.24390022456645966,\n",
       "  0.23177573084831238,\n",
       "  0.31884250044822693,\n",
       "  -0.10725637525320053,\n",
       "  -0.2860272228717804,\n",
       "  -0.21971635520458221,\n",
       "  -0.29555580019950867,\n",
       "  0.3177889287471771,\n",
       "  -0.11162906140089035,\n",
       "  0.21935592591762543,\n",
       "  0.08060990273952484,\n",
       "  0.06216880679130554,\n",
       "  -0.11729424446821213,\n",
       "  0.21121692657470703,\n",
       "  -0.12110460549592972,\n",
       "  0.43867769837379456,\n",
       "  -0.2540470063686371,\n",
       "  -0.2394268661737442,\n",
       "  -0.0909498929977417,\n",
       "  0.20677007734775543,\n",
       "  -0.22091518342494965,\n",
       "  -0.3037639260292053,\n",
       "  0.30394017696380615,\n",
       "  0.153153657913208,\n",
       "  -0.35198014974594116,\n",
       "  -0.021952742710709572,\n",
       "  -0.23249979317188263,\n",
       "  -0.2581396996974945,\n",
       "  -0.2827187478542328,\n",
       "  0.36873558163642883,\n",
       "  0.3709706664085388,\n",
       "  -0.08481775969266891,\n",
       "  -0.2627639174461365,\n",
       "  0.4583421051502228,\n",
       "  -0.31300434470176697,\n",
       "  0.3708840310573578,\n",
       "  -0.10305589437484741,\n",
       "  0.3000417947769165,\n",
       "  -0.19622379541397095,\n",
       "  -0.2834450900554657,\n",
       "  0.11438899487257004,\n",
       "  -0.257830947637558,\n",
       "  -0.27232030034065247,\n",
       "  0.1493559032678604,\n",
       "  -0.26720958948135376,\n",
       "  -0.3887597322463989,\n",
       "  0.27934569120407104,\n",
       "  0.028316641226410866,\n",
       "  -0.20324905216693878,\n",
       "  0.237822026014328,\n",
       "  0.1887531876564026,\n",
       "  -0.4624069333076477,\n",
       "  0.042809564620256424,\n",
       "  -0.33468934893608093,\n",
       "  -0.1961463987827301,\n",
       "  -0.019646966829895973,\n",
       "  0.13418740034103394,\n",
       "  0.1526598483324051,\n",
       "  0.3831208050251007,\n",
       "  -0.07263857126235962,\n",
       "  0.030955949798226357,\n",
       "  -0.02819211035966873,\n",
       "  -0.11854786425828934,\n",
       "  -0.1984373927116394,\n",
       "  -0.25410041213035583,\n",
       "  -0.10539098083972931,\n",
       "  0.12409134209156036,\n",
       "  0.08357381075620651,\n",
       "  -0.19571514427661896,\n",
       "  0.19625496864318848,\n",
       "  0.18196646869182587,\n",
       "  0.3224068880081177,\n",
       "  0.20778512954711914,\n",
       "  0.23307889699935913,\n",
       "  0.2136409878730774,\n",
       "  0.309150367975235,\n",
       "  0.3247028887271881,\n",
       "  0.22155316174030304,\n",
       "  0.04499324783682823,\n",
       "  0.16452085971832275,\n",
       "  0.030625561252236366,\n",
       "  0.19001346826553345,\n",
       "  0.19127871096134186,\n",
       "  0.2723590135574341,\n",
       "  0.22710856795310974,\n",
       "  0.22784772515296936,\n",
       "  -0.14124195277690887,\n",
       "  0.3209955394268036],\n",
       " [-0.35791340470314026,\n",
       "  0.26698946952819824,\n",
       "  -0.09512882679700851,\n",
       "  -0.0047071343287825584,\n",
       "  0.1654966175556183,\n",
       "  0.14154905080795288,\n",
       "  -0.3338702917098999,\n",
       "  -0.12367624789476395,\n",
       "  0.18952010571956635,\n",
       "  -0.11053846031427383,\n",
       "  -0.2618904411792755,\n",
       "  -0.11754820495843887,\n",
       "  0.17547045648097992,\n",
       "  -0.03597959131002426,\n",
       "  0.4411907196044922,\n",
       "  0.014064689166843891,\n",
       "  0.01666095107793808,\n",
       "  0.1770620495080948,\n",
       "  0.3251374661922455,\n",
       "  0.003986166324466467,\n",
       "  0.2823714315891266,\n",
       "  -0.27818432450294495,\n",
       "  0.37367361783981323,\n",
       "  0.042778804898262024,\n",
       "  0.28152137994766235,\n",
       "  0.12793771922588348,\n",
       "  0.2897695004940033,\n",
       "  0.007131745107471943,\n",
       "  0.19353102147579193,\n",
       "  -0.2322344034910202,\n",
       "  -0.13017696142196655,\n",
       "  -0.04692450165748596,\n",
       "  0.21807120740413666,\n",
       "  0.21550297737121582,\n",
       "  -0.08499440550804138,\n",
       "  -0.3457574248313904,\n",
       "  -0.3608167767524719,\n",
       "  0.1919105052947998,\n",
       "  -0.18918801844120026,\n",
       "  0.13556288182735443,\n",
       "  0.24390022456645966,\n",
       "  0.23177573084831238,\n",
       "  0.31884250044822693,\n",
       "  -0.10725637525320053,\n",
       "  -0.2860272228717804,\n",
       "  -0.21971635520458221,\n",
       "  -0.29555580019950867,\n",
       "  0.3177889287471771,\n",
       "  -0.11162906140089035,\n",
       "  0.21935592591762543,\n",
       "  0.08060990273952484,\n",
       "  0.06216880679130554,\n",
       "  -0.11729424446821213,\n",
       "  0.21121692657470703,\n",
       "  -0.12110460549592972,\n",
       "  0.43867769837379456,\n",
       "  -0.2540470063686371,\n",
       "  -0.2394268661737442,\n",
       "  -0.0909498929977417,\n",
       "  0.20677007734775543,\n",
       "  -0.22091518342494965,\n",
       "  -0.3037639260292053,\n",
       "  0.30394017696380615,\n",
       "  0.153153657913208,\n",
       "  -0.35198014974594116,\n",
       "  -0.021952742710709572,\n",
       "  -0.23249979317188263,\n",
       "  -0.2581396996974945,\n",
       "  -0.2827187478542328,\n",
       "  0.36873558163642883,\n",
       "  0.3709706664085388,\n",
       "  -0.08481775969266891,\n",
       "  -0.2627639174461365,\n",
       "  0.4583421051502228,\n",
       "  -0.31300434470176697,\n",
       "  0.3708840310573578,\n",
       "  -0.10305589437484741,\n",
       "  0.3000417947769165,\n",
       "  -0.19622379541397095,\n",
       "  -0.2834450900554657,\n",
       "  0.11438899487257004,\n",
       "  -0.257830947637558,\n",
       "  -0.27232030034065247,\n",
       "  0.1493559032678604,\n",
       "  -0.26720958948135376,\n",
       "  -0.3887597322463989,\n",
       "  0.27934569120407104,\n",
       "  0.028316641226410866,\n",
       "  -0.20324905216693878,\n",
       "  0.237822026014328,\n",
       "  0.1887531876564026,\n",
       "  -0.4624069333076477,\n",
       "  0.042809564620256424,\n",
       "  -0.33468934893608093,\n",
       "  -0.1961463987827301,\n",
       "  -0.019646966829895973,\n",
       "  0.13418740034103394,\n",
       "  0.1526598483324051,\n",
       "  0.3831208050251007,\n",
       "  -0.07263857126235962,\n",
       "  0.030955949798226357,\n",
       "  -0.02819211035966873,\n",
       "  -0.11854786425828934,\n",
       "  -0.1984373927116394,\n",
       "  -0.25410041213035583,\n",
       "  -0.10539098083972931,\n",
       "  0.12409134209156036,\n",
       "  0.08357381075620651,\n",
       "  -0.19571514427661896,\n",
       "  0.19625496864318848,\n",
       "  0.18196646869182587,\n",
       "  0.3224068880081177,\n",
       "  0.20778512954711914,\n",
       "  0.23307889699935913,\n",
       "  0.2136409878730774,\n",
       "  0.309150367975235,\n",
       "  0.3247028887271881,\n",
       "  0.22155316174030304,\n",
       "  0.04499324783682823,\n",
       "  0.16452085971832275,\n",
       "  0.030625561252236366,\n",
       "  0.19001346826553345,\n",
       "  0.19127871096134186,\n",
       "  0.2723590135574341,\n",
       "  0.22710856795310974,\n",
       "  0.22784772515296936,\n",
       "  -0.14124195277690887,\n",
       "  0.3209955394268036],\n",
       " [-0.35791340470314026,\n",
       "  0.26698946952819824,\n",
       "  -0.09512882679700851,\n",
       "  -0.0047071343287825584,\n",
       "  0.1654966175556183,\n",
       "  0.14154905080795288,\n",
       "  -0.3338702917098999,\n",
       "  -0.12367624789476395,\n",
       "  0.18952010571956635,\n",
       "  -0.11053846031427383,\n",
       "  -0.2618904411792755,\n",
       "  -0.11754820495843887,\n",
       "  0.17547045648097992,\n",
       "  -0.03597959131002426,\n",
       "  0.4411907196044922,\n",
       "  0.014064689166843891,\n",
       "  0.01666095107793808,\n",
       "  0.1770620495080948,\n",
       "  0.3251374661922455,\n",
       "  0.003986166324466467,\n",
       "  0.2823714315891266,\n",
       "  -0.27818432450294495,\n",
       "  0.37367361783981323,\n",
       "  0.042778804898262024,\n",
       "  0.28152137994766235,\n",
       "  0.12793771922588348,\n",
       "  0.2897695004940033,\n",
       "  0.007131745107471943,\n",
       "  0.19353102147579193,\n",
       "  -0.2322344034910202,\n",
       "  -0.13017696142196655,\n",
       "  -0.04692450165748596,\n",
       "  0.21807120740413666,\n",
       "  0.21550297737121582,\n",
       "  -0.08499440550804138,\n",
       "  -0.3457574248313904,\n",
       "  -0.3608167767524719,\n",
       "  0.1919105052947998,\n",
       "  -0.18918801844120026,\n",
       "  0.13556288182735443,\n",
       "  0.24390022456645966,\n",
       "  0.23177573084831238,\n",
       "  0.31884250044822693,\n",
       "  -0.10725637525320053,\n",
       "  -0.2860272228717804,\n",
       "  -0.21971635520458221,\n",
       "  -0.29555580019950867,\n",
       "  0.3177889287471771,\n",
       "  -0.11162906140089035,\n",
       "  0.21935592591762543,\n",
       "  0.08060990273952484,\n",
       "  0.06216880679130554,\n",
       "  -0.11729424446821213,\n",
       "  0.21121692657470703,\n",
       "  -0.12110460549592972,\n",
       "  0.43867769837379456,\n",
       "  -0.2540470063686371,\n",
       "  -0.2394268661737442,\n",
       "  -0.0909498929977417,\n",
       "  0.20677007734775543,\n",
       "  -0.22091518342494965,\n",
       "  -0.3037639260292053,\n",
       "  0.30394017696380615,\n",
       "  0.153153657913208,\n",
       "  -0.35198014974594116,\n",
       "  -0.021952742710709572,\n",
       "  -0.23249979317188263,\n",
       "  -0.2581396996974945,\n",
       "  -0.2827187478542328,\n",
       "  0.36873558163642883,\n",
       "  0.3709706664085388,\n",
       "  -0.08481775969266891,\n",
       "  -0.2627639174461365,\n",
       "  0.4583421051502228,\n",
       "  -0.31300434470176697,\n",
       "  0.3708840310573578,\n",
       "  -0.10305589437484741,\n",
       "  0.3000417947769165,\n",
       "  -0.19622379541397095,\n",
       "  -0.2834450900554657,\n",
       "  0.11438899487257004,\n",
       "  -0.257830947637558,\n",
       "  -0.27232030034065247,\n",
       "  0.1493559032678604,\n",
       "  -0.26720958948135376,\n",
       "  -0.3887597322463989,\n",
       "  0.27934569120407104,\n",
       "  0.028316641226410866,\n",
       "  -0.20324905216693878,\n",
       "  0.237822026014328,\n",
       "  0.1887531876564026,\n",
       "  -0.4624069333076477,\n",
       "  0.042809564620256424,\n",
       "  -0.33468934893608093,\n",
       "  -0.1961463987827301,\n",
       "  -0.019646966829895973,\n",
       "  0.13418740034103394,\n",
       "  0.1526598483324051,\n",
       "  0.3831208050251007,\n",
       "  -0.07263857126235962,\n",
       "  0.030955949798226357,\n",
       "  -0.02819211035966873,\n",
       "  -0.11854786425828934,\n",
       "  -0.1984373927116394,\n",
       "  -0.25410041213035583,\n",
       "  -0.10539098083972931,\n",
       "  0.12409134209156036,\n",
       "  0.08357381075620651,\n",
       "  -0.19571514427661896,\n",
       "  0.19625496864318848,\n",
       "  0.18196646869182587,\n",
       "  0.3224068880081177,\n",
       "  0.20778512954711914,\n",
       "  0.23307889699935913,\n",
       "  0.2136409878730774,\n",
       "  0.309150367975235,\n",
       "  0.3247028887271881,\n",
       "  0.22155316174030304,\n",
       "  0.04499324783682823,\n",
       "  0.16452085971832275,\n",
       "  0.030625561252236366,\n",
       "  0.19001346826553345,\n",
       "  0.19127871096134186,\n",
       "  0.2723590135574341,\n",
       "  0.22710856795310974,\n",
       "  0.22784772515296936,\n",
       "  -0.14124195277690887,\n",
       "  0.3209955394268036]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['102844412722519367'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyein",
   "language": "python",
   "name": "hyein"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
