{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import string\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "import math\n",
    "import torch.utils.data as data\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy\n",
    "import torch.utils.data.sampler as sampler\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/video_statistic_features_one2.pickle','rb') as f2:\n",
    "    temp_data = pickle.load(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        with open('./data/video_raw_feature_sum_moving_average.pickle','rb') as f2:\n",
    "            temp_data = pickle.load(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'102844212428895431': array([0.        , 0.        , 0.        , ..., 0.13457849, 0.01878852,\n",
       "        0.01444573]),\n",
       " '102844212429092040': array([0.        , 0.        , 0.        , ..., 0.018911  , 0.01511   ,\n",
       "        0.00792267]),\n",
       " '102844212429419722': array([0.        , 0.        , 0.        , ..., 0.0243045 , 0.01570233,\n",
       "        0.00831002]),\n",
       " '102844212429550795': array([0.        , 0.        , 0.        , ..., 0.16514719, 0.01869862,\n",
       "        0.01338622]),\n",
       " '102844212429747404': array([0.        , 0.        , 0.        , ..., 0.0192366 , 0.01674523,\n",
       "        0.01439629]),\n",
       " '102844212429944013': array([0.        , 0.        , 0.        , ..., 0.01838888, 0.01582532,\n",
       "        0.01029176]),\n",
       " '102844212430075086': array([0.        , 0.        , 0.        , ..., 0.03960217, 0.03498364,\n",
       "        0.02480106]),\n",
       " '102844212430402768': array([0.        , 0.        , 0.        , ..., 0.02681384, 0.02354126,\n",
       "        0.02003448]),\n",
       " '102844212430599377': array([0.        , 0.        , 0.        , ..., 0.01150023, 0.00945952,\n",
       "        0.00760482]),\n",
       " '102844212430730450': array([0.        , 0.        , 0.        , ..., 0.01628664, 0.01433225,\n",
       "        0.01237785]),\n",
       " '102844212430927059': array([0.        , 0.        , 0.        , ..., 0.01723098, 0.01520393,\n",
       "        0.0132691 ]),\n",
       " '102844212431058132': array([0.        , 0.        , 0.        , ..., 0.23979383, 0.04637054,\n",
       "        0.01742119]),\n",
       " '102844212431516886': array([0.        , 0.        , 0.        , ..., 0.05634894, 0.052065  ,\n",
       "        0.02340535]),\n",
       " '102844212431779031': array([0.        , 0.        , 0.        , ..., 0.01913076, 0.01686944,\n",
       "        0.01470076]),\n",
       " '102844212431975640': array([0.        , 0.        , 0.        , ..., 0.03143695, 0.02374249,\n",
       "        0.02056417]),\n",
       " '102844224145685626': array([0.        , 0.        , 0.        , ..., 0.02432162, 0.02104554,\n",
       "        0.01776946]),\n",
       " '102844224146472059': array([0.        , 0.        , 0.        , ..., 0.02535159, 0.01927786,\n",
       "        0.01627931]),\n",
       " '102844224146930812': array([0.        , 0.        , 0.        , ..., 0.01740827, 0.01740827,\n",
       "        0.00853928]),\n",
       " '102844224147717245': array([0.        , 0.        , 0.        , ..., 0.41883875, 0.01879222,\n",
       "        0.0138276 ]),\n",
       " '102844224148503678': array([0.        , 0.        , 0.        , ..., 0.13396404, 0.13114735,\n",
       "        0.01368417]),\n",
       " '102844224148896895': array([0.        , 0.        , 0.        , ..., 0.01724178, 0.01477413,\n",
       "        0.01256364]),\n",
       " '102844235746868664': array([0.        , 0.        , 0.        , ..., 0.08926618, 0.01791445,\n",
       "        0.01791445]),\n",
       " '102844235747261881': array([0.        , 0.        , 0.        , ..., 0.02496187, 0.02123556,\n",
       "        0.02123556]),\n",
       " '102844235747982779': array([0.        , 0.        , 0.        , ..., 0.02179985, 0.01994575,\n",
       "        0.00862188]),\n",
       " '102844235748310460': array([0.        , 0.        , 0.        , ..., 0.02071021, 0.01829145,\n",
       "        0.01829145]),\n",
       " '102844235748703677': array([0.        , 0.        , 0.        , ..., 0.02303593, 0.01681457,\n",
       "        0.014659  ]),\n",
       " '102844235749031358': array([0.        , 0.        , 0.        , ..., 0.01459129, 0.0107818 ,\n",
       "        0.0107818 ]),\n",
       " '102844235749424575': array([0.        , 0.        , 0.        , ..., 0.02268079, 0.01814569,\n",
       "        0.00769737]),\n",
       " '102844235750997440': array([0.        , 0.        , 0.        , ..., 0.06209203, 0.02121884,\n",
       "        0.01333804]),\n",
       " '102844235751390657': array([0.        , 0.        , 0.        , ..., 0.02137032, 0.02137032,\n",
       "        0.01148586]),\n",
       " '102844235751783874': array([0.        , 0.        , 0.        , ..., 0.02342902, 0.01570033,\n",
       "        0.01196945]),\n",
       " '102844235752963525': array([0.        , 0.        , 0.        , ..., 0.0153995 , 0.01024508,\n",
       "        0.01024508]),\n",
       " '102844235753356742': array([0.        , 0.        , 0.        , ..., 0.02438645, 0.01960045,\n",
       "        0.01666374]),\n",
       " '102844235753749959': array([0.        , 0.        , 0.        , ..., 0.04265444, 0.0144854 ,\n",
       "        0.0144854 ]),\n",
       " '102844283020453971': array([0.        , 0.        , 0.        , ..., 0.07031957, 0.02034949,\n",
       "        0.01716487]),\n",
       " '102844283023206486': array([0.        , 0.        , 0.        , ..., 0.0181867 , 0.01605757,\n",
       "        0.01368875]),\n",
       " '102844283023599703': array([0.        , 0.        , 0.        , ..., 0.01932615, 0.01682459,\n",
       "        0.01456933]),\n",
       " '102844283025696858': array([0.        , 0.        , 0.        , ..., 0.01880328, 0.01426711,\n",
       "        0.01241241]),\n",
       " '102844283027531868': array([0.        , 0.        , 0.        , ..., 0.03013803, 0.02425958,\n",
       "        0.02425958]),\n",
       " '102844283027925085': array([0.        , 0.        , 0.        , ..., 0.30892823, 0.01735075,\n",
       "        0.01735075]),\n",
       " '102844294666422466': array([0.        , 0.        , 0.        , ..., 0.0181081 , 0.01594711,\n",
       "        0.00882982]),\n",
       " '102844294666881219': array([0.        , 0.        , 0.        , ..., 0.19334269, 0.01674694,\n",
       "        0.01320963]),\n",
       " '102844294667405508': array([0.        , 0.        , 0.        , ..., 0.0184283 , 0.01604588,\n",
       "        0.01379685]),\n",
       " '102844294669568199': array([0.        , 0.        , 0.        , ..., 0.13608847, 0.13347713,\n",
       "        0.13086579]),\n",
       " '102844294670026952': array([0.        , 0.        , 0.        , ..., 0.02470917, 0.02143658,\n",
       "        0.0179298 ]),\n",
       " '102844294670551241': array([0.        , 0.        , 0.        , ..., 0.01426998, 0.01426998,\n",
       "        0.01147194]),\n",
       " '102844294670878922': array([0.        , 0.        , 0.        , ..., 0.01660177, 0.01446321,\n",
       "        0.01113222]),\n",
       " '102844294671796427': array([0.        , 0.        , 0.        , ..., 0.03805951, 0.01998032,\n",
       "        0.0145145 ]),\n",
       " '102844294674286796': array([0.        , 0.        , 0.        , ..., 0.01665346, 0.01392339,\n",
       "        0.01119331]),\n",
       " '102844294674876621': array([0.        , 0.        , 0.        , ..., 0.02546208, 0.02237018,\n",
       "        0.01884605]),\n",
       " '102844341902586509': array([0.        , 0.        , 0.        , ..., 0.0183571 , 0.01612422,\n",
       "        0.00692194]),\n",
       " '102844341904683662': array([0.        , 0.        , 0.        , ..., 0.01637971, 0.01423918,\n",
       "        0.01237785]),\n",
       " '102844341905011343': array([0.        , 0.        , 0.        , ..., 0.02093485, 0.01464793,\n",
       "        0.01223699]),\n",
       " '102844341905404560': array([0.        , 0.        , 0.        , ..., 0.01639395, 0.01365961,\n",
       "        0.01101738]),\n",
       " '102844341906649746': array([0.        , 0.        , 0.        , ..., 0.01999799, 0.01228834,\n",
       "        0.00989699]),\n",
       " '102844341906977427': array([0.        , 0.        , 0.        , ..., 0.20299596, 0.0571497 ,\n",
       "        0.01561131]),\n",
       " '102844341907370644': array([0.        , 0.        , 0.        , ..., 0.03881904, 0.02142222,\n",
       "        0.01956751]),\n",
       " '102844341908026005': array([0.        , 0.        , 0.        , ..., 0.02031858, 0.01844156,\n",
       "        0.01665112]),\n",
       " '102844341909598870': array([0.        , 0.        , 0.        , ..., 0.01530612, 0.01273775,\n",
       "        0.01020408]),\n",
       " '102844341912220311': array([0.        , 0.        , 0.        , ..., 0.20136005, 0.19952609,\n",
       "        0.01388867]),\n",
       " '102844341912679064': array([0.        , 0.        , 0.        , ..., 0.02676767, 0.01960847,\n",
       "        0.01960847]),\n",
       " '102844401151219358': array([0.        , 0.        , 0.        , ..., 0.03323036, 0.01685177,\n",
       "        0.01685177]),\n",
       " '102844401152071328': array([0.        , 0.        , 0.        , ..., 0.02719594, 0.02053089,\n",
       "        0.02053089]),\n",
       " '102844401152267937': array([0.        , 0.        , 0.        , ..., 0.02057544, 0.01818217,\n",
       "        0.01398067]),\n",
       " '102844401152857762': array([0.        , 0.        , 0.        , ..., 0.02658184, 0.02218868,\n",
       "        0.01779552]),\n",
       " '102844401153447587': array([0.        , 0.        , 0.        , ..., 0.02225877, 0.01944901,\n",
       "        0.01944901]),\n",
       " '102844401153578660': array([0.        , 0.        , 0.        , ..., 0.01609379, 0.01423306,\n",
       "        0.01227927]),\n",
       " '102844401153971877': array([0.        , 0.        , 0.        , ..., 0.04177578, 0.03731353,\n",
       "        0.03166658]),\n",
       " '102844401154168486': array([0.        , 0.        , 0.        , ..., 0.01630318, 0.01419792,\n",
       "        0.01419792]),\n",
       " '102844401154430631': array([0.        , 0.        , 0.        , ..., 0.02170669, 0.01913648,\n",
       "        0.01191597]),\n",
       " '102844401155937960': array([0.        , 0.        , 0.        , ..., 0.02575471, 0.01723088,\n",
       "        0.01481212]),\n",
       " '102844401156069033': array([0.        , 0.        , 0.        , ..., 0.02210307, 0.02210307,\n",
       "        0.01863989]),\n",
       " '102844412704496937': array([0.        , 0.        , 0.        , ..., 0.04161781, 0.01979885,\n",
       "        0.01575034]),\n",
       " '102844412704890154': array([0.        , 0.        , 0.        , ..., 0.02212569, 0.01954051,\n",
       "        0.01549664]),\n",
       " '102844412705217835': array([0.        , 0.        , 0.        , ..., 0.19395536, 0.0238614 ,\n",
       "        0.02007447]),\n",
       " '102844412705545516': array([0.        , 0.        , 0.        , ..., 0.49827417, 0.48830165,\n",
       "        0.34544451]),\n",
       " '102844412706659630': array([0.        , 0.        , 0.        , ..., 0.01716762, 0.01423062,\n",
       "        0.01423062]),\n",
       " '102844412706987311': array([0.        , 0.        , 0.        , ..., 0.02327444, 0.02008564,\n",
       "        0.01689684]),\n",
       " '102844412707380528': array([0.        , 0.        , 0.        , ..., 0.0209868 , 0.01827413,\n",
       "        0.01576539]),\n",
       " '102844412708953395': array([0.        , 0.        , 0.        , ..., 0.0196396 , 0.0196396 ,\n",
       "        0.01704601]),\n",
       " '102844412709346612': array([0.        , 0.        , 0.        , ..., 0.03973109, 0.0145567 ,\n",
       "        0.01259083]),\n",
       " '102844412709674293': array([0.        , 0.        , 0.        , ..., 0.02761712, 0.02135467,\n",
       "        0.01051346]),\n",
       " '102844412710001974': array([0.        , 0.        , 0.        , ..., 0.03107455, 0.02384094,\n",
       "        0.02384094]),\n",
       " '102844412711116088': array([0.        , 0.        , 0.        , ..., 0.01672312, 0.01398163,\n",
       "        0.01124013]),\n",
       " '102844412711443769': array([0.        , 0.        , 0.        , ..., 0.18220586, 0.18034755,\n",
       "        0.01236546]),\n",
       " '102844412711836986': array([0.        , 0.        , 0.        , ..., 0.01855841, 0.0161416 ,\n",
       "        0.01400219]),\n",
       " '102844412712164667': array([0.        , 0.        , 0.        , ..., 0.01994909, 0.01810447,\n",
       "        0.01232322]),\n",
       " '102844412716227901': array([0.        , 0.        , 0.        , ..., 0.02038679, 0.01776586,\n",
       "        0.01776586]),\n",
       " '102844412716686654': array([0.        , 0.        , 0.        , ..., 0.0174397 , 0.0174397 ,\n",
       "        0.01478521]),\n",
       " '102844412717014335': array([0.        , 0.        , 0.        , ..., 0.02653442, 0.02262676,\n",
       "        0.00800839]),\n",
       " '102844412717407552': array([0.        , 0.        , 0.        , ..., 0.02797619, 0.02555804,\n",
       "        0.02555804]),\n",
       " '102844412721339716': array([0.        , 0.        , 0.        , ..., 0.04495882, 0.02234564,\n",
       "        0.02234564]),\n",
       " '102844412722519367': array([0.        , 0.        , 0.        , ..., 0.22333474, 0.22102516,\n",
       "        0.01799552]),\n",
       " '102844412722847048': array([0.        , 0.        , 0.        , ..., 0.02025477, 0.01744532,\n",
       "        0.01240984]),\n",
       " '102844412723174729': array([0.        , 0.        , 0.        , ..., 0.39054959, 0.38734984,\n",
       "        0.10163556]),\n",
       " '102844412723567946': array([0.        , 0.        , 0.        , ..., 0.01935784, 0.01647278,\n",
       "        0.01247092]),\n",
       " '102904869420860038': array([0.        , 0.        , 0.        , ..., 0.18329797, 0.17501173,\n",
       "        0.02839616]),\n",
       " '102910307641576395': array([0.        , 0.        , 0.        , ..., 0.0373004 , 0.0279862 ,\n",
       "        0.01787381]),\n",
       " '102979081290790284': array([0.        , 0.        , 0.        , ..., 0.0212377 , 0.01875748,\n",
       "        0.01602924])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    if epoch<20:\n",
    "        lr = 0.01\n",
    "    else:\n",
    "        lr = 0.001\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model\n",
    "output, feature를 한번에 뽑음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size=1\n",
    "hidden_size=128\n",
    "num_layers=3\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._clf1 = nn.LSTM(input_size, hidden_size,3,batch_first=True)\n",
    "        self._lin = nn.Sequential(nn.Linear(hidden_size, hidden_size),\n",
    "                                 nn.Linear(hidden_size,2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=x.cuda()\n",
    "        hidden = Variable(torch.zeros(num_layers,x.size(0),hidden_size)).cuda() # (num_layers * num_directions, batch, hidden_size)\n",
    "        cell = Variable(torch.zeros(num_layers,x.size(0),hidden_size)).cuda() # (num_layers * num_directions, batch, hidden_size)        out,hidden = self._clf1(x,h0)\n",
    "        out,hidden = self._clf1(x,(hidden,cell))#batch*7*3\n",
    "        ft = out[:,-1,:]\n",
    "        out = self._lin(out[:,-1,:])\n",
    "        return out,ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 준비\n",
    "1. d_type : train, val, test 를 받음\n",
    "2. 7초 window size shift 진행 --> *** 이부분 수정해야함.***\n",
    "3. 하이라이트 구간 25%추출 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "class data_ds(data.Dataset):\n",
    "    def __init__(self,d_type):\n",
    "        self.gt_range =  1-0.25\n",
    "        self.d_type=d_type\n",
    "        \n",
    "        #label load\n",
    "        with open('label/label.pickle','rb') as f1:  \n",
    "            self.gt=pickle.load(f1)\n",
    "        \n",
    "        #원본 데이터로드\n",
    "        with open('./data/video_raw_feature_sum_moving_average.pickle','rb') as f2:\n",
    "            temp_data = pickle.load(f2)\n",
    "            if temp_data['102844412722519367'][0] is not list:\n",
    "                for i in temp_data:\n",
    "                    temp_data[i] = np.array(temp_data[i])[:,np.newaxis]\n",
    "            self.data= temp_data\n",
    "        if d_type=='train':\n",
    "            self.sample = ['102844412722519367','102844212429550795','102844401151219358','102844401154430631','102844412717014335','102844401153971877','102844224148503678','102844412722847048','102844401152857762','102844412707380528','102844212431516886','102844283027925085','102844412716227901','102844412710001974','102844294670878922','102844294670551241','102844283023599703','102844412704496937','102844235751783874','102844401152071328','102844412709674293','102844401153447587','102844224148896895','102844235746868664','102979081290790284','102844283027531868','102844212431975640','102844401155937960','102844212429092040','102844341906649746','102844412706987311','102844412721339716','102844212430402768','102844341905011343','102844235753356742','102844235750997440','102844412709346612','102844412705217835','102844235752963525','102844412712164667','102844412705545516','102844341912220311','102844341907370644','102844235749424575','102844212429419722','102844294669568199','102844212431779031','102844294666422466','102844224146472059','102844212428895431','102844212429747404','102844235748703677','102844224146930812','102844212430730450','102844294674876621','102844341909598870','102844283020453971','102844294670026952','102844412723174729','102844341904683662','102844283025696858','102844235747261881','102844401154168486','102844235748310460','102844412711836986','102844412723567946','102844235749031358','102844294674286796','102844294666881219','102844412716686654']\n",
    "        if d_type=='val':\n",
    "            self.sample = ['102844294671796427','102844224145685626','102844412717407552','102844235751390657','102844401156069033','102904869420860038','102910307641576395','102844341905404560','102844341906977427','102844212430075086','102844412711116088','102844401153578660','102844294667405508','102844412706659630']\n",
    "        if d_type=='test':\n",
    "            self.sample = ['102844212431058132','102844341902586509','102844401152267937','102844212430927059','102844412708953395','102844212429944013','102844341912679064','102844235753749959','102844341908026005','102844283023206486','102844224147717245','102844412704890154','102844212430599377','102844412711443769','102844235747982779']\n",
    "        if d_type == 'total':\n",
    "            self.sample = ['102844412722519367','102844212429550795','102844401151219358','102844401154430631','102844412717014335','102844401153971877','102844224148503678','102844412722847048','102844401152857762','102844412707380528','102844212431516886','102844283027925085','102844412716227901','102844412710001974','102844294670878922','102844294670551241','102844283023599703','102844412704496937','102844235751783874','102844401152071328','102844412709674293','102844401153447587','102844224148896895','102844235746868664','102979081290790284','102844283027531868','102844212431975640','102844401155937960','102844212429092040','102844341906649746','102844412706987311','102844412721339716','102844212430402768','102844341905011343','102844235753356742','102844235750997440','102844412709346612','102844412705217835','102844235752963525','102844412712164667','102844412705545516','102844341912220311','102844341907370644','102844235749424575','102844212429419722','102844294669568199','102844212431779031','102844294666422466','102844224146472059','102844212428895431','102844212429747404','102844235748703677','102844224146930812','102844212430730450','102844294674876621','102844341909598870','102844283020453971','102844294670026952','102844412723174729','102844341904683662','102844283025696858','102844235747261881','102844401154168486','102844235748310460','102844412711836986','102844412723567946','102844235749031358','102844294674286796','102844294666881219','102844412716686654','102844294671796427','102844224145685626','102844412717407552','102844235751390657','102844401156069033','102904869420860038','102910307641576395','102844341905404560','102844341906977427','102844212430075086','102844412711116088','102844401153578660','102844294667405508','102844412706659630','102844212431058132','102844341902586509','102844401152267937','102844212430927059','102844412708953395','102844212429944013','102844341912679064','102844235753749959','102844341908026005','102844283023206486','102844224147717245','102844412704890154','102844212430599377','102844412711443769','102844235747982779']\n",
    "\n",
    "        \n",
    "        #sampling 대상이 될 데이터 저장 (25% 구간때문에 진행)\n",
    "        self.WeightedSampling=[]\n",
    "        for i in self.sample:\n",
    "            self.WeightedSampling.extend(copy.copy(self.gt[str(i)]))\n",
    "        \n",
    "        sampling = np.array(self.WeightedSampling)\n",
    "        neg_idx = np.where(sampling == 0)[0] #general\n",
    "        pos_idx = np.where(sampling == 1)[0] #highlight\n",
    "        sampling = sampling.astype(np.float32)\n",
    "\n",
    "        #구간 구분하면서, 25% 만 추출 및 weightedsampling 에 저장\n",
    "        begin_pos = 0 \n",
    "        hl_frames = []\n",
    "        for it, cur_pos in enumerate(pos_idx):\n",
    "            if it+1 < len(pos_idx): \n",
    "                if((pos_idx[it+1] - cur_pos) > 1):#cur_pos와 cur_pos+1 간격이 1보다 크면, 즉 다른 구간이면\n",
    "                    begin = int((it+1 - begin_pos) * self.gt_range) + begin_pos\n",
    "                    hl_frames.extend( pos_idx[begin: it] ) #한구간의 하이라이트 25%만 사용하겠다.\n",
    "                    begin_pos = it+1\n",
    "\n",
    "        #sampling 비율 맞춰줌\n",
    "        sampling.fill(0)\n",
    "        sampling[neg_idx] = len(sampling) / float(len(neg_idx))\n",
    "       # self.WeightedSampling[pos_idx] = len(self.WeightedSampling) / float(len(pos_idx))\n",
    "        sampling[hl_frames] = len(sampling) / float(len(hl_frames))\n",
    "        self.WeightedSampling = sampling\n",
    "        \n",
    "        #sum : 각 경기당 데이터수 누적으로 저장 -->지금 어디 경기인지, 몇번째 프레임인지 확인할때 사용\n",
    "        self.sum=np.insert(np.cumsum([len(self.gt[str(i)]) for i in self.sample]),0,0)\n",
    "        print(\"data load fin\")\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.WeightedSampling)\n",
    "    def __getitem__(self,index):\n",
    "            global input_size\n",
    "            #누적 데이터수 - index 로 프레임과 경기를 찾음\n",
    "            vid=np.histogram(index,self.sum)\n",
    "            vid = np.where(vid[0]>0)[0][0]\n",
    "            vframe=index-self.sum[vid]\n",
    "            game_id=str(self.sample[vid])\n",
    "\n",
    "            window=[]#batch*7(window size)*3(highlight result)\n",
    "            for idx in range(23): #7 : window size\n",
    "                s_window=[]\n",
    "                if vframe+idx<len(self.data[game_id]):\n",
    "                    s_window=(self.data[game_id][vframe+idx])#vframe의 image\n",
    "                else:\n",
    "                    #s_window=[0,0,0]#padding value\n",
    "                    s_window=[0]*input_size # input_size\n",
    "                window.append(s_window)\n",
    "                \n",
    "\n",
    "\n",
    "            #label 값 받기\n",
    "            label=self.gt[str(game_id)][vframe]\n",
    "            \n",
    "            \n",
    "            return np.array(window),label,str(game_id)\n",
    "        \n",
    "            \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 경기별 score 평균\n",
    "def fmeasure2(frames,label):\n",
    "    average = [0,0,0,0,0]\n",
    "    for key in frames.keys():\n",
    "        TP = len(np.where((np.array(frames[key])==1)&(label[key]==1)==True)[0])\n",
    "        FP = len(np.where((np.array(frames[key])==1)&(label[key]==0)==True)[0])\n",
    "        TN = len(np.where((np.array(frames[key])==0)&(label[key]==0)==True)[0])\n",
    "        FN = len(np.where((np.array(frames[key])==0)&(label[key]==1)==True)[0])\n",
    "        precision = TP/(TP+FP)\n",
    "        recall = TP/(TP+FN)\n",
    "        accuracy = (TP+TN)/(TP+FN+FP+TN)\n",
    "        print(precision,recall,accuracy)\n",
    "        if precision==0 and recall == 0:\n",
    "            print('error')\n",
    "        else:\n",
    "            f1 = (2*precision*recall / (precision + recall))\n",
    "            print(key)\n",
    "            print('precision : {}, recall : {}, f1 : {}, accuracy : {}'.format(precision,recall,f1,accuracy))\n",
    "            average[0]+= precision\n",
    "            average[1]+= recall\n",
    "            average[2]+= f1\n",
    "            average[3]+= accuracy\n",
    "            average[4]+=1\n",
    "    print('==precision : {}, recall : {}, f1 : {}, accuracy : {}'.format(average[0]/average[4],average[1]/average[4],average[2]/average[4],average[3]/average[4]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 전체 데이터에 대해서 score\n",
    "def fmeasure(output, target):\n",
    "    _, pred = output.topk(1, 1, True, True)\n",
    "    pred = pred.view(-1,1)\n",
    "    target = target.view(-1,1)\n",
    "\n",
    "    #overlap = ((pred== 1) + (target == 1)).gt(1)\n",
    "    #overlap = overlap.view(-1,1)\n",
    "    TP = len(np.where((pred==1)&(target==1)==True)[0]) # True positive\n",
    "    FP = len(np.where((pred==1)&(target==0)==True)[0]) # Condition positive = TP + FN\n",
    "    TN = len(np.where((pred==0)&(target==0)==True)[0])\n",
    "    FN = len(np.where((pred==0)&(target==1)==True)[0])\n",
    "\n",
    "    \n",
    "    #overlap_len = overlap.data.long().sum()\n",
    "    pred_len = pred.data.long().sum()\n",
    "    gt_len   =  target.data.long().sum()\n",
    "\n",
    "    return TP,FP,TN,FN,pred_len, gt_len,pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset load\n",
    "train : undersampling 진행,  validation : 그냥 평등하게 sampling (sequential sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SampleSequentialSampler(sampler.Sampler):\n",
    "    \"\"\"Samples elements sequentially, always in the same order.\n",
    "    Arguments:\n",
    "        data_source (Dataset): dataset to sample from\n",
    "        offset (int): offset between the samples\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_source, offset=10):\n",
    "        self.num_samples = len(data_source) \n",
    "        self.offset = offset\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(np.arange(0, self.num_samples, self.offset ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(np.arange(0, self.num_samples, self.offset ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load fin\n",
      "data load fin\n"
     ]
    }
   ],
   "source": [
    "# train, validation dataset 준비\n",
    "train=data_ds('train')\n",
    "val=data_ds('val')\n",
    "\n",
    "# dataloader with sampling\n",
    "sampler1 = torch.utils.data.sampler.WeightedRandomSampler(weights=train.WeightedSampling.tolist(), num_samples=40000)\n",
    "train_loader=torch.utils.data.DataLoader(train,batch_size=32,sampler=sampler1)\n",
    "sampler2 =  SampleSequentialSampler(val, 30)\n",
    "val_loader=torch.utils.data.DataLoader(val,batch_size=32,sampler= sampler2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]), 0.0, '102844412722519367')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model load 및 실험\n",
    "\n",
    "- weight_dir/train_result 에 모든 결과가 저장됨.\n",
    "- fmeasure (전체 데이터에 대한 score 로 weight를 저장..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_dir='./video_raw_feature_sum_moving_average2/'\n",
    "\n",
    "###### model load #####\n",
    "model=LSTM().cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "epoch 0 train_loss : 0.6935967803001404 , val_loss : 0.6660415530204773, val_acc : 0.8261363636363637\n",
      "1\n",
      "epoch 1 train_loss : 0.6937519907951355 , val_loss : 0.710422933101654, val_acc : 0.17386363636363636\n",
      "2\n",
      "epoch 2 train_loss : 0.6937795877456665 , val_loss : 0.6839006543159485, val_acc : 0.8261363636363637\n",
      "3\n",
      "epoch 3 train_loss : 0.6937023401260376 , val_loss : 0.7093675136566162, val_acc : 0.17386363636363636\n",
      "4\n",
      "epoch 4 train_loss : 0.6936056613922119 , val_loss : 0.6907477974891663, val_acc : 0.8261363636363637\n",
      "5\n",
      "epoch 5 train_loss : 0.6935888528823853 , val_loss : 0.7176907062530518, val_acc : 0.17386363636363636\n",
      "6\n",
      "epoch 6 train_loss : 0.6937092542648315 , val_loss : 0.6690042018890381, val_acc : 0.8261363636363637\n",
      "7\n",
      "epoch 7 train_loss : 0.6935508847236633 , val_loss : 0.6514782309532166, val_acc : 0.8261363636363637\n",
      "8\n",
      "epoch 8 train_loss : 0.6934142112731934 , val_loss : 0.697493314743042, val_acc : 0.17386363636363636\n",
      "9\n",
      "epoch 9 train_loss : 0.6937239766120911 , val_loss : 0.705227255821228, val_acc : 0.17386363636363636\n",
      "10\n",
      "epoch 10 train_loss : 0.6935869455337524 , val_loss : 0.7132568955421448, val_acc : 0.17386363636363636\n",
      "11\n",
      "epoch 11 train_loss : 0.6937045454978943 , val_loss : 0.6759153008460999, val_acc : 0.8261363636363637\n",
      "12\n",
      "epoch 12 train_loss : 0.6936050057411194 , val_loss : 0.6863037347793579, val_acc : 0.8261363636363637\n",
      "13\n",
      "epoch 13 train_loss : 0.6934682726860046 , val_loss : 0.6980555653572083, val_acc : 0.17386363636363636\n",
      "14\n",
      "epoch 14 train_loss : 0.6935228705406189 , val_loss : 0.7016052603721619, val_acc : 0.17386363636363636\n",
      "15\n",
      "epoch 15 train_loss : 0.6934338808059692 , val_loss : 0.6996014714241028, val_acc : 0.17386363636363636\n",
      "16\n",
      "epoch 16 train_loss : 0.6935719847679138 , val_loss : 0.6695083379745483, val_acc : 0.8261363636363637\n",
      "17\n",
      "epoch 17 train_loss : 0.6935487389564514 , val_loss : 0.6993085741996765, val_acc : 0.17386363636363636\n",
      "18\n",
      "epoch 18 train_loss : 0.6935901045799255 , val_loss : 0.6735007166862488, val_acc : 0.8261363636363637\n",
      "19\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5ec370b9f833>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgame_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yes/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yes/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yes/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yes/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-6316c6f96afe>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mglobal\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m#누적 데이터수 - index 로 프레임과 경기를 찾음\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mvid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0mvid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mvframe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mhistogram\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/yes/lib/python3.6/site-packages/numpy/lib/histograms.py\u001b[0m in \u001b[0;36mhistogram\u001b[0;34m(a, bins, range, normed, weights, density)\u001b[0m\n\u001b[1;32m    791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m     \"\"\"\n\u001b[0;32m--> 793\u001b[0;31m     \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ravel_and_check_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0mbin_edges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniform_bins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_bin_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not os.path.exists(weight_dir):\n",
    "    os.makedirs(weight_dir)\n",
    "optimizer = torch.optim.SGD(model.parameters(), 0.01,momentum=0.9,weight_decay=1e-4)\n",
    "# scheduler = lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "with open(weight_dir+'train_result','a') as f:\n",
    "\n",
    "\n",
    "    best_losses=1000000\n",
    "    for epoch in range(200):\n",
    "        lr = adjust_learning_rate(optimizer, epoch)\n",
    "        train_loss=0\n",
    "        print(epoch)\n",
    "        model.train()\n",
    "        for i, (data,labels,game_id) in enumerate(train_loader):\n",
    "            inputs = Variable(data).float().cuda()\n",
    "            labels = Variable(labels).cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output,_=model(inputs)\n",
    "\n",
    "            loss=criterion(output,labels.long())\n",
    "            train_loss+=loss\n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.) #rnn 계열 gradient 장치\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        #validation\n",
    "        model.eval()\n",
    "        val_loss=0\n",
    "        acc=0\n",
    "        gt_sum=0\n",
    "        val_result ={}\n",
    "        with open(weight_dir+'train_result','a') as f:\n",
    "            with torch.no_grad():\n",
    "                for it, (data,labels,game_id) in enumerate(val_loader):\n",
    "                    inputs = Variable(data).float().cuda()\n",
    "                    labels = Variable(labels).cuda()\n",
    "                    output,_=model(inputs)\n",
    "                    loss=criterion(output,labels.long())\n",
    "                    val_loss+=loss\n",
    "                    TP,FP,TN,FN,pred_len, gt_len,pred=fmeasure(output.cpu(),labels.cpu())\n",
    "                    acc=acc+TP+TN\n",
    "                    gt_sum+=len(output)\n",
    "                val_acc=acc/gt_sum\n",
    "                print(\"epoch {} train_loss : {} , val_loss : {}, val_acc : {}\".format(epoch,train_loss/len(train_loader),val_loss/len(val_loader),val_acc))\n",
    "                f.write(\"epoch {} train_loss : {} , val_loss : {}, val_acc : {}\\n\".format(epoch,train_loss/len(train_loader),val_loss/len(val_loader),val_acc))\n",
    "                if best_losses>val_loss:\n",
    "                    best_losses=val_loss\n",
    "                    torch.save(model.state_dict(),'{}'.format(weight_dir+str(epoch)+\"train_best\"))#epoch 변화 확인위해\n",
    "                    torch.save(model.state_dict(),'{}'.format(weight_dir+\"train_best\"))#train best 덮어써짐\n",
    "\n",
    "                    f.write(\"epoch {} saved\\n\".format(epoch))\n",
    "                else:\n",
    "                    torch.save(model.state_dict(),'{}'.format(weight_dir+str(epoch)+\"train\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "- test set 로드\n",
    "- best weight load\n",
    "- weight_dir/train_result 에 결과 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load fin\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "2 0 27 3 tensor(2) tensor(5)\n",
      "8 0 23 1 tensor(8) tensor(9)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "6 3 20 3 tensor(9) tensor(9)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 2 14 16 tensor(2) tensor(16)\n",
      "7 0 15 10 tensor(7) tensor(17)\n",
      "0 1 31 0 tensor(1) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "11 5 15 1 tensor(16) tensor(12)\n",
      "0 7 25 0 tensor(7) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 15 17 0 tensor(15) tensor(0)\n",
      "0 11 21 0 tensor(11) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 7 18 7 tensor(7) tensor(7)\n",
      "13 0 14 5 tensor(13) tensor(18)\n",
      "7 3 21 1 tensor(10) tensor(8)\n",
      "0 12 20 0 tensor(12) tensor(0)\n",
      "11 0 19 2 tensor(11) tensor(13)\n",
      "6 12 14 0 tensor(18) tensor(6)\n",
      "5 3 24 0 tensor(8) tensor(5)\n",
      "0 3 29 0 tensor(3) tensor(0)\n",
      "0 5 27 0 tensor(5) tensor(0)\n",
      "13 4 15 0 tensor(17) tensor(13)\n",
      "3 6 23 0 tensor(9) tensor(3)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "6 2 23 1 tensor(8) tensor(7)\n",
      "0 5 27 0 tensor(5) tensor(0)\n",
      "0 3 29 0 tensor(3) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 6 26 0 tensor(6) tensor(0)\n",
      "0 3 29 0 tensor(3) tensor(0)\n",
      "0 1 31 0 tensor(1) tensor(0)\n",
      "0 5 27 0 tensor(5) tensor(0)\n",
      "0 5 27 0 tensor(5) tensor(0)\n",
      "13 0 15 4 tensor(13) tensor(17)\n",
      "3 20 1 8 tensor(23) tensor(11)\n",
      "0 0 30 2 tensor(0) tensor(2)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "11 1 18 2 tensor(12) tensor(13)\n",
      "22 0 9 1 tensor(22) tensor(23)\n",
      "10 14 8 0 tensor(24) tensor(10)\n",
      "0 8 24 0 tensor(8) tensor(0)\n",
      "0 4 28 0 tensor(4) tensor(0)\n",
      "0 30 2 0 tensor(30) tensor(0)\n",
      "28 0 2 2 tensor(28) tensor(30)\n",
      "0 15 17 0 tensor(15) tensor(0)\n",
      "8 2 19 3 tensor(10) tensor(11)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 5 27 0 tensor(5) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "2 0 20 10 tensor(2) tensor(12)\n",
      "5 0 23 4 tensor(5) tensor(9)\n",
      "10 0 13 9 tensor(10) tensor(19)\n",
      "9 3 20 0 tensor(12) tensor(9)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "9 0 10 13 tensor(9) tensor(22)\n",
      "16 0 15 1 tensor(16) tensor(17)\n",
      "22 0 4 6 tensor(22) tensor(28)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "8 0 15 9 tensor(8) tensor(17)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 1 31 0 tensor(1) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 2 30 0 tensor(2) tensor(0)\n",
      "0 1 31 0 tensor(1) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 1 31 0 tensor(1) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "17 5 9 1 tensor(22) tensor(18)\n",
      "7 5 20 0 tensor(12) tensor(7)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 6 26 0 tensor(6) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 2 30 0 tensor(2) tensor(0)\n",
      "0 12 20 0 tensor(12) tensor(0)\n",
      "0 5 27 0 tensor(5) tensor(0)\n",
      "0 9 23 0 tensor(9) tensor(0)\n",
      "8 17 7 0 tensor(25) tensor(8)\n",
      "9 0 22 1 tensor(9) tensor(10)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 6 26 0 tensor(6) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 18 14 0 tensor(18) tensor(0)\n",
      "0 5 27 0 tensor(5) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 10 22 0 tensor(10) tensor(0)\n",
      "10 3 19 0 tensor(13) tensor(10)\n",
      "18 0 14 0 tensor(18) tensor(18)\n",
      "5 3 24 0 tensor(8) tensor(5)\n",
      "0 2 30 0 tensor(2) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "16 6 10 0 tensor(22) tensor(16)\n",
      "22 0 7 3 tensor(22) tensor(25)\n",
      "0 10 22 0 tensor(10) tensor(0)\n",
      "0 1 31 0 tensor(1) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "11 1 20 0 tensor(12) tensor(11)\n",
      "15 0 14 3 tensor(15) tensor(18)\n",
      "8 4 10 10 tensor(12) tensor(18)\n",
      "0 7 25 0 tensor(7) tensor(0)\n",
      "0 20 12 0 tensor(20) tensor(0)\n",
      "0 4 28 0 tensor(4) tensor(0)\n",
      "0 11 21 0 tensor(11) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "14 15 3 0 tensor(29) tensor(14)\n",
      "24 0 0 8 tensor(24) tensor(32)\n",
      "24 0 6 2 tensor(24) tensor(26)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "7 1 24 0 tensor(8) tensor(7)\n",
      "14 0 17 1 tensor(14) tensor(15)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "1 0 27 4 tensor(1) tensor(5)\n",
      "4 3 25 0 tensor(7) tensor(4)\n",
      "17 1 14 0 tensor(18) tensor(17)\n",
      "3 0 27 2 tensor(3) tensor(5)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 2 30 0 tensor(2) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "9 1 22 0 tensor(10) tensor(9)\n",
      "12 0 19 1 tensor(12) tensor(13)\n",
      "10 2 20 0 tensor(12) tensor(10)\n",
      "0 16 16 0 tensor(16) tensor(0)\n",
      "11 5 16 0 tensor(16) tensor(11)\n",
      "12 1 19 0 tensor(13) tensor(12)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 19 13 0 tensor(19) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "3 11 18 0 tensor(14) tensor(3)\n",
      "17 8 4 3 tensor(25) tensor(20)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "6 1 23 2 tensor(7) tensor(8)\n",
      "0 2 30 0 tensor(2) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "6 3 23 0 tensor(9) tensor(6)\n",
      "5 0 27 0 tensor(5) tensor(5)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "20 3 2 7 tensor(23) tensor(27)\n",
      "20 0 7 5 tensor(20) tensor(25)\n",
      "0 9 23 0 tensor(9) tensor(0)\n",
      "0 9 23 0 tensor(9) tensor(0)\n",
      "12 17 3 0 tensor(29) tensor(12)\n",
      "17 6 7 2 tensor(23) tensor(19)\n",
      "6 18 8 0 tensor(24) tensor(6)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "7 5 20 0 tensor(12) tensor(7)\n",
      "18 0 9 5 tensor(18) tensor(23)\n",
      "32 0 0 0 tensor(32) tensor(32)\n",
      "5 0 24 3 tensor(5) tensor(8)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "2 1 29 0 tensor(3) tensor(2)\n",
      "16 0 3 13 tensor(16) tensor(29)\n",
      "7 4 12 9 tensor(11) tensor(16)\n",
      "14 5 11 2 tensor(19) tensor(16)\n",
      "19 13 0 0 tensor(32) tensor(19)\n",
      "11 0 21 0 tensor(11) tensor(11)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 31 1 tensor(0) tensor(1)\n",
      "9 0 14 9 tensor(9) tensor(18)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 27 5 tensor(0) tensor(5)\n",
      "9 0 5 18 tensor(9) tensor(27)\n",
      "8 0 20 4 tensor(8) tensor(12)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "13 0 5 14 tensor(13) tensor(27)\n",
      "16 7 4 5 tensor(23) tensor(21)\n",
      "0 11 21 0 tensor(11) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "8 0 17 7 tensor(8) tensor(15)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "12 1 18 1 tensor(13) tensor(13)\n",
      "13 8 10 1 tensor(21) tensor(14)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "6 6 18 2 tensor(12) tensor(8)\n",
      "0 8 24 0 tensor(8) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 8 24 0 tensor(8) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "7 0 21 4 tensor(7) tensor(11)\n",
      "15 0 16 1 tensor(15) tensor(16)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "19 0 8 5 tensor(19) tensor(24)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 26 6 tensor(0) tensor(6)\n",
      "8 0 16 8 tensor(8) tensor(16)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "8 0 22 2 tensor(8) tensor(10)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 7 25 0 tensor(7) tensor(0)\n",
      "12 5 10 5 tensor(17) tensor(17)\n",
      "9 0 19 4 tensor(9) tensor(13)\n",
      "10 3 19 0 tensor(13) tensor(10)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 19 13 0 tensor(19) tensor(0)\n",
      "0 9 23 0 tensor(9) tensor(0)\n",
      "14 0 18 0 tensor(14) tensor(14)\n",
      "4 0 21 7 tensor(4) tensor(11)\n",
      "9 1 21 1 tensor(10) tensor(10)\n",
      "0 0 30 2 tensor(0) tensor(2)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 10 22 0 tensor(10) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 0 17 3 tensor(12) tensor(15)\n",
      "7 0 23 2 tensor(7) tensor(9)\n",
      "0 1 31 0 tensor(1) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 8 24 0 tensor(8) tensor(0)\n",
      "3 1 28 0 tensor(4) tensor(3)\n",
      "12 0 0 20 tensor(12) tensor(32)\n",
      "16 0 12 4 tensor(16) tensor(20)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "24 0 3 5 tensor(24) tensor(29)\n",
      "8 0 20 4 tensor(8) tensor(12)\n",
      "0 10 22 0 tensor(10) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 9 23 0 tensor(9) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "11 0 11 10 tensor(11) tensor(21)\n",
      "8 3 3 18 tensor(11) tensor(26)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 30 2 tensor(0) tensor(2)\n",
      "8 0 12 12 tensor(8) tensor(20)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "3 0 27 2 tensor(3) tensor(5)\n",
      "20 0 8 4 tensor(20) tensor(24)\n",
      "0 0 25 7 tensor(0) tensor(7)\n",
      "10 2 20 0 tensor(12) tensor(10)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "10 0 8 14 tensor(10) tensor(24)\n",
      "0 0 15 17 tensor(0) tensor(17)\n",
      "12 0 7 13 tensor(12) tensor(25)\n",
      "26 0 0 6 tensor(26) tensor(32)\n",
      "4 0 26 2 tensor(4) tensor(6)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 27 5 tensor(0) tensor(5)\n",
      "7 0 10 15 tensor(7) tensor(22)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "13 0 13 6 tensor(13) tensor(19)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "20 0 4 8 tensor(20) tensor(28)\n",
      "6 7 17 2 tensor(13) tensor(8)\n",
      "14 4 11 3 tensor(18) tensor(17)\n",
      "3 0 28 1 tensor(3) tensor(4)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 10 22 0 tensor(10) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 5 27 0 tensor(5) tensor(0)\n",
      "0 12 20 0 tensor(12) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "6 3 21 2 tensor(9) tensor(8)\n",
      "0 6 26 0 tensor(6) tensor(0)\n",
      "0 10 22 0 tensor(10) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "13 17 2 0 tensor(30) tensor(13)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 5 27 0 tensor(5) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 2 30 0 tensor(2) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "6 2 24 0 tensor(8) tensor(6)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 20 12 0 tensor(20) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 1 31 0 tensor(1) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "8 0 15 9 tensor(8) tensor(17)\n",
      "11 2 17 2 tensor(13) tensor(13)\n",
      "0 12 20 0 tensor(12) tensor(0)\n",
      "0 2 30 0 tensor(2) tensor(0)\n",
      "7 4 14 7 tensor(11) tensor(14)\n",
      "5 17 10 0 tensor(22) tensor(5)\n",
      "1 16 11 4 tensor(17) tensor(5)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 11 21 0 tensor(11) tensor(0)\n",
      "4 14 14 0 tensor(18) tensor(4)\n",
      "17 8 7 0 tensor(25) tensor(17)\n",
      "0 1 31 0 tensor(1) tensor(0)\n",
      "0 11 21 0 tensor(11) tensor(0)\n",
      "0 5 22 5 tensor(5) tensor(5)\n",
      "19 0 11 2 tensor(19) tensor(21)\n",
      "23 0 1 8 tensor(23) tensor(31)\n",
      "10 0 18 4 tensor(10) tensor(14)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "11 0 9 12 tensor(11) tensor(23)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 24 8 tensor(0) tensor(8)\n",
      "13 0 8 11 tensor(13) tensor(24)\n",
      "9 0 21 2 tensor(9) tensor(11)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "14 4 13 1 tensor(18) tensor(15)\n",
      "11 0 20 1 tensor(11) tensor(12)\n",
      "0 1 31 0 tensor(1) tensor(0)\n",
      "6 10 13 3 tensor(16) tensor(9)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 30 2 tensor(0) tensor(2)\n",
      "19 0 0 13 tensor(19) tensor(32)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 2 30 0 tensor(2) tensor(0)\n",
      "0 11 21 0 tensor(11) tensor(0)\n",
      "0 20 12 0 tensor(20) tensor(0)\n",
      "8 5 19 0 tensor(13) tensor(8)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 2 30 0 tensor(2) tensor(0)\n",
      "13 1 16 2 tensor(14) tensor(15)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 1 31 0 tensor(1) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 22 10 0 tensor(22) tensor(0)\n",
      "0 2 30 0 tensor(2) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "14 2 16 0 tensor(16) tensor(14)\n",
      "6 0 25 1 tensor(6) tensor(7)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 1 31 0 tensor(1) tensor(0)\n",
      "8 7 17 0 tensor(15) tensor(8)\n",
      "17 2 12 1 tensor(19) tensor(18)\n",
      "0 11 21 0 tensor(11) tensor(0)\n",
      "2 1 29 0 tensor(3) tensor(2)\n",
      "16 11 0 5 tensor(27) tensor(21)\n",
      "23 1 2 6 tensor(24) tensor(29)\n",
      "0 8 24 0 tensor(8) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 7 25 0 tensor(7) tensor(0)\n",
      "0 5 27 0 tensor(5) tensor(0)\n",
      "19 0 0 13 tensor(19) tensor(32)\n",
      "12 5 15 0 tensor(17) tensor(12)\n",
      "0 4 28 0 tensor(4) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 10 22 0 tensor(10) tensor(0)\n",
      "25 1 6 0 tensor(26) tensor(25)\n",
      "9 0 0 23 tensor(9) tensor(32)\n",
      "8 0 20 4 tensor(8) tensor(12)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "17 0 11 4 tensor(17) tensor(21)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "3 0 23 6 tensor(3) tensor(9)\n",
      "27 0 2 3 tensor(27) tensor(30)\n",
      "0 1 31 0 tensor(1) tensor(0)\n",
      "12 2 15 3 tensor(14) tensor(15)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "10 6 16 0 tensor(16) tensor(10)\n",
      "3 0 28 1 tensor(3) tensor(4)\n",
      "0 2 30 0 tensor(2) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 3 29 0 tensor(3) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 1 31 0 tensor(1) tensor(0)\n",
      "8 5 19 0 tensor(13) tensor(8)\n",
      "0 25 7 0 tensor(25) tensor(0)\n",
      "0 12 20 0 tensor(12) tensor(0)\n",
      "0 5 27 0 tensor(5) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 4 28 0 tensor(4) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 12 20 0 tensor(12) tensor(0)\n",
      "0 9 23 0 tensor(9) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "10 6 16 0 tensor(16) tensor(10)\n",
      "0 1 31 0 tensor(1) tensor(0)\n",
      "0 19 13 0 tensor(19) tensor(0)\n",
      "0 17 15 0 tensor(17) tensor(0)\n",
      "0 5 27 0 tensor(5) tensor(0)\n",
      "0 2 30 0 tensor(2) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "12 2 18 0 tensor(14) tensor(12)\n",
      "2 1 29 0 tensor(3) tensor(2)\n",
      "6 16 10 0 tensor(22) tensor(6)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "18 0 9 5 tensor(18) tensor(23)\n",
      "0 0 30 2 tensor(0) tensor(2)\n",
      "0 3 29 0 tensor(3) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "5 3 22 2 tensor(8) tensor(7)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "5 8 19 0 tensor(13) tensor(5)\n",
      "4 11 12 5 tensor(15) tensor(9)\n",
      "0 4 17 11 tensor(4) tensor(11)\n",
      "0 5 24 3 tensor(5) tensor(3)\n",
      "3 0 25 4 tensor(3) tensor(7)\n",
      "32 0 0 0 tensor(32) tensor(32)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "13 0 16 3 tensor(13) tensor(16)\n",
      "0 0 31 1 tensor(0) tensor(1)\n",
      "5 0 26 1 tensor(5) tensor(6)\n",
      "5 0 26 1 tensor(5) tensor(6)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "5 9 13 5 tensor(14) tensor(10)\n",
      "19 0 0 13 tensor(19) tensor(32)\n",
      "1 0 28 3 tensor(1) tensor(4)\n",
      "12 0 17 3 tensor(12) tensor(15)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "8 0 22 2 tensor(8) tensor(10)\n",
      "1 5 24 2 tensor(6) tensor(3)\n",
      "0 18 14 0 tensor(18) tensor(0)\n",
      "0 0 24 8 tensor(0) tensor(8)\n",
      "21 0 3 8 tensor(21) tensor(29)\n",
      "0 23 9 0 tensor(23) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 2 30 0 tensor(2) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 2 30 0 tensor(2) tensor(0)\n",
      "7 4 21 0 tensor(11) tensor(7)\n",
      "25 0 4 3 tensor(25) tensor(28)\n",
      "5 3 20 4 tensor(8) tensor(9)\n",
      "0 3 29 0 tensor(3) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 14 18 0 tensor(14) tensor(0)\n",
      "10 5 16 1 tensor(15) tensor(11)\n",
      "0 13 19 0 tensor(13) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "2 0 23 7 tensor(2) tensor(9)\n",
      "7 0 23 2 tensor(7) tensor(9)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "4 6 22 0 tensor(10) tensor(4)\n",
      "0 20 12 0 tensor(20) tensor(0)\n",
      "9 4 18 1 tensor(13) tensor(10)\n",
      "11 2 18 1 tensor(13) tensor(12)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 2 30 0 tensor(2) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "3 1 28 0 tensor(4) tensor(3)\n",
      "21 6 3 2 tensor(27) tensor(23)\n",
      "8 4 19 1 tensor(12) tensor(9)\n",
      "0 4 28 0 tensor(4) tensor(0)\n",
      "0 1 31 0 tensor(1) tensor(0)\n",
      "0 10 15 7 tensor(10) tensor(7)\n",
      "0 0 26 6 tensor(0) tensor(6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 5 15 0 tensor(17) tensor(12)\n",
      "16 4 8 4 tensor(20) tensor(20)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 2 30 0 tensor(2) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "9 0 23 0 tensor(9) tensor(9)\n",
      "7 8 17 0 tensor(15) tensor(7)\n",
      "0 1 29 2 tensor(1) tensor(2)\n",
      "1 0 10 21 tensor(1) tensor(22)\n",
      "26 0 6 0 tensor(26) tensor(26)\n",
      "8 0 24 0 tensor(8) tensor(8)\n",
      "13 0 9 10 tensor(13) tensor(23)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "14 0 13 5 tensor(14) tensor(19)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "5 2 23 2 tensor(7) tensor(7)\n",
      "11 0 20 1 tensor(11) tensor(12)\n",
      "0 0 30 2 tensor(0) tensor(2)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "17 4 9 2 tensor(21) tensor(19)\n",
      "0 0 30 2 tensor(0) tensor(2)\n",
      "8 1 12 11 tensor(9) tensor(19)\n",
      "0 10 22 0 tensor(10) tensor(0)\n",
      "0 20 12 0 tensor(20) tensor(0)\n",
      "0 2 30 0 tensor(2) tensor(0)\n",
      "11 0 3 18 tensor(11) tensor(29)\n",
      "11 0 19 2 tensor(11) tensor(13)\n",
      "2 9 19 2 tensor(11) tensor(4)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "19 5 4 4 tensor(24) tensor(23)\n",
      "12 11 8 1 tensor(23) tensor(13)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "8 0 13 11 tensor(8) tensor(19)\n",
      "18 0 8 6 tensor(18) tensor(24)\n",
      "3 2 27 0 tensor(5) tensor(3)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "9 0 13 10 tensor(9) tensor(19)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 10 22 0 tensor(10) tensor(0)\n",
      "8 7 16 1 tensor(15) tensor(9)\n",
      "13 1 18 0 tensor(14) tensor(13)\n",
      "16 0 11 5 tensor(16) tensor(21)\n",
      "13 0 15 4 tensor(13) tensor(17)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 20 12 0 tensor(20) tensor(0)\n",
      "11 3 18 0 tensor(14) tensor(11)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "3 3 26 0 tensor(6) tensor(3)\n",
      "11 0 17 4 tensor(11) tensor(15)\n",
      "8 0 23 1 tensor(8) tensor(9)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "14 0 7 11 tensor(14) tensor(25)\n",
      "12 0 9 11 tensor(12) tensor(23)\n",
      "0 18 14 0 tensor(18) tensor(0)\n",
      "6 4 22 0 tensor(10) tensor(6)\n",
      "9 7 15 1 tensor(16) tensor(10)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "15 0 8 9 tensor(15) tensor(24)\n",
      "7 3 17 5 tensor(10) tensor(12)\n",
      "4 9 19 0 tensor(13) tensor(4)\n",
      "0 1 31 0 tensor(1) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "11 0 16 5 tensor(11) tensor(16)\n",
      "5 0 17 10 tensor(5) tensor(15)\n",
      "4 0 20 8 tensor(4) tensor(12)\n",
      "31 0 0 1 tensor(31) tensor(32)\n",
      "9 0 20 3 tensor(9) tensor(12)\n",
      "1 6 25 0 tensor(7) tensor(1)\n",
      "5 1 26 0 tensor(6) tensor(5)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "15 0 11 6 tensor(15) tensor(21)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "24 6 2 0 tensor(30) tensor(24)\n",
      "3 0 28 1 tensor(3) tensor(4)\n",
      "3 5 24 0 tensor(8) tensor(3)\n",
      "20 0 3 9 tensor(20) tensor(29)\n",
      "0 0 21 11 tensor(0) tensor(11)\n",
      "0 0 30 2 tensor(0) tensor(2)\n",
      "0 0 25 7 tensor(0) tensor(7)\n",
      "8 0 19 5 tensor(8) tensor(13)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "1 1 30 0 tensor(2) tensor(1)\n",
      "20 1 11 0 tensor(21) tensor(20)\n",
      "15 0 10 7 tensor(15) tensor(22)\n",
      "8 0 20 4 tensor(8) tensor(12)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "11 0 11 10 tensor(11) tensor(21)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "7 0 21 4 tensor(7) tensor(11)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "6 0 23 3 tensor(6) tensor(9)\n",
      "6 0 25 1 tensor(6) tensor(7)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 13 19 0 tensor(13) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "8 3 19 2 tensor(11) tensor(10)\n",
      "11 2 1 18 tensor(13) tensor(29)\n",
      "19 1 9 3 tensor(20) tensor(22)\n",
      "0 14 18 0 tensor(14) tensor(0)\n",
      "0 9 23 0 tensor(9) tensor(0)\n",
      "0 16 16 0 tensor(16) tensor(0)\n",
      "0 19 13 0 tensor(19) tensor(0)\n",
      "8 0 16 8 tensor(8) tensor(16)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "10 0 9 13 tensor(10) tensor(23)\n",
      "0 11 21 0 tensor(11) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 8 24 0 tensor(8) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 3 29 0 tensor(3) tensor(0)\n",
      "0 8 24 0 tensor(8) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 3 29 0 tensor(3) tensor(0)\n",
      "31 1 0 0 tensor(32) tensor(31)\n",
      "12 0 0 20 tensor(12) tensor(32)\n",
      "10 3 1 18 tensor(13) tensor(28)\n",
      "2 3 27 0 tensor(5) tensor(2)\n",
      "0 8 24 0 tensor(8) tensor(0)\n",
      "8 18 6 0 tensor(26) tensor(8)\n",
      "30 0 0 2 tensor(30) tensor(32)\n",
      "20 0 11 1 tensor(20) tensor(21)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "3 0 25 4 tensor(3) tensor(7)\n",
      "3 2 27 0 tensor(5) tensor(3)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "8 0 20 4 tensor(8) tensor(12)\n",
      "4 0 27 1 tensor(4) tensor(5)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "7 0 7 18 tensor(7) tensor(25)\n",
      "14 0 13 5 tensor(14) tensor(19)\n",
      "13 2 17 0 tensor(15) tensor(13)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "1 0 15 16 tensor(1) tensor(17)\n",
      "20 2 6 4 tensor(22) tensor(24)\n",
      "0 21 11 0 tensor(21) tensor(0)\n",
      "13 5 9 5 tensor(18) tensor(18)\n",
      "7 0 17 8 tensor(7) tensor(15)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 3 29 0 tensor(3) tensor(0)\n",
      "30 0 1 1 tensor(30) tensor(31)\n",
      "0 14 18 0 tensor(14) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "8 0 13 11 tensor(8) tensor(19)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "1 0 24 7 tensor(1) tensor(8)\n",
      "21 0 6 5 tensor(21) tensor(26)\n",
      "6 3 23 0 tensor(9) tensor(6)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 10 22 0 tensor(10) tensor(0)\n",
      "0 2 30 0 tensor(2) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 23 9 0 tensor(23) tensor(0)\n",
      "5 4 21 2 tensor(9) tensor(7)\n",
      "5 0 25 2 tensor(5) tensor(7)\n",
      "17 5 9 1 tensor(22) tensor(18)\n",
      "30 1 1 0 tensor(31) tensor(30)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "13 0 14 5 tensor(13) tensor(18)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "8 4 20 0 tensor(12) tensor(8)\n",
      "0 3 29 0 tensor(3) tensor(0)\n",
      "9 5 14 4 tensor(14) tensor(13)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 1 31 0 tensor(1) tensor(0)\n",
      "0 11 21 0 tensor(11) tensor(0)\n",
      "0 12 20 0 tensor(12) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 4 28 0 tensor(4) tensor(0)\n",
      "2 19 11 0 tensor(21) tensor(2)\n",
      "2 3 27 0 tensor(5) tensor(2)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "21 3 8 0 tensor(24) tensor(21)\n",
      "4 0 27 1 tensor(4) tensor(5)\n",
      "0 5 27 0 tensor(5) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 7 25 0 tensor(7) tensor(0)\n",
      "0 15 17 0 tensor(15) tensor(0)\n",
      "0 14 18 0 tensor(14) tensor(0)\n",
      "1 0 28 3 tensor(1) tensor(4)\n",
      "4 2 26 0 tensor(6) tensor(4)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 2 30 0 tensor(2) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 12 20 0 tensor(12) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "27 0 4 1 tensor(27) tensor(28)\n",
      "4 6 21 1 tensor(10) tensor(5)\n",
      "0 2 30 0 tensor(2) tensor(0)\n",
      "23 1 8 0 tensor(24) tensor(23)\n",
      "13 1 17 1 tensor(14) tensor(14)\n",
      "6 4 22 0 tensor(10) tensor(6)\n",
      "3 0 25 4 tensor(3) tensor(7)\n",
      "29 1 2 0 tensor(30) tensor(29)\n",
      "0 2 30 0 tensor(2) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 8 24 0 tensor(8) tensor(0)\n",
      "21 8 2 1 tensor(29) tensor(22)\n",
      "4 7 18 3 tensor(11) tensor(7)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "6 10 16 0 tensor(16) tensor(6)\n",
      "0 1 31 0 tensor(1) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "9 0 20 3 tensor(9) tensor(12)\n",
      "20 3 8 1 tensor(23) tensor(21)\n",
      "8 8 16 0 tensor(16) tensor(8)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "8 0 16 8 tensor(8) tensor(16)\n",
      "26 0 6 0 tensor(26) tensor(26)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 10 22 tensor(0) tensor(22)\n",
      "13 0 3 16 tensor(13) tensor(29)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "6 0 25 1 tensor(6) tensor(7)\n",
      "2 1 29 0 tensor(3) tensor(2)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "8 0 15 9 tensor(8) tensor(17)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 18 14 0 tensor(18) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "6 0 25 1 tensor(6) tensor(7)\n",
      "0 1 31 0 tensor(1) tensor(0)\n",
      "0 8 24 0 tensor(8) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 9 23 0 tensor(9) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "1 2 28 1 tensor(3) tensor(2)\n",
      "6 1 25 0 tensor(7) tensor(6)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 12 20 0 tensor(12) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 11 21 0 tensor(11) tensor(0)\n",
      "17 0 9 6 tensor(17) tensor(23)\n",
      "10 2 19 1 tensor(12) tensor(11)\n",
      "1 0 30 1 tensor(1) tensor(2)\n",
      "5 2 25 0 tensor(7) tensor(5)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 10 22 0 tensor(10) tensor(0)\n",
      "0 12 20 0 tensor(12) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "11 2 18 1 tensor(13) tensor(12)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "5 3 21 3 tensor(8) tensor(8)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "6 1 25 0 tensor(7) tensor(6)\n",
      "10 13 7 2 tensor(23) tensor(12)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "19 1 0 12 tensor(20) tensor(31)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 9 23 0 tensor(9) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "9 1 22 0 tensor(10) tensor(9)\n",
      "22 0 7 3 tensor(22) tensor(25)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "7 0 14 11 tensor(7) tensor(18)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 19 13 tensor(0) tensor(13)\n",
      "13 0 10 9 tensor(13) tensor(22)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 31 1 tensor(0) tensor(1)\n",
      "12 0 6 14 tensor(12) tensor(26)\n",
      "2 6 24 0 tensor(8) tensor(2)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "14 0 17 1 tensor(14) tensor(15)\n",
      "14 0 0 18 tensor(14) tensor(32)\n",
      "5 0 26 1 tensor(5) tensor(6)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 3 29 0 tensor(3) tensor(0)\n",
      "8 7 6 11 tensor(15) tensor(19)\n",
      "9 0 15 8 tensor(9) tensor(17)\n",
      "0 8 24 0 tensor(8) tensor(0)\n",
      "6 2 23 1 tensor(8) tensor(7)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 29 3 tensor(0) tensor(3)\n",
      "11 0 7 14 tensor(11) tensor(25)\n",
      "12 5 15 0 tensor(17) tensor(12)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "8 1 23 0 tensor(9) tensor(8)\n",
      "10 8 12 2 tensor(18) tensor(12)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "1 0 28 3 tensor(1) tensor(4)\n",
      "4 3 25 0 tensor(7) tensor(4)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "23 0 9 0 tensor(23) tensor(23)\n",
      "18 2 9 3 tensor(20) tensor(21)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "8 3 21 0 tensor(11) tensor(8)\n",
      "16 12 3 1 tensor(28) tensor(17)\n",
      "28 1 0 3 tensor(29) tensor(31)\n",
      "4 0 28 0 tensor(4) tensor(4)\n",
      "3 2 27 0 tensor(5) tensor(3)\n",
      "23 0 0 9 tensor(23) tensor(32)\n",
      "0 0 29 3 tensor(0) tensor(3)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "7 0 13 12 tensor(7) tensor(19)\n",
      "28 0 0 4 tensor(28) tensor(32)\n",
      "24 1 6 1 tensor(25) tensor(25)\n",
      "0 0 22 10 tensor(0) tensor(10)\n",
      "22 0 9 1 tensor(22) tensor(23)\n",
      "7 2 22 1 tensor(9) tensor(8)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "11 0 11 10 tensor(11) tensor(21)\n",
      "12 0 19 1 tensor(12) tensor(13)\n",
      "10 2 9 11 tensor(12) tensor(21)\n",
      "0 1 31 0 tensor(1) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "3 2 27 0 tensor(5) tensor(3)\n",
      "22 0 9 1 tensor(22) tensor(23)\n",
      "24 1 4 3 tensor(25) tensor(27)\n",
      "8 0 21 3 tensor(8) tensor(11)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "6 0 19 7 tensor(6) tensor(13)\n",
      "32 0 0 0 tensor(32) tensor(32)\n",
      "9 0 23 0 tensor(9) tensor(9)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "10 0 14 8 tensor(10) tensor(18)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "20 0 1 11 tensor(20) tensor(31)\n",
      "9 0 11 12 tensor(9) tensor(21)\n",
      "2 0 20 10 tensor(2) tensor(12)\n",
      "6 0 24 2 tensor(6) tensor(8)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "7 1 22 2 tensor(8) tensor(9)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "8 2 22 0 tensor(10) tensor(8)\n",
      "10 0 17 5 tensor(10) tensor(15)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "25 0 6 1 tensor(25) tensor(26)\n",
      "28 1 0 3 tensor(29) tensor(31)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 8 24 0 tensor(8) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "16 0 10 6 tensor(16) tensor(22)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "14 0 14 4 tensor(14) tensor(18)\n",
      "16 3 8 5 tensor(19) tensor(21)\n",
      "12 6 14 0 tensor(18) tensor(12)\n",
      "6 0 18 8 tensor(6) tensor(14)\n",
      "20 9 3 0 tensor(29) tensor(20)\n",
      "5 2 25 0 tensor(7) tensor(5)\n",
      "11 7 12 2 tensor(18) tensor(13)\n",
      "11 0 18 3 tensor(11) tensor(14)\n",
      "6 1 25 0 tensor(7) tensor(6)\n",
      "5 0 22 5 tensor(5) tensor(10)\n",
      "0 0 29 3 tensor(0) tensor(3)\n",
      "14 10 6 2 tensor(24) tensor(16)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 31 1 tensor(0) tensor(1)\n",
      "10 9 5 8 tensor(19) tensor(18)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "8 0 23 1 tensor(8) tensor(9)\n",
      "17 0 2 13 tensor(17) tensor(30)\n",
      "3 0 25 4 tensor(3) tensor(7)\n",
      "25 0 0 7 tensor(25) tensor(32)\n",
      "15 2 1 14 tensor(17) tensor(29)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "8 2 8 14 tensor(10) tensor(22)\n",
      "0 23 9 0 tensor(23) tensor(0)\n",
      "5 5 22 0 tensor(10) tensor(5)\n",
      "0 2 30 0 tensor(2) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 25 7 tensor(0) tensor(7)\n",
      "13 0 7 12 tensor(13) tensor(25)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "12 0 11 9 tensor(12) tensor(21)\n",
      "27 1 0 4 tensor(28) tensor(31)\n",
      "0 16 16 0 tensor(16) tensor(0)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "7 1 22 2 tensor(8) tensor(9)\n",
      "3 4 25 0 tensor(7) tensor(3)\n",
      "1 0 24 7 tensor(1) tensor(8)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 8 24 0 tensor(8) tensor(0)\n",
      "0 2 30 0 tensor(2) tensor(0)\n",
      "0 8 9 15 tensor(8) tensor(15)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "0 0 31 1 tensor(0) tensor(1)\n",
      "27 0 0 5 tensor(27) tensor(32)\n",
      "1 17 14 0 tensor(18) tensor(1)\n",
      "0 0 32 0 tensor(0) tensor(0)\n",
      "11 3 17 1 tensor(14) tensor(12)\n",
      "17 0 12 3 tensor(17) tensor(20)\n",
      "21 0 10 1 tensor(21) tensor(22)\n",
      "0 0 5 12 tensor(0) tensor(12)\n",
      "4455 2401 1779\n",
      "[1100/1101], prec:0.6497957992998833, recall:0.7146294513955727, f1:68.0672268907563, acc: 0.8813073231677883\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "\n",
    "\n",
    "test=data_ds('test')#test dataset\n",
    "test_loader=torch.utils.data.DataLoader(test,batch_size=32)#test dataloader\n",
    "dataset=weight_dir+'7train' #best weight\n",
    "checkpoint=torch.load(dataset,map_location='cuda:0')#model weight load\n",
    "model.load_state_dict(checkpoint)\n",
    "model.cuda()\n",
    "\n",
    "#test exp\n",
    "model.eval()\n",
    "pred_sum = 0#model output\n",
    "gt_sum = 0#label\n",
    "tp_sum=0\n",
    "fp_sum=0\n",
    "fn_sum=0\n",
    "acc=0\n",
    "sum=0\n",
    "result={}\n",
    "with torch.no_grad():\n",
    "    for it, (data,labels,g) in enumerate(test_loader):\n",
    "        inputs = data.float().cuda()\n",
    "        labels = labels.cuda()\n",
    "        output,_=model(inputs)\n",
    "\n",
    "        TP,FP,TN,FN,pred_len, gt_len,pred=fmeasure(output.cpu(),labels.cpu())\n",
    "        for idx,game_id in enumerate(g):\n",
    "            if game_id not in result.keys():\n",
    "                result[game_id]=pred[idx].tolist()\n",
    "            else:\n",
    "                result[game_id]+=pred[idx].tolist()\n",
    "        print(TP,FP,TN,FN,pred_len, gt_len)\n",
    "        tp_sum += TP\n",
    "        fp_sum += FP\n",
    "        fn_sum += FN\n",
    "        pred_sum += pred_len\n",
    "        gt_sum += gt_len\n",
    "        acc=acc+TP+TN\n",
    "        sum+=len(output)\n",
    "    with open(weight_dir+'/train_result','a') as f:\n",
    "        if tp_sum>0 and fp_sum>0 and fn_sum>0:\n",
    "            precision = tp_sum/(tp_sum+fp_sum)\n",
    "            recall = tp_sum / (tp_sum+fn_sum)\n",
    "            f1 = (2*precision*recall / (precision + recall)) * 100\n",
    "            accuracy=acc/sum\n",
    "            print( tp_sum, fp_sum, fn_sum)\n",
    "            print('[{}/{}], prec:{}, recall:{}, f1:{}, acc: {}'.format(it, len(test_loader), precision, recall, f1,accuracy))\n",
    "            f.write('{}, prec:{}, recall:{}, f1:{}, acc : {}\\n'.format(dataset, precision, recall, f1,accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5352112676056338 0.6981627296587927 0.859577922077922\n",
      "102844212431058132\n",
      "precision : 0.5352112676056338, recall : 0.6981627296587927, f1 : 0.6059225512528473, accuracy : 0.859577922077922\n",
      "0.5294117647058824 0.8537549407114624 0.8928905519176801\n",
      "102844341902586509\n",
      "precision : 0.5294117647058824, recall : 0.8537549407114624, f1 : 0.6535552193645991, accuracy : 0.8928905519176801\n",
      "0.6738703339882122 0.8532338308457711 0.9017038007863696\n",
      "102844401152267937\n",
      "precision : 0.6738703339882122, recall : 0.8532338308457711, f1 : 0.7530186608122942, accuracy : 0.9017038007863696\n",
      "0.7462962962962963 0.6106060606060606 0.896397580857218\n",
      "102844212430927059\n",
      "precision : 0.7462962962962963, recall : 0.6106060606060606, f1 : 0.6716666666666667, accuracy : 0.896397580857218\n",
      "0.4825 0.6992753623188406 0.8589494163424124\n",
      "102844412708953395\n",
      "precision : 0.4825, recall : 0.6992753623188406, f1 : 0.5710059171597633, accuracy : 0.8589494163424124\n",
      "0.6261904761904762 0.7088948787061995 0.8704789833822092\n",
      "102844212429944013\n",
      "precision : 0.6261904761904762, recall : 0.7088948787061995, f1 : 0.6649810366624526, accuracy : 0.8704789833822092\n",
      "0.4822888283378747 0.7831858407079646 0.8768041237113402\n",
      "102844341912679064\n",
      "precision : 0.4822888283378747, recall : 0.7831858407079646, f1 : 0.596964586846543, accuracy : 0.8768041237113402\n",
      "0.6082251082251082 0.7150127226463104 0.8677797833935018\n",
      "102844235753749959\n",
      "precision : 0.6082251082251082, recall : 0.7150127226463104, f1 : 0.657309941520468, accuracy : 0.8677797833935018\n",
      "0.7258064516129032 0.6828528072837633 0.8700719917723688\n",
      "102844341908026005\n",
      "precision : 0.7258064516129032, recall : 0.6828528072837633, f1 : 0.7036747458952307, accuracy : 0.8700719917723688\n",
      "0.6262886597938144 0.680672268907563 0.8586244541484717\n",
      "102844283023206486\n",
      "precision : 0.6262886597938144, recall : 0.680672268907563, f1 : 0.6523489932885906, accuracy : 0.8586244541484717\n",
      "0.6900958466453674 0.6967741935483871 0.895856052344602\n",
      "102844224147717245\n",
      "precision : 0.6900958466453674, recall : 0.6967741935483871, f1 : 0.6934189406099518, accuracy : 0.895856052344602\n",
      "0.5956043956043956 0.8827361563517915 0.8939759036144578\n",
      "102844412704890154\n",
      "precision : 0.5956043956043956, recall : 0.8827361563517915, f1 : 0.7112860892388451, accuracy : 0.8939759036144578\n",
      "0.5688405797101449 0.6652542372881356 0.915203426124197\n",
      "102844212430599377\n",
      "precision : 0.5688405797101449, recall : 0.6652542372881356, f1 : 0.61328125, accuracy : 0.915203426124197\n",
      "0.8705035971223022 0.7138643067846607 0.8954402515723271\n",
      "102844412711443769\n",
      "precision : 0.8705035971223022, recall : 0.7138643067846607, f1 : 0.7844408427876823, accuracy : 0.8954402515723271\n",
      "0.7627906976744186 0.6786206896551724 0.8585043988269795\n",
      "102844235747982779\n",
      "precision : 0.7627906976744186, recall : 0.6786206896551724, f1 : 0.7182481751824819, accuracy : 0.8585043988269795\n",
      "==precision : 0.6349282869008552, recall : 0.728193401734725, f1 : 0.6700749078192276, accuracy : 0.8808172427248038\n"
     ]
    }
   ],
   "source": [
    "with open('./label/label.pickle',\"rb\") as f4:  \n",
    "    real_result=pickle.load(f4)\n",
    "fmeasure2(result,real_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature 추출 (마지막 LSTM feature)\n",
    "- test, train, validation 모두 진행해야함!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load fin\n",
      "102844412722519367\n",
      "102844212429550795\n",
      "102844401151219358\n",
      "102844401154430631\n",
      "102844412717014335\n",
      "102844401153971877\n",
      "102844224148503678\n",
      "102844412722847048\n",
      "102844401152857762\n",
      "102844412707380528\n",
      "102844212431516886\n",
      "102844283027925085\n",
      "102844412716227901\n",
      "102844412710001974\n",
      "102844294670878922\n",
      "102844294670551241\n",
      "102844283023599703\n",
      "102844412704496937\n",
      "102844235751783874\n",
      "102844401152071328\n",
      "102844412709674293\n",
      "102844401153447587\n",
      "102844224148896895\n",
      "102844235746868664\n",
      "102979081290790284\n",
      "102844283027531868\n",
      "102844212431975640\n",
      "102844401155937960\n",
      "102844212429092040\n",
      "102844341906649746\n",
      "102844412706987311\n",
      "102844412721339716\n",
      "102844212430402768\n",
      "102844341905011343\n",
      "102844235753356742\n",
      "102844235750997440\n",
      "102844412709346612\n",
      "102844412705217835\n",
      "102844235752963525\n",
      "102844412712164667\n",
      "102844412705545516\n",
      "102844341912220311\n",
      "102844341907370644\n",
      "102844235749424575\n",
      "102844212429419722\n",
      "102844294669568199\n",
      "102844212431779031\n",
      "102844294666422466\n",
      "102844224146472059\n",
      "102844212428895431\n",
      "102844212429747404\n",
      "102844235748703677\n",
      "102844224146930812\n",
      "102844212430730450\n",
      "102844294674876621\n",
      "102844341909598870\n",
      "102844283020453971\n",
      "102844294670026952\n",
      "102844412723174729\n",
      "102844341904683662\n",
      "102844283025696858\n",
      "102844235747261881\n",
      "102844401154168486\n",
      "102844235748310460\n",
      "102844412711836986\n",
      "102844412723567946\n",
      "102844235749031358\n",
      "102844294674286796\n",
      "102844294666881219\n",
      "102844412716686654\n",
      "102844294671796427\n",
      "102844224145685626\n",
      "102844412717407552\n",
      "102844235751390657\n",
      "102844401156069033\n",
      "102904869420860038\n",
      "102910307641576395\n",
      "102844341905404560\n",
      "102844341906977427\n",
      "102844212430075086\n",
      "102844412711116088\n",
      "102844401153578660\n",
      "102844294667405508\n",
      "102844412706659630\n",
      "102844212431058132\n",
      "102844341902586509\n",
      "102844401152267937\n",
      "102844212430927059\n",
      "102844412708953395\n",
      "102844212429944013\n",
      "102844341912679064\n",
      "102844235753749959\n",
      "102844341908026005\n",
      "102844283023206486\n",
      "102844224147717245\n",
      "102844412704890154\n",
      "102844212430599377\n",
      "102844412711443769\n",
      "102844235747982779\n"
     ]
    }
   ],
   "source": [
    "save_dir = 'video_feature.pickle' #feature 어디에 저장할지\n",
    "\n",
    "test=data_ds('total')\n",
    "test_loader=torch.utils.data.DataLoader(test,batch_size=1)\n",
    "dataset=weight_dir+'train_best'\n",
    "checkpoint=torch.load(dataset,map_location='cuda:0')\n",
    "model.load_state_dict(checkpoint)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "result={}\n",
    "with torch.no_grad():\n",
    "    for it, (data,labels,g) in enumerate(test_loader):\n",
    "        inputs = data.float().cuda()\n",
    "        labels = labels.cuda()\n",
    "        _,output_f=model(inputs) #feature 만 추출\n",
    "\n",
    "        if g[0] not in result.keys():\n",
    "            print(g[0])\n",
    "            result[g[0]]=[(output_f[0]).tolist()]\n",
    "            \n",
    "        else:\n",
    "            result[g[0]]+=[(output_f[0]).tolist()]\n",
    "\n",
    "with open(weight_dir+save_dir,'wb') as f:\n",
    "    pickle.dump(result,f)       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.35791340470314026,\n",
       "  0.26698946952819824,\n",
       "  -0.09512882679700851,\n",
       "  -0.0047071343287825584,\n",
       "  0.1654966175556183,\n",
       "  0.14154905080795288,\n",
       "  -0.3338702917098999,\n",
       "  -0.12367624789476395,\n",
       "  0.18952010571956635,\n",
       "  -0.11053846031427383,\n",
       "  -0.2618904411792755,\n",
       "  -0.11754820495843887,\n",
       "  0.17547045648097992,\n",
       "  -0.03597959131002426,\n",
       "  0.4411907196044922,\n",
       "  0.014064689166843891,\n",
       "  0.01666095107793808,\n",
       "  0.1770620495080948,\n",
       "  0.3251374661922455,\n",
       "  0.003986166324466467,\n",
       "  0.2823714315891266,\n",
       "  -0.27818432450294495,\n",
       "  0.37367361783981323,\n",
       "  0.042778804898262024,\n",
       "  0.28152137994766235,\n",
       "  0.12793771922588348,\n",
       "  0.2897695004940033,\n",
       "  0.007131745107471943,\n",
       "  0.19353102147579193,\n",
       "  -0.2322344034910202,\n",
       "  -0.13017696142196655,\n",
       "  -0.04692450165748596,\n",
       "  0.21807120740413666,\n",
       "  0.21550297737121582,\n",
       "  -0.08499440550804138,\n",
       "  -0.3457574248313904,\n",
       "  -0.3608167767524719,\n",
       "  0.1919105052947998,\n",
       "  -0.18918801844120026,\n",
       "  0.13556288182735443,\n",
       "  0.24390022456645966,\n",
       "  0.23177573084831238,\n",
       "  0.31884250044822693,\n",
       "  -0.10725637525320053,\n",
       "  -0.2860272228717804,\n",
       "  -0.21971635520458221,\n",
       "  -0.29555580019950867,\n",
       "  0.3177889287471771,\n",
       "  -0.11162906140089035,\n",
       "  0.21935592591762543,\n",
       "  0.08060990273952484,\n",
       "  0.06216880679130554,\n",
       "  -0.11729424446821213,\n",
       "  0.21121692657470703,\n",
       "  -0.12110460549592972,\n",
       "  0.43867769837379456,\n",
       "  -0.2540470063686371,\n",
       "  -0.2394268661737442,\n",
       "  -0.0909498929977417,\n",
       "  0.20677007734775543,\n",
       "  -0.22091518342494965,\n",
       "  -0.3037639260292053,\n",
       "  0.30394017696380615,\n",
       "  0.153153657913208,\n",
       "  -0.35198014974594116,\n",
       "  -0.021952742710709572,\n",
       "  -0.23249979317188263,\n",
       "  -0.2581396996974945,\n",
       "  -0.2827187478542328,\n",
       "  0.36873558163642883,\n",
       "  0.3709706664085388,\n",
       "  -0.08481775969266891,\n",
       "  -0.2627639174461365,\n",
       "  0.4583421051502228,\n",
       "  -0.31300434470176697,\n",
       "  0.3708840310573578,\n",
       "  -0.10305589437484741,\n",
       "  0.3000417947769165,\n",
       "  -0.19622379541397095,\n",
       "  -0.2834450900554657,\n",
       "  0.11438899487257004,\n",
       "  -0.257830947637558,\n",
       "  -0.27232030034065247,\n",
       "  0.1493559032678604,\n",
       "  -0.26720958948135376,\n",
       "  -0.3887597322463989,\n",
       "  0.27934569120407104,\n",
       "  0.028316641226410866,\n",
       "  -0.20324905216693878,\n",
       "  0.237822026014328,\n",
       "  0.1887531876564026,\n",
       "  -0.4624069333076477,\n",
       "  0.042809564620256424,\n",
       "  -0.33468934893608093,\n",
       "  -0.1961463987827301,\n",
       "  -0.019646966829895973,\n",
       "  0.13418740034103394,\n",
       "  0.1526598483324051,\n",
       "  0.3831208050251007,\n",
       "  -0.07263857126235962,\n",
       "  0.030955949798226357,\n",
       "  -0.02819211035966873,\n",
       "  -0.11854786425828934,\n",
       "  -0.1984373927116394,\n",
       "  -0.25410041213035583,\n",
       "  -0.10539098083972931,\n",
       "  0.12409134209156036,\n",
       "  0.08357381075620651,\n",
       "  -0.19571514427661896,\n",
       "  0.19625496864318848,\n",
       "  0.18196646869182587,\n",
       "  0.3224068880081177,\n",
       "  0.20778512954711914,\n",
       "  0.23307889699935913,\n",
       "  0.2136409878730774,\n",
       "  0.309150367975235,\n",
       "  0.3247028887271881,\n",
       "  0.22155316174030304,\n",
       "  0.04499324783682823,\n",
       "  0.16452085971832275,\n",
       "  0.030625561252236366,\n",
       "  0.19001346826553345,\n",
       "  0.19127871096134186,\n",
       "  0.2723590135574341,\n",
       "  0.22710856795310974,\n",
       "  0.22784772515296936,\n",
       "  -0.14124195277690887,\n",
       "  0.3209955394268036],\n",
       " [-0.35791340470314026,\n",
       "  0.26698946952819824,\n",
       "  -0.09512882679700851,\n",
       "  -0.0047071343287825584,\n",
       "  0.1654966175556183,\n",
       "  0.14154905080795288,\n",
       "  -0.3338702917098999,\n",
       "  -0.12367624789476395,\n",
       "  0.18952010571956635,\n",
       "  -0.11053846031427383,\n",
       "  -0.2618904411792755,\n",
       "  -0.11754820495843887,\n",
       "  0.17547045648097992,\n",
       "  -0.03597959131002426,\n",
       "  0.4411907196044922,\n",
       "  0.014064689166843891,\n",
       "  0.01666095107793808,\n",
       "  0.1770620495080948,\n",
       "  0.3251374661922455,\n",
       "  0.003986166324466467,\n",
       "  0.2823714315891266,\n",
       "  -0.27818432450294495,\n",
       "  0.37367361783981323,\n",
       "  0.042778804898262024,\n",
       "  0.28152137994766235,\n",
       "  0.12793771922588348,\n",
       "  0.2897695004940033,\n",
       "  0.007131745107471943,\n",
       "  0.19353102147579193,\n",
       "  -0.2322344034910202,\n",
       "  -0.13017696142196655,\n",
       "  -0.04692450165748596,\n",
       "  0.21807120740413666,\n",
       "  0.21550297737121582,\n",
       "  -0.08499440550804138,\n",
       "  -0.3457574248313904,\n",
       "  -0.3608167767524719,\n",
       "  0.1919105052947998,\n",
       "  -0.18918801844120026,\n",
       "  0.13556288182735443,\n",
       "  0.24390022456645966,\n",
       "  0.23177573084831238,\n",
       "  0.31884250044822693,\n",
       "  -0.10725637525320053,\n",
       "  -0.2860272228717804,\n",
       "  -0.21971635520458221,\n",
       "  -0.29555580019950867,\n",
       "  0.3177889287471771,\n",
       "  -0.11162906140089035,\n",
       "  0.21935592591762543,\n",
       "  0.08060990273952484,\n",
       "  0.06216880679130554,\n",
       "  -0.11729424446821213,\n",
       "  0.21121692657470703,\n",
       "  -0.12110460549592972,\n",
       "  0.43867769837379456,\n",
       "  -0.2540470063686371,\n",
       "  -0.2394268661737442,\n",
       "  -0.0909498929977417,\n",
       "  0.20677007734775543,\n",
       "  -0.22091518342494965,\n",
       "  -0.3037639260292053,\n",
       "  0.30394017696380615,\n",
       "  0.153153657913208,\n",
       "  -0.35198014974594116,\n",
       "  -0.021952742710709572,\n",
       "  -0.23249979317188263,\n",
       "  -0.2581396996974945,\n",
       "  -0.2827187478542328,\n",
       "  0.36873558163642883,\n",
       "  0.3709706664085388,\n",
       "  -0.08481775969266891,\n",
       "  -0.2627639174461365,\n",
       "  0.4583421051502228,\n",
       "  -0.31300434470176697,\n",
       "  0.3708840310573578,\n",
       "  -0.10305589437484741,\n",
       "  0.3000417947769165,\n",
       "  -0.19622379541397095,\n",
       "  -0.2834450900554657,\n",
       "  0.11438899487257004,\n",
       "  -0.257830947637558,\n",
       "  -0.27232030034065247,\n",
       "  0.1493559032678604,\n",
       "  -0.26720958948135376,\n",
       "  -0.3887597322463989,\n",
       "  0.27934569120407104,\n",
       "  0.028316641226410866,\n",
       "  -0.20324905216693878,\n",
       "  0.237822026014328,\n",
       "  0.1887531876564026,\n",
       "  -0.4624069333076477,\n",
       "  0.042809564620256424,\n",
       "  -0.33468934893608093,\n",
       "  -0.1961463987827301,\n",
       "  -0.019646966829895973,\n",
       "  0.13418740034103394,\n",
       "  0.1526598483324051,\n",
       "  0.3831208050251007,\n",
       "  -0.07263857126235962,\n",
       "  0.030955949798226357,\n",
       "  -0.02819211035966873,\n",
       "  -0.11854786425828934,\n",
       "  -0.1984373927116394,\n",
       "  -0.25410041213035583,\n",
       "  -0.10539098083972931,\n",
       "  0.12409134209156036,\n",
       "  0.08357381075620651,\n",
       "  -0.19571514427661896,\n",
       "  0.19625496864318848,\n",
       "  0.18196646869182587,\n",
       "  0.3224068880081177,\n",
       "  0.20778512954711914,\n",
       "  0.23307889699935913,\n",
       "  0.2136409878730774,\n",
       "  0.309150367975235,\n",
       "  0.3247028887271881,\n",
       "  0.22155316174030304,\n",
       "  0.04499324783682823,\n",
       "  0.16452085971832275,\n",
       "  0.030625561252236366,\n",
       "  0.19001346826553345,\n",
       "  0.19127871096134186,\n",
       "  0.2723590135574341,\n",
       "  0.22710856795310974,\n",
       "  0.22784772515296936,\n",
       "  -0.14124195277690887,\n",
       "  0.3209955394268036],\n",
       " [-0.35791340470314026,\n",
       "  0.26698946952819824,\n",
       "  -0.09512882679700851,\n",
       "  -0.0047071343287825584,\n",
       "  0.1654966175556183,\n",
       "  0.14154905080795288,\n",
       "  -0.3338702917098999,\n",
       "  -0.12367624789476395,\n",
       "  0.18952010571956635,\n",
       "  -0.11053846031427383,\n",
       "  -0.2618904411792755,\n",
       "  -0.11754820495843887,\n",
       "  0.17547045648097992,\n",
       "  -0.03597959131002426,\n",
       "  0.4411907196044922,\n",
       "  0.014064689166843891,\n",
       "  0.01666095107793808,\n",
       "  0.1770620495080948,\n",
       "  0.3251374661922455,\n",
       "  0.003986166324466467,\n",
       "  0.2823714315891266,\n",
       "  -0.27818432450294495,\n",
       "  0.37367361783981323,\n",
       "  0.042778804898262024,\n",
       "  0.28152137994766235,\n",
       "  0.12793771922588348,\n",
       "  0.2897695004940033,\n",
       "  0.007131745107471943,\n",
       "  0.19353102147579193,\n",
       "  -0.2322344034910202,\n",
       "  -0.13017696142196655,\n",
       "  -0.04692450165748596,\n",
       "  0.21807120740413666,\n",
       "  0.21550297737121582,\n",
       "  -0.08499440550804138,\n",
       "  -0.3457574248313904,\n",
       "  -0.3608167767524719,\n",
       "  0.1919105052947998,\n",
       "  -0.18918801844120026,\n",
       "  0.13556288182735443,\n",
       "  0.24390022456645966,\n",
       "  0.23177573084831238,\n",
       "  0.31884250044822693,\n",
       "  -0.10725637525320053,\n",
       "  -0.2860272228717804,\n",
       "  -0.21971635520458221,\n",
       "  -0.29555580019950867,\n",
       "  0.3177889287471771,\n",
       "  -0.11162906140089035,\n",
       "  0.21935592591762543,\n",
       "  0.08060990273952484,\n",
       "  0.06216880679130554,\n",
       "  -0.11729424446821213,\n",
       "  0.21121692657470703,\n",
       "  -0.12110460549592972,\n",
       "  0.43867769837379456,\n",
       "  -0.2540470063686371,\n",
       "  -0.2394268661737442,\n",
       "  -0.0909498929977417,\n",
       "  0.20677007734775543,\n",
       "  -0.22091518342494965,\n",
       "  -0.3037639260292053,\n",
       "  0.30394017696380615,\n",
       "  0.153153657913208,\n",
       "  -0.35198014974594116,\n",
       "  -0.021952742710709572,\n",
       "  -0.23249979317188263,\n",
       "  -0.2581396996974945,\n",
       "  -0.2827187478542328,\n",
       "  0.36873558163642883,\n",
       "  0.3709706664085388,\n",
       "  -0.08481775969266891,\n",
       "  -0.2627639174461365,\n",
       "  0.4583421051502228,\n",
       "  -0.31300434470176697,\n",
       "  0.3708840310573578,\n",
       "  -0.10305589437484741,\n",
       "  0.3000417947769165,\n",
       "  -0.19622379541397095,\n",
       "  -0.2834450900554657,\n",
       "  0.11438899487257004,\n",
       "  -0.257830947637558,\n",
       "  -0.27232030034065247,\n",
       "  0.1493559032678604,\n",
       "  -0.26720958948135376,\n",
       "  -0.3887597322463989,\n",
       "  0.27934569120407104,\n",
       "  0.028316641226410866,\n",
       "  -0.20324905216693878,\n",
       "  0.237822026014328,\n",
       "  0.1887531876564026,\n",
       "  -0.4624069333076477,\n",
       "  0.042809564620256424,\n",
       "  -0.33468934893608093,\n",
       "  -0.1961463987827301,\n",
       "  -0.019646966829895973,\n",
       "  0.13418740034103394,\n",
       "  0.1526598483324051,\n",
       "  0.3831208050251007,\n",
       "  -0.07263857126235962,\n",
       "  0.030955949798226357,\n",
       "  -0.02819211035966873,\n",
       "  -0.11854786425828934,\n",
       "  -0.1984373927116394,\n",
       "  -0.25410041213035583,\n",
       "  -0.10539098083972931,\n",
       "  0.12409134209156036,\n",
       "  0.08357381075620651,\n",
       "  -0.19571514427661896,\n",
       "  0.19625496864318848,\n",
       "  0.18196646869182587,\n",
       "  0.3224068880081177,\n",
       "  0.20778512954711914,\n",
       "  0.23307889699935913,\n",
       "  0.2136409878730774,\n",
       "  0.309150367975235,\n",
       "  0.3247028887271881,\n",
       "  0.22155316174030304,\n",
       "  0.04499324783682823,\n",
       "  0.16452085971832275,\n",
       "  0.030625561252236366,\n",
       "  0.19001346826553345,\n",
       "  0.19127871096134186,\n",
       "  0.2723590135574341,\n",
       "  0.22710856795310974,\n",
       "  0.22784772515296936,\n",
       "  -0.14124195277690887,\n",
       "  0.3209955394268036],\n",
       " [-0.35791340470314026,\n",
       "  0.26698946952819824,\n",
       "  -0.09512882679700851,\n",
       "  -0.0047071343287825584,\n",
       "  0.1654966175556183,\n",
       "  0.14154905080795288,\n",
       "  -0.3338702917098999,\n",
       "  -0.12367624789476395,\n",
       "  0.18952010571956635,\n",
       "  -0.11053846031427383,\n",
       "  -0.2618904411792755,\n",
       "  -0.11754820495843887,\n",
       "  0.17547045648097992,\n",
       "  -0.03597959131002426,\n",
       "  0.4411907196044922,\n",
       "  0.014064689166843891,\n",
       "  0.01666095107793808,\n",
       "  0.1770620495080948,\n",
       "  0.3251374661922455,\n",
       "  0.003986166324466467,\n",
       "  0.2823714315891266,\n",
       "  -0.27818432450294495,\n",
       "  0.37367361783981323,\n",
       "  0.042778804898262024,\n",
       "  0.28152137994766235,\n",
       "  0.12793771922588348,\n",
       "  0.2897695004940033,\n",
       "  0.007131745107471943,\n",
       "  0.19353102147579193,\n",
       "  -0.2322344034910202,\n",
       "  -0.13017696142196655,\n",
       "  -0.04692450165748596,\n",
       "  0.21807120740413666,\n",
       "  0.21550297737121582,\n",
       "  -0.08499440550804138,\n",
       "  -0.3457574248313904,\n",
       "  -0.3608167767524719,\n",
       "  0.1919105052947998,\n",
       "  -0.18918801844120026,\n",
       "  0.13556288182735443,\n",
       "  0.24390022456645966,\n",
       "  0.23177573084831238,\n",
       "  0.31884250044822693,\n",
       "  -0.10725637525320053,\n",
       "  -0.2860272228717804,\n",
       "  -0.21971635520458221,\n",
       "  -0.29555580019950867,\n",
       "  0.3177889287471771,\n",
       "  -0.11162906140089035,\n",
       "  0.21935592591762543,\n",
       "  0.08060990273952484,\n",
       "  0.06216880679130554,\n",
       "  -0.11729424446821213,\n",
       "  0.21121692657470703,\n",
       "  -0.12110460549592972,\n",
       "  0.43867769837379456,\n",
       "  -0.2540470063686371,\n",
       "  -0.2394268661737442,\n",
       "  -0.0909498929977417,\n",
       "  0.20677007734775543,\n",
       "  -0.22091518342494965,\n",
       "  -0.3037639260292053,\n",
       "  0.30394017696380615,\n",
       "  0.153153657913208,\n",
       "  -0.35198014974594116,\n",
       "  -0.021952742710709572,\n",
       "  -0.23249979317188263,\n",
       "  -0.2581396996974945,\n",
       "  -0.2827187478542328,\n",
       "  0.36873558163642883,\n",
       "  0.3709706664085388,\n",
       "  -0.08481775969266891,\n",
       "  -0.2627639174461365,\n",
       "  0.4583421051502228,\n",
       "  -0.31300434470176697,\n",
       "  0.3708840310573578,\n",
       "  -0.10305589437484741,\n",
       "  0.3000417947769165,\n",
       "  -0.19622379541397095,\n",
       "  -0.2834450900554657,\n",
       "  0.11438899487257004,\n",
       "  -0.257830947637558,\n",
       "  -0.27232030034065247,\n",
       "  0.1493559032678604,\n",
       "  -0.26720958948135376,\n",
       "  -0.3887597322463989,\n",
       "  0.27934569120407104,\n",
       "  0.028316641226410866,\n",
       "  -0.20324905216693878,\n",
       "  0.237822026014328,\n",
       "  0.1887531876564026,\n",
       "  -0.4624069333076477,\n",
       "  0.042809564620256424,\n",
       "  -0.33468934893608093,\n",
       "  -0.1961463987827301,\n",
       "  -0.019646966829895973,\n",
       "  0.13418740034103394,\n",
       "  0.1526598483324051,\n",
       "  0.3831208050251007,\n",
       "  -0.07263857126235962,\n",
       "  0.030955949798226357,\n",
       "  -0.02819211035966873,\n",
       "  -0.11854786425828934,\n",
       "  -0.1984373927116394,\n",
       "  -0.25410041213035583,\n",
       "  -0.10539098083972931,\n",
       "  0.12409134209156036,\n",
       "  0.08357381075620651,\n",
       "  -0.19571514427661896,\n",
       "  0.19625496864318848,\n",
       "  0.18196646869182587,\n",
       "  0.3224068880081177,\n",
       "  0.20778512954711914,\n",
       "  0.23307889699935913,\n",
       "  0.2136409878730774,\n",
       "  0.309150367975235,\n",
       "  0.3247028887271881,\n",
       "  0.22155316174030304,\n",
       "  0.04499324783682823,\n",
       "  0.16452085971832275,\n",
       "  0.030625561252236366,\n",
       "  0.19001346826553345,\n",
       "  0.19127871096134186,\n",
       "  0.2723590135574341,\n",
       "  0.22710856795310974,\n",
       "  0.22784772515296936,\n",
       "  -0.14124195277690887,\n",
       "  0.3209955394268036],\n",
       " [-0.35791340470314026,\n",
       "  0.26698946952819824,\n",
       "  -0.09512882679700851,\n",
       "  -0.0047071343287825584,\n",
       "  0.1654966175556183,\n",
       "  0.14154905080795288,\n",
       "  -0.3338702917098999,\n",
       "  -0.12367624789476395,\n",
       "  0.18952010571956635,\n",
       "  -0.11053846031427383,\n",
       "  -0.2618904411792755,\n",
       "  -0.11754820495843887,\n",
       "  0.17547045648097992,\n",
       "  -0.03597959131002426,\n",
       "  0.4411907196044922,\n",
       "  0.014064689166843891,\n",
       "  0.01666095107793808,\n",
       "  0.1770620495080948,\n",
       "  0.3251374661922455,\n",
       "  0.003986166324466467,\n",
       "  0.2823714315891266,\n",
       "  -0.27818432450294495,\n",
       "  0.37367361783981323,\n",
       "  0.042778804898262024,\n",
       "  0.28152137994766235,\n",
       "  0.12793771922588348,\n",
       "  0.2897695004940033,\n",
       "  0.007131745107471943,\n",
       "  0.19353102147579193,\n",
       "  -0.2322344034910202,\n",
       "  -0.13017696142196655,\n",
       "  -0.04692450165748596,\n",
       "  0.21807120740413666,\n",
       "  0.21550297737121582,\n",
       "  -0.08499440550804138,\n",
       "  -0.3457574248313904,\n",
       "  -0.3608167767524719,\n",
       "  0.1919105052947998,\n",
       "  -0.18918801844120026,\n",
       "  0.13556288182735443,\n",
       "  0.24390022456645966,\n",
       "  0.23177573084831238,\n",
       "  0.31884250044822693,\n",
       "  -0.10725637525320053,\n",
       "  -0.2860272228717804,\n",
       "  -0.21971635520458221,\n",
       "  -0.29555580019950867,\n",
       "  0.3177889287471771,\n",
       "  -0.11162906140089035,\n",
       "  0.21935592591762543,\n",
       "  0.08060990273952484,\n",
       "  0.06216880679130554,\n",
       "  -0.11729424446821213,\n",
       "  0.21121692657470703,\n",
       "  -0.12110460549592972,\n",
       "  0.43867769837379456,\n",
       "  -0.2540470063686371,\n",
       "  -0.2394268661737442,\n",
       "  -0.0909498929977417,\n",
       "  0.20677007734775543,\n",
       "  -0.22091518342494965,\n",
       "  -0.3037639260292053,\n",
       "  0.30394017696380615,\n",
       "  0.153153657913208,\n",
       "  -0.35198014974594116,\n",
       "  -0.021952742710709572,\n",
       "  -0.23249979317188263,\n",
       "  -0.2581396996974945,\n",
       "  -0.2827187478542328,\n",
       "  0.36873558163642883,\n",
       "  0.3709706664085388,\n",
       "  -0.08481775969266891,\n",
       "  -0.2627639174461365,\n",
       "  0.4583421051502228,\n",
       "  -0.31300434470176697,\n",
       "  0.3708840310573578,\n",
       "  -0.10305589437484741,\n",
       "  0.3000417947769165,\n",
       "  -0.19622379541397095,\n",
       "  -0.2834450900554657,\n",
       "  0.11438899487257004,\n",
       "  -0.257830947637558,\n",
       "  -0.27232030034065247,\n",
       "  0.1493559032678604,\n",
       "  -0.26720958948135376,\n",
       "  -0.3887597322463989,\n",
       "  0.27934569120407104,\n",
       "  0.028316641226410866,\n",
       "  -0.20324905216693878,\n",
       "  0.237822026014328,\n",
       "  0.1887531876564026,\n",
       "  -0.4624069333076477,\n",
       "  0.042809564620256424,\n",
       "  -0.33468934893608093,\n",
       "  -0.1961463987827301,\n",
       "  -0.019646966829895973,\n",
       "  0.13418740034103394,\n",
       "  0.1526598483324051,\n",
       "  0.3831208050251007,\n",
       "  -0.07263857126235962,\n",
       "  0.030955949798226357,\n",
       "  -0.02819211035966873,\n",
       "  -0.11854786425828934,\n",
       "  -0.1984373927116394,\n",
       "  -0.25410041213035583,\n",
       "  -0.10539098083972931,\n",
       "  0.12409134209156036,\n",
       "  0.08357381075620651,\n",
       "  -0.19571514427661896,\n",
       "  0.19625496864318848,\n",
       "  0.18196646869182587,\n",
       "  0.3224068880081177,\n",
       "  0.20778512954711914,\n",
       "  0.23307889699935913,\n",
       "  0.2136409878730774,\n",
       "  0.309150367975235,\n",
       "  0.3247028887271881,\n",
       "  0.22155316174030304,\n",
       "  0.04499324783682823,\n",
       "  0.16452085971832275,\n",
       "  0.030625561252236366,\n",
       "  0.19001346826553345,\n",
       "  0.19127871096134186,\n",
       "  0.2723590135574341,\n",
       "  0.22710856795310974,\n",
       "  0.22784772515296936,\n",
       "  -0.14124195277690887,\n",
       "  0.3209955394268036],\n",
       " [-0.35791340470314026,\n",
       "  0.26698946952819824,\n",
       "  -0.09512882679700851,\n",
       "  -0.0047071343287825584,\n",
       "  0.1654966175556183,\n",
       "  0.14154905080795288,\n",
       "  -0.3338702917098999,\n",
       "  -0.12367624789476395,\n",
       "  0.18952010571956635,\n",
       "  -0.11053846031427383,\n",
       "  -0.2618904411792755,\n",
       "  -0.11754820495843887,\n",
       "  0.17547045648097992,\n",
       "  -0.03597959131002426,\n",
       "  0.4411907196044922,\n",
       "  0.014064689166843891,\n",
       "  0.01666095107793808,\n",
       "  0.1770620495080948,\n",
       "  0.3251374661922455,\n",
       "  0.003986166324466467,\n",
       "  0.2823714315891266,\n",
       "  -0.27818432450294495,\n",
       "  0.37367361783981323,\n",
       "  0.042778804898262024,\n",
       "  0.28152137994766235,\n",
       "  0.12793771922588348,\n",
       "  0.2897695004940033,\n",
       "  0.007131745107471943,\n",
       "  0.19353102147579193,\n",
       "  -0.2322344034910202,\n",
       "  -0.13017696142196655,\n",
       "  -0.04692450165748596,\n",
       "  0.21807120740413666,\n",
       "  0.21550297737121582,\n",
       "  -0.08499440550804138,\n",
       "  -0.3457574248313904,\n",
       "  -0.3608167767524719,\n",
       "  0.1919105052947998,\n",
       "  -0.18918801844120026,\n",
       "  0.13556288182735443,\n",
       "  0.24390022456645966,\n",
       "  0.23177573084831238,\n",
       "  0.31884250044822693,\n",
       "  -0.10725637525320053,\n",
       "  -0.2860272228717804,\n",
       "  -0.21971635520458221,\n",
       "  -0.29555580019950867,\n",
       "  0.3177889287471771,\n",
       "  -0.11162906140089035,\n",
       "  0.21935592591762543,\n",
       "  0.08060990273952484,\n",
       "  0.06216880679130554,\n",
       "  -0.11729424446821213,\n",
       "  0.21121692657470703,\n",
       "  -0.12110460549592972,\n",
       "  0.43867769837379456,\n",
       "  -0.2540470063686371,\n",
       "  -0.2394268661737442,\n",
       "  -0.0909498929977417,\n",
       "  0.20677007734775543,\n",
       "  -0.22091518342494965,\n",
       "  -0.3037639260292053,\n",
       "  0.30394017696380615,\n",
       "  0.153153657913208,\n",
       "  -0.35198014974594116,\n",
       "  -0.021952742710709572,\n",
       "  -0.23249979317188263,\n",
       "  -0.2581396996974945,\n",
       "  -0.2827187478542328,\n",
       "  0.36873558163642883,\n",
       "  0.3709706664085388,\n",
       "  -0.08481775969266891,\n",
       "  -0.2627639174461365,\n",
       "  0.4583421051502228,\n",
       "  -0.31300434470176697,\n",
       "  0.3708840310573578,\n",
       "  -0.10305589437484741,\n",
       "  0.3000417947769165,\n",
       "  -0.19622379541397095,\n",
       "  -0.2834450900554657,\n",
       "  0.11438899487257004,\n",
       "  -0.257830947637558,\n",
       "  -0.27232030034065247,\n",
       "  0.1493559032678604,\n",
       "  -0.26720958948135376,\n",
       "  -0.3887597322463989,\n",
       "  0.27934569120407104,\n",
       "  0.028316641226410866,\n",
       "  -0.20324905216693878,\n",
       "  0.237822026014328,\n",
       "  0.1887531876564026,\n",
       "  -0.4624069333076477,\n",
       "  0.042809564620256424,\n",
       "  -0.33468934893608093,\n",
       "  -0.1961463987827301,\n",
       "  -0.019646966829895973,\n",
       "  0.13418740034103394,\n",
       "  0.1526598483324051,\n",
       "  0.3831208050251007,\n",
       "  -0.07263857126235962,\n",
       "  0.030955949798226357,\n",
       "  -0.02819211035966873,\n",
       "  -0.11854786425828934,\n",
       "  -0.1984373927116394,\n",
       "  -0.25410041213035583,\n",
       "  -0.10539098083972931,\n",
       "  0.12409134209156036,\n",
       "  0.08357381075620651,\n",
       "  -0.19571514427661896,\n",
       "  0.19625496864318848,\n",
       "  0.18196646869182587,\n",
       "  0.3224068880081177,\n",
       "  0.20778512954711914,\n",
       "  0.23307889699935913,\n",
       "  0.2136409878730774,\n",
       "  0.309150367975235,\n",
       "  0.3247028887271881,\n",
       "  0.22155316174030304,\n",
       "  0.04499324783682823,\n",
       "  0.16452085971832275,\n",
       "  0.030625561252236366,\n",
       "  0.19001346826553345,\n",
       "  0.19127871096134186,\n",
       "  0.2723590135574341,\n",
       "  0.22710856795310974,\n",
       "  0.22784772515296936,\n",
       "  -0.14124195277690887,\n",
       "  0.3209955394268036],\n",
       " [-0.35791340470314026,\n",
       "  0.26698946952819824,\n",
       "  -0.09512882679700851,\n",
       "  -0.0047071343287825584,\n",
       "  0.1654966175556183,\n",
       "  0.14154905080795288,\n",
       "  -0.3338702917098999,\n",
       "  -0.12367624789476395,\n",
       "  0.18952010571956635,\n",
       "  -0.11053846031427383,\n",
       "  -0.2618904411792755,\n",
       "  -0.11754820495843887,\n",
       "  0.17547045648097992,\n",
       "  -0.03597959131002426,\n",
       "  0.4411907196044922,\n",
       "  0.014064689166843891,\n",
       "  0.01666095107793808,\n",
       "  0.1770620495080948,\n",
       "  0.3251374661922455,\n",
       "  0.003986166324466467,\n",
       "  0.2823714315891266,\n",
       "  -0.27818432450294495,\n",
       "  0.37367361783981323,\n",
       "  0.042778804898262024,\n",
       "  0.28152137994766235,\n",
       "  0.12793771922588348,\n",
       "  0.2897695004940033,\n",
       "  0.007131745107471943,\n",
       "  0.19353102147579193,\n",
       "  -0.2322344034910202,\n",
       "  -0.13017696142196655,\n",
       "  -0.04692450165748596,\n",
       "  0.21807120740413666,\n",
       "  0.21550297737121582,\n",
       "  -0.08499440550804138,\n",
       "  -0.3457574248313904,\n",
       "  -0.3608167767524719,\n",
       "  0.1919105052947998,\n",
       "  -0.18918801844120026,\n",
       "  0.13556288182735443,\n",
       "  0.24390022456645966,\n",
       "  0.23177573084831238,\n",
       "  0.31884250044822693,\n",
       "  -0.10725637525320053,\n",
       "  -0.2860272228717804,\n",
       "  -0.21971635520458221,\n",
       "  -0.29555580019950867,\n",
       "  0.3177889287471771,\n",
       "  -0.11162906140089035,\n",
       "  0.21935592591762543,\n",
       "  0.08060990273952484,\n",
       "  0.06216880679130554,\n",
       "  -0.11729424446821213,\n",
       "  0.21121692657470703,\n",
       "  -0.12110460549592972,\n",
       "  0.43867769837379456,\n",
       "  -0.2540470063686371,\n",
       "  -0.2394268661737442,\n",
       "  -0.0909498929977417,\n",
       "  0.20677007734775543,\n",
       "  -0.22091518342494965,\n",
       "  -0.3037639260292053,\n",
       "  0.30394017696380615,\n",
       "  0.153153657913208,\n",
       "  -0.35198014974594116,\n",
       "  -0.021952742710709572,\n",
       "  -0.23249979317188263,\n",
       "  -0.2581396996974945,\n",
       "  -0.2827187478542328,\n",
       "  0.36873558163642883,\n",
       "  0.3709706664085388,\n",
       "  -0.08481775969266891,\n",
       "  -0.2627639174461365,\n",
       "  0.4583421051502228,\n",
       "  -0.31300434470176697,\n",
       "  0.3708840310573578,\n",
       "  -0.10305589437484741,\n",
       "  0.3000417947769165,\n",
       "  -0.19622379541397095,\n",
       "  -0.2834450900554657,\n",
       "  0.11438899487257004,\n",
       "  -0.257830947637558,\n",
       "  -0.27232030034065247,\n",
       "  0.1493559032678604,\n",
       "  -0.26720958948135376,\n",
       "  -0.3887597322463989,\n",
       "  0.27934569120407104,\n",
       "  0.028316641226410866,\n",
       "  -0.20324905216693878,\n",
       "  0.237822026014328,\n",
       "  0.1887531876564026,\n",
       "  -0.4624069333076477,\n",
       "  0.042809564620256424,\n",
       "  -0.33468934893608093,\n",
       "  -0.1961463987827301,\n",
       "  -0.019646966829895973,\n",
       "  0.13418740034103394,\n",
       "  0.1526598483324051,\n",
       "  0.3831208050251007,\n",
       "  -0.07263857126235962,\n",
       "  0.030955949798226357,\n",
       "  -0.02819211035966873,\n",
       "  -0.11854786425828934,\n",
       "  -0.1984373927116394,\n",
       "  -0.25410041213035583,\n",
       "  -0.10539098083972931,\n",
       "  0.12409134209156036,\n",
       "  0.08357381075620651,\n",
       "  -0.19571514427661896,\n",
       "  0.19625496864318848,\n",
       "  0.18196646869182587,\n",
       "  0.3224068880081177,\n",
       "  0.20778512954711914,\n",
       "  0.23307889699935913,\n",
       "  0.2136409878730774,\n",
       "  0.309150367975235,\n",
       "  0.3247028887271881,\n",
       "  0.22155316174030304,\n",
       "  0.04499324783682823,\n",
       "  0.16452085971832275,\n",
       "  0.030625561252236366,\n",
       "  0.19001346826553345,\n",
       "  0.19127871096134186,\n",
       "  0.2723590135574341,\n",
       "  0.22710856795310974,\n",
       "  0.22784772515296936,\n",
       "  -0.14124195277690887,\n",
       "  0.3209955394268036],\n",
       " [-0.35791340470314026,\n",
       "  0.26698946952819824,\n",
       "  -0.09512882679700851,\n",
       "  -0.0047071343287825584,\n",
       "  0.1654966175556183,\n",
       "  0.14154905080795288,\n",
       "  -0.3338702917098999,\n",
       "  -0.12367624789476395,\n",
       "  0.18952010571956635,\n",
       "  -0.11053846031427383,\n",
       "  -0.2618904411792755,\n",
       "  -0.11754820495843887,\n",
       "  0.17547045648097992,\n",
       "  -0.03597959131002426,\n",
       "  0.4411907196044922,\n",
       "  0.014064689166843891,\n",
       "  0.01666095107793808,\n",
       "  0.1770620495080948,\n",
       "  0.3251374661922455,\n",
       "  0.003986166324466467,\n",
       "  0.2823714315891266,\n",
       "  -0.27818432450294495,\n",
       "  0.37367361783981323,\n",
       "  0.042778804898262024,\n",
       "  0.28152137994766235,\n",
       "  0.12793771922588348,\n",
       "  0.2897695004940033,\n",
       "  0.007131745107471943,\n",
       "  0.19353102147579193,\n",
       "  -0.2322344034910202,\n",
       "  -0.13017696142196655,\n",
       "  -0.04692450165748596,\n",
       "  0.21807120740413666,\n",
       "  0.21550297737121582,\n",
       "  -0.08499440550804138,\n",
       "  -0.3457574248313904,\n",
       "  -0.3608167767524719,\n",
       "  0.1919105052947998,\n",
       "  -0.18918801844120026,\n",
       "  0.13556288182735443,\n",
       "  0.24390022456645966,\n",
       "  0.23177573084831238,\n",
       "  0.31884250044822693,\n",
       "  -0.10725637525320053,\n",
       "  -0.2860272228717804,\n",
       "  -0.21971635520458221,\n",
       "  -0.29555580019950867,\n",
       "  0.3177889287471771,\n",
       "  -0.11162906140089035,\n",
       "  0.21935592591762543,\n",
       "  0.08060990273952484,\n",
       "  0.06216880679130554,\n",
       "  -0.11729424446821213,\n",
       "  0.21121692657470703,\n",
       "  -0.12110460549592972,\n",
       "  0.43867769837379456,\n",
       "  -0.2540470063686371,\n",
       "  -0.2394268661737442,\n",
       "  -0.0909498929977417,\n",
       "  0.20677007734775543,\n",
       "  -0.22091518342494965,\n",
       "  -0.3037639260292053,\n",
       "  0.30394017696380615,\n",
       "  0.153153657913208,\n",
       "  -0.35198014974594116,\n",
       "  -0.021952742710709572,\n",
       "  -0.23249979317188263,\n",
       "  -0.2581396996974945,\n",
       "  -0.2827187478542328,\n",
       "  0.36873558163642883,\n",
       "  0.3709706664085388,\n",
       "  -0.08481775969266891,\n",
       "  -0.2627639174461365,\n",
       "  0.4583421051502228,\n",
       "  -0.31300434470176697,\n",
       "  0.3708840310573578,\n",
       "  -0.10305589437484741,\n",
       "  0.3000417947769165,\n",
       "  -0.19622379541397095,\n",
       "  -0.2834450900554657,\n",
       "  0.11438899487257004,\n",
       "  -0.257830947637558,\n",
       "  -0.27232030034065247,\n",
       "  0.1493559032678604,\n",
       "  -0.26720958948135376,\n",
       "  -0.3887597322463989,\n",
       "  0.27934569120407104,\n",
       "  0.028316641226410866,\n",
       "  -0.20324905216693878,\n",
       "  0.237822026014328,\n",
       "  0.1887531876564026,\n",
       "  -0.4624069333076477,\n",
       "  0.042809564620256424,\n",
       "  -0.33468934893608093,\n",
       "  -0.1961463987827301,\n",
       "  -0.019646966829895973,\n",
       "  0.13418740034103394,\n",
       "  0.1526598483324051,\n",
       "  0.3831208050251007,\n",
       "  -0.07263857126235962,\n",
       "  0.030955949798226357,\n",
       "  -0.02819211035966873,\n",
       "  -0.11854786425828934,\n",
       "  -0.1984373927116394,\n",
       "  -0.25410041213035583,\n",
       "  -0.10539098083972931,\n",
       "  0.12409134209156036,\n",
       "  0.08357381075620651,\n",
       "  -0.19571514427661896,\n",
       "  0.19625496864318848,\n",
       "  0.18196646869182587,\n",
       "  0.3224068880081177,\n",
       "  0.20778512954711914,\n",
       "  0.23307889699935913,\n",
       "  0.2136409878730774,\n",
       "  0.309150367975235,\n",
       "  0.3247028887271881,\n",
       "  0.22155316174030304,\n",
       "  0.04499324783682823,\n",
       "  0.16452085971832275,\n",
       "  0.030625561252236366,\n",
       "  0.19001346826553345,\n",
       "  0.19127871096134186,\n",
       "  0.2723590135574341,\n",
       "  0.22710856795310974,\n",
       "  0.22784772515296936,\n",
       "  -0.14124195277690887,\n",
       "  0.3209955394268036],\n",
       " [-0.35791340470314026,\n",
       "  0.26698946952819824,\n",
       "  -0.09512882679700851,\n",
       "  -0.0047071343287825584,\n",
       "  0.1654966175556183,\n",
       "  0.14154905080795288,\n",
       "  -0.3338702917098999,\n",
       "  -0.12367624789476395,\n",
       "  0.18952010571956635,\n",
       "  -0.11053846031427383,\n",
       "  -0.2618904411792755,\n",
       "  -0.11754820495843887,\n",
       "  0.17547045648097992,\n",
       "  -0.03597959131002426,\n",
       "  0.4411907196044922,\n",
       "  0.014064689166843891,\n",
       "  0.01666095107793808,\n",
       "  0.1770620495080948,\n",
       "  0.3251374661922455,\n",
       "  0.003986166324466467,\n",
       "  0.2823714315891266,\n",
       "  -0.27818432450294495,\n",
       "  0.37367361783981323,\n",
       "  0.042778804898262024,\n",
       "  0.28152137994766235,\n",
       "  0.12793771922588348,\n",
       "  0.2897695004940033,\n",
       "  0.007131745107471943,\n",
       "  0.19353102147579193,\n",
       "  -0.2322344034910202,\n",
       "  -0.13017696142196655,\n",
       "  -0.04692450165748596,\n",
       "  0.21807120740413666,\n",
       "  0.21550297737121582,\n",
       "  -0.08499440550804138,\n",
       "  -0.3457574248313904,\n",
       "  -0.3608167767524719,\n",
       "  0.1919105052947998,\n",
       "  -0.18918801844120026,\n",
       "  0.13556288182735443,\n",
       "  0.24390022456645966,\n",
       "  0.23177573084831238,\n",
       "  0.31884250044822693,\n",
       "  -0.10725637525320053,\n",
       "  -0.2860272228717804,\n",
       "  -0.21971635520458221,\n",
       "  -0.29555580019950867,\n",
       "  0.3177889287471771,\n",
       "  -0.11162906140089035,\n",
       "  0.21935592591762543,\n",
       "  0.08060990273952484,\n",
       "  0.06216880679130554,\n",
       "  -0.11729424446821213,\n",
       "  0.21121692657470703,\n",
       "  -0.12110460549592972,\n",
       "  0.43867769837379456,\n",
       "  -0.2540470063686371,\n",
       "  -0.2394268661737442,\n",
       "  -0.0909498929977417,\n",
       "  0.20677007734775543,\n",
       "  -0.22091518342494965,\n",
       "  -0.3037639260292053,\n",
       "  0.30394017696380615,\n",
       "  0.153153657913208,\n",
       "  -0.35198014974594116,\n",
       "  -0.021952742710709572,\n",
       "  -0.23249979317188263,\n",
       "  -0.2581396996974945,\n",
       "  -0.2827187478542328,\n",
       "  0.36873558163642883,\n",
       "  0.3709706664085388,\n",
       "  -0.08481775969266891,\n",
       "  -0.2627639174461365,\n",
       "  0.4583421051502228,\n",
       "  -0.31300434470176697,\n",
       "  0.3708840310573578,\n",
       "  -0.10305589437484741,\n",
       "  0.3000417947769165,\n",
       "  -0.19622379541397095,\n",
       "  -0.2834450900554657,\n",
       "  0.11438899487257004,\n",
       "  -0.257830947637558,\n",
       "  -0.27232030034065247,\n",
       "  0.1493559032678604,\n",
       "  -0.26720958948135376,\n",
       "  -0.3887597322463989,\n",
       "  0.27934569120407104,\n",
       "  0.028316641226410866,\n",
       "  -0.20324905216693878,\n",
       "  0.237822026014328,\n",
       "  0.1887531876564026,\n",
       "  -0.4624069333076477,\n",
       "  0.042809564620256424,\n",
       "  -0.33468934893608093,\n",
       "  -0.1961463987827301,\n",
       "  -0.019646966829895973,\n",
       "  0.13418740034103394,\n",
       "  0.1526598483324051,\n",
       "  0.3831208050251007,\n",
       "  -0.07263857126235962,\n",
       "  0.030955949798226357,\n",
       "  -0.02819211035966873,\n",
       "  -0.11854786425828934,\n",
       "  -0.1984373927116394,\n",
       "  -0.25410041213035583,\n",
       "  -0.10539098083972931,\n",
       "  0.12409134209156036,\n",
       "  0.08357381075620651,\n",
       "  -0.19571514427661896,\n",
       "  0.19625496864318848,\n",
       "  0.18196646869182587,\n",
       "  0.3224068880081177,\n",
       "  0.20778512954711914,\n",
       "  0.23307889699935913,\n",
       "  0.2136409878730774,\n",
       "  0.309150367975235,\n",
       "  0.3247028887271881,\n",
       "  0.22155316174030304,\n",
       "  0.04499324783682823,\n",
       "  0.16452085971832275,\n",
       "  0.030625561252236366,\n",
       "  0.19001346826553345,\n",
       "  0.19127871096134186,\n",
       "  0.2723590135574341,\n",
       "  0.22710856795310974,\n",
       "  0.22784772515296936,\n",
       "  -0.14124195277690887,\n",
       "  0.3209955394268036],\n",
       " [-0.35791340470314026,\n",
       "  0.26698946952819824,\n",
       "  -0.09512882679700851,\n",
       "  -0.0047071343287825584,\n",
       "  0.1654966175556183,\n",
       "  0.14154905080795288,\n",
       "  -0.3338702917098999,\n",
       "  -0.12367624789476395,\n",
       "  0.18952010571956635,\n",
       "  -0.11053846031427383,\n",
       "  -0.2618904411792755,\n",
       "  -0.11754820495843887,\n",
       "  0.17547045648097992,\n",
       "  -0.03597959131002426,\n",
       "  0.4411907196044922,\n",
       "  0.014064689166843891,\n",
       "  0.01666095107793808,\n",
       "  0.1770620495080948,\n",
       "  0.3251374661922455,\n",
       "  0.003986166324466467,\n",
       "  0.2823714315891266,\n",
       "  -0.27818432450294495,\n",
       "  0.37367361783981323,\n",
       "  0.042778804898262024,\n",
       "  0.28152137994766235,\n",
       "  0.12793771922588348,\n",
       "  0.2897695004940033,\n",
       "  0.007131745107471943,\n",
       "  0.19353102147579193,\n",
       "  -0.2322344034910202,\n",
       "  -0.13017696142196655,\n",
       "  -0.04692450165748596,\n",
       "  0.21807120740413666,\n",
       "  0.21550297737121582,\n",
       "  -0.08499440550804138,\n",
       "  -0.3457574248313904,\n",
       "  -0.3608167767524719,\n",
       "  0.1919105052947998,\n",
       "  -0.18918801844120026,\n",
       "  0.13556288182735443,\n",
       "  0.24390022456645966,\n",
       "  0.23177573084831238,\n",
       "  0.31884250044822693,\n",
       "  -0.10725637525320053,\n",
       "  -0.2860272228717804,\n",
       "  -0.21971635520458221,\n",
       "  -0.29555580019950867,\n",
       "  0.3177889287471771,\n",
       "  -0.11162906140089035,\n",
       "  0.21935592591762543,\n",
       "  0.08060990273952484,\n",
       "  0.06216880679130554,\n",
       "  -0.11729424446821213,\n",
       "  0.21121692657470703,\n",
       "  -0.12110460549592972,\n",
       "  0.43867769837379456,\n",
       "  -0.2540470063686371,\n",
       "  -0.2394268661737442,\n",
       "  -0.0909498929977417,\n",
       "  0.20677007734775543,\n",
       "  -0.22091518342494965,\n",
       "  -0.3037639260292053,\n",
       "  0.30394017696380615,\n",
       "  0.153153657913208,\n",
       "  -0.35198014974594116,\n",
       "  -0.021952742710709572,\n",
       "  -0.23249979317188263,\n",
       "  -0.2581396996974945,\n",
       "  -0.2827187478542328,\n",
       "  0.36873558163642883,\n",
       "  0.3709706664085388,\n",
       "  -0.08481775969266891,\n",
       "  -0.2627639174461365,\n",
       "  0.4583421051502228,\n",
       "  -0.31300434470176697,\n",
       "  0.3708840310573578,\n",
       "  -0.10305589437484741,\n",
       "  0.3000417947769165,\n",
       "  -0.19622379541397095,\n",
       "  -0.2834450900554657,\n",
       "  0.11438899487257004,\n",
       "  -0.257830947637558,\n",
       "  -0.27232030034065247,\n",
       "  0.1493559032678604,\n",
       "  -0.26720958948135376,\n",
       "  -0.3887597322463989,\n",
       "  0.27934569120407104,\n",
       "  0.028316641226410866,\n",
       "  -0.20324905216693878,\n",
       "  0.237822026014328,\n",
       "  0.1887531876564026,\n",
       "  -0.4624069333076477,\n",
       "  0.042809564620256424,\n",
       "  -0.33468934893608093,\n",
       "  -0.1961463987827301,\n",
       "  -0.019646966829895973,\n",
       "  0.13418740034103394,\n",
       "  0.1526598483324051,\n",
       "  0.3831208050251007,\n",
       "  -0.07263857126235962,\n",
       "  0.030955949798226357,\n",
       "  -0.02819211035966873,\n",
       "  -0.11854786425828934,\n",
       "  -0.1984373927116394,\n",
       "  -0.25410041213035583,\n",
       "  -0.10539098083972931,\n",
       "  0.12409134209156036,\n",
       "  0.08357381075620651,\n",
       "  -0.19571514427661896,\n",
       "  0.19625496864318848,\n",
       "  0.18196646869182587,\n",
       "  0.3224068880081177,\n",
       "  0.20778512954711914,\n",
       "  0.23307889699935913,\n",
       "  0.2136409878730774,\n",
       "  0.309150367975235,\n",
       "  0.3247028887271881,\n",
       "  0.22155316174030304,\n",
       "  0.04499324783682823,\n",
       "  0.16452085971832275,\n",
       "  0.030625561252236366,\n",
       "  0.19001346826553345,\n",
       "  0.19127871096134186,\n",
       "  0.2723590135574341,\n",
       "  0.22710856795310974,\n",
       "  0.22784772515296936,\n",
       "  -0.14124195277690887,\n",
       "  0.3209955394268036]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['102844412722519367'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyein",
   "language": "python",
   "name": "hyein"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
